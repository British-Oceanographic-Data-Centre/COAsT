[{"body":"This script is designed to be a brief introduction to the Gridded class including:\n1. Creation of a Gridded object 2. Loading data into the Gridded object. 3. Combining Gridded output and Gridded domain data. 4. Interrogating the Gridded object. 5. Basic manipulation ans subsetting 6. Looking at the data with matplotlib  Up to date as of: 05/10/2021\nLoading and Interrogating Begin by importing COAsT and define some file paths for NEMO output data and a NEMO domain, as an example of model data suitable for the Gridded object.\nimport coast import matplotlib.pyplot as plt import datetime import numpy as np fn_nemo_dat = \u0026#34;./example_files/coast_example_nemo_data.nc\u0026#34; fn_nemo_dom = \u0026#34;./example_files/coast_example_nemo_domain.nc\u0026#34; fn_config_t_grid = \u0026#34;./config/example_nemo_grid_t.json\u0026#34; We can create a new Gridded object by simple calling coast.Gridded(). By passing this a NEMO data file and a NEMO domain file, COAsT will combine the two into a single xarray dataset within the Gridded object. Each individual Gridded object should be for a specified NEMO grid type, which is specified in a configuration file which is also passed as an argument. The Dask library is switched on by default, chunking can be specified in the configuration file.\nnemo_t = coast.Gridded(fn_data = fn_nemo_dat, fn_domain = fn_nemo_dom, config=fn_config_t_grid) Our new Gridded object nemo_t contains a variable called dataset, which holds information on the two files we passed. Let\u0026rsquo;s have a look at this:\nnemo_t.dataset This is an xarray dataset, which has all the information on netCDF style structures. You can see dimensions, coordinates and data variables. At the moment, none of the actual data is loaded to memory and will remain that way until it needs to be accessed.\nAlong with temperature (which has been renamed from votemper) a number of other things have happen under the hood:\n1. The dimensions have been renamed to `t_dim`, `x_dim`, `y_dim`, `z_dim` 2. The coordinates have been renamed to `time`, `longitude`, `latitude` and `depth_0`. These are the coordinates for this grid (the t-grid). Also `depth_0` has been calculated as the 3D depth array at time zero. 3. The variables `e1`, `e2` and `e3_0` have been created. These are the metrics for the t-grid in the x-dim, y-dim and z_dim (at time zero) directions.  So we see that the Gridded class has standardised some variable names and created an object based on this discretisation grid by combining the appropriate grid information with all the variables on that grid.\nWe can interact with this as an xarray Dataset object. So to extract a specific variable (say temperature):\nssh = nemo_t.dataset.ssh ssh Or as a numpy array:\nssh_np = ssh.values ssh_np.shape Then lets plot up a single time snapshot of ssh using matplotlib:\nplt.pcolormesh(nemo_t.dataset.longitude, nemo_t.dataset.latitude, nemo_t.dataset.ssh[0]) Some Manipulation There are currently some basic subsetting routines for Gridded objects, to cut out specified regions of data. Fundamentally, this can be done using xarray\u0026rsquo;s isel or sel routines to index the data. In this case, the Gridded object will pass arguments straight through to xarray.isel.\nLets get the indices of all model points within 111km km of (5W, 55N):\nind_y, ind_x = nemo_t.subset_indices_by_distance(centre_lon=-5, centre_lat=55, radius=111) ind_x.shape Now create a new, smaller subsetted Gridded object by passing those indices to isel.\nnemo_t_subset = nemo_t.isel(x_dim=ind_x, y_dim=ind_y) nemo_t_subset.dataset Alternatively, xarray.isel can be applied directly to the xarray.Dataset object.\nA longitude/latitude box of data can also be extracted using Gridded.subset_indices().\nExample for NEMO-ERSEM biogechemical variables Import COAsT, define some file paths for NEMO-ERSEM output data and a NEMO domain, and read/load your NEMO-ERSEM data into a gridded object, example:\nimport coast import matplotlib.pyplot as plt fn_bgc_dat = \u0026#34;./example_files/coast_example_SEAsia_BGC_1990.nc\u0026#34; fn_bgc_dom = \u0026#34;./example_files/coast_example_domain_SEAsia.nc\u0026#34; fn_config_bgc_grid = \u0026#34;./config/example_nemo_bgc.json\u0026#34; nemo_bgc = coast.Gridded(fn_data = fn_bgc_dat, fn_domain = fn_bgc_dom, config=fn_config_bgc_grid) nemo_bgc.dataset As an example plot a snapshot of dissolved inorganic carbon at the sea surface\nfig = plt.figure() plt.pcolormesh( nemo_bgc.dataset.longitude, nemo_bgc.dataset.latitude, nemo_bgc.dataset.dic.isel(t_dim=0).isel(z_dim=0), cmap=\u0026#34;RdYlBu_r\u0026#34;, vmin=1600, vmax=2080, ) plt.colorbar() plt.title(\u0026#34;DIC, mmol/m^3\u0026#34;) plt.xlabel(\u0026#34;longitude\u0026#34;) plt.ylabel(\u0026#34;latitude\u0026#34;) plt.show()    ","excerpt":"This script is designed to be a brief introduction to the Gridded class including:\n1. Creation of a …","ref":"/COAsT/docs/examples/intro_gridded_class/","title":"The Gridded class"},{"body":"COAsT (Coastal Ocean Assessment Toolkit) is a diagnostics and assessment python toolbox for kilometric scale regional models. The aim is that this toolbox is community-ready and flexible.\nThe initial focus will be on delivering a limited number of novel diagnostics for NEMO configurations, but that the toolbox would be expanded to include other diagnostics and other ocean models.\n","excerpt":"COAsT (Coastal Ocean Assessment Toolkit) is a diagnostics and assessment python toolbox for …","ref":"/COAsT/docs/overview/","title":"Overview"},{"body":"Python as a language comes with more stringent recommendations than most when it comes to code styling. This is advantageous in our case as it gives us an obvious set of guidelines to adopt.\nWhen it comes to simple code styling, much of what\u0026rsquo;s recommended here will be copied from Python Enhancement Proposal (PEP) 8, an officially proposed and accepted Python style guide.\nCode Styling Conventions Let\u0026rsquo;s keep things simple to start with\u0026hellip;\n  Indentation should be achieved with spaces rather than tabs and each new level of indentation should be indented by four columns (i.e four spaces).\n  Any single line, including its indentation characters, should not exceed 79 characters in length.\n  Top-level (i.e at the module/file level rather than inside a function or class) function and class definitions should be separated by two blank lines.\n  Method (functions within a class) definitions are separated by a single blank line.\n  Usually, \u0026ldquo;import\u0026rdquo; statements should be on separate lines, that is to say that you should have one line per distinct module or package import. An exception to this rule is when multiple objects are imported from a single module or package, using a \u0026ldquo;from\u0026rdquo; statement, in which case individual objects can be imported on the same line, separated by commas.\n  PEP 8 does not make a recommendation relating to the use of double or single quotes in general use, but for the sake of consistency, this document suggests the use of double quotes wherever practical. This recommendation is intended for the sake of consistency with triple-quoted strings, as per Docstring Conventions (PEP 257).\n  Operators should be separated by single columns (i.e one space) either side, unless inside parentheses, in which case no whitespace is required.\n  Comments (beginning with the # character) should be indented as if they were code. In the case of inline comments, separate the comment with two spaces following the code it shares the line with.\n  All functions should contain a docstring, which provides basic information on its usage. For this project, the reStructuredText docstring format is suggested.\n  When it comes to naming variables and functions, snake case (lower_case_words_separated_by_underscores) is preferred. There are however a few exceptions to this rule: Class names should be styled as camel case (EveryNewWordIsCapitalised). Constants (Variables that should not be changed) can be indicated by the use of screaming snake case (UPPER_CASE_WORDS_SEPARATED_BY_UNDERSCORES). Note that this library currently targets Python 3.7, so the use of typing.Final official support for constant variables, new as of Python 3.8: is not currently supported.\n  In general, it is suggested to avoid the use of single-character variable names, but this is acceptable in certain cases, such as when defining coordinates (such as x, y and z), as these will be commonly recognized and enforcing different rules could cause confusion. PEP 8 advises the following regarding names to avoid: \u0026ldquo;Never use the characters \u0026lsquo;l\u0026rsquo; (lowercase letter el), \u0026lsquo;O\u0026rsquo; (uppercase letter oh), or \u0026lsquo;I\u0026rsquo; (uppercase letter eye) as single character variable names.\u0026rdquo; These specific characters should be avoided because they present an accessibility issue, as under many fonts these characters may be difficult to distinguish or completely indistinguishable from numerals one (1) and zero (0).\n  In the interest of readability, where named iterator variables are required, this document suggests the use of double characters (e.g. \u0026ldquo;ii\u0026rdquo; rather than \u0026ldquo;i\u0026rdquo;).\n  Object-Oriented Programming The general principles of OOP are fairly straightforward and well documented, so I won\u0026rsquo;t waste your precious time by regurgitating that particular wall of text here. Instead, I\u0026rsquo;ll focus on some general pointers specific to this language and use case.\n  In Python, all class attributes are technically public, but semantically, attributes can be designated as non-public by including leading underscores in the name. For instance, \u0026ldquo;my_variable\u0026rdquo; becomes \u0026ldquo;_my_variable\u0026rdquo;. These attributes are generally referred to as \u0026ldquo;protected\u0026rdquo;.\n  When you define a Python class, it is a best practice to inherit from the base object type. This convention stems from Python 2.X, as classes and types were not originally synonymous. This behaviour is implicit in Python 3.X but the convention has persisted nonetheless. Classes defined this way are referred to as \u0026ldquo;new-style\u0026rdquo; classes.\n  When defining a class that inherits from another, it is important to remember that overridden methods (in particular, this behaviour is important when dealing with __init__ methods) do not implicitly call the parent method. What this means is that unless you want to deliberately prevent the behaviour of the parent class (this is a very niche use-case), it is important to include a reference to the parent method. An example of this is: super().__init__() This functionality is advantageous as it prevents unnecessary duplication of code, which is a key tenet of object-oriented software.\n  ","excerpt":"Python as a language comes with more stringent recommendations than most when it comes to code …","ref":"/COAsT/docs/contributing_package/python_style/","title":"Python: Style"},{"body":"** Notes on Object Structure and Loading (for contributors):\nCOAsT is an object-orientated package, meaning that data is stored within Python object structures. In addition to data storage, these objects contain methods (subroutines) which allow for manipulation of this data. An example of such an object is the Gridded object, which allows for the storage and manipulation of e.g. NEMO output and domain data. It is important to understand how to load data using COAsT and the structure of the resulting objects.\nA Gridded object is created and initialised by passing it the paths of the domain and data files. Ideally, the grid type should also be specified (T, U, V or F in the case of NEMO). For example, to load in data from a file containing data on a NEMO T-grid:\nimport coast fn_data = \u0026quot;\u0026lt;path to T-grid data file(s)\u0026gt;\u0026quot; fn_domain = \u0026quot;\u0026lt;path to domain file\u0026gt;\u0026quot; fn_config = \u0026quot;\u0026lt;path to json config file\u0026gt;\u0026quot; data = coast.Gridded(fn_data, fn_domain, fn_config) Ideally, Gridded model output data should be in grid-specific files, i.e. containing output variables situated on a NEMO T, U, V or F grid, whereas the grid variables are in a single domain file. On loading into COAsT, only the grid specific variables appropriate for the paired data are placed into the Gridded object. A Gridded object therefore contains grid-specific data and all corresponding grid variables. One of the file names can be omitted (to get a data-only or grid only object), however functionality in this case will be limited.\nOnce loaded, data is stored inside the object using an xarray.dataset object. Following on from the previous code example, this can be viewed by calling:\ndata.dataset This reveals all netcdf-type aspects of the data and domain variables that were loaded, including dimensions, coordinates, variables and attributes. For example:\n\u0026lt;xarray.Dataset\u0026gt; Dimensions: (axis_nbounds: 2, t_dim: 7, x_dim: 297, y_dim: 375, z_dim: 51) Coordinates: time (t_dim) datetime64[ns] 2007-01-01T11:58:56 ... 2007-01-31T11:58:56 longitude (y_dim, x_dim) float32 ... latitude (y_dim, x_dim) float32 ... Dimensions without coordinates: axis_nbounds, t_dim, x_dim, y_dim, z_dim Data variables: deptht_bounds (z_dim, axis_nbounds) float32 ... sossheig (t_dim, y_dim, x_dim) float32 ... time_counter_bounds (t_dim, axis_nbounds) datetime64[ns] ... time_instant (t_dim) datetime64[ns] ... temperature (t_dim, z_dim, y_dim, x_dim) float32 ... e1 (y_dim, x_dim) float32 ... e2 (y_dim, x_dim) float32 ... e3_0 (z_dim, y_dim, x_dim) float32 1.0 1.0 1.0 ... 1.0 1.0 Variables may be obtained in a number of ways. For example, to get temperature data, the following are all equivalent:\ntemp = data.dataset.temperature temp = data.dataset['temperature'] temp = data['temperature'] These commands will all return an xarray.dataarray object. Manipulation of this object can be done using xarray commands, for example indexing using [] or xarray.isel. Be aware that indexing will preserve lazy loading, however and direct access or modifying of the data will not. For this reason, if you require a subset of the data, it is best to index first.\nThe names of common grid variables are standardised within the COAsT package using JSON configuration files. For example, the following lists COAsT internal variable followed by the typical NEMO variable names:\n longitude [glamt / glamu / glamv / glamf] latitude [gphit / gphiu / gphiv / gphif] time [time_counter] e1 [e1t / e1u / e1v / e1f] (dx variable) e2 [e1t / e1u / e1v / e1f] (dy variable) e3_0 [e3t_0 / e3u_0 / e3v_0 / e3f_0] (dz variable at time 0)  Longitude, latitude and time are also set as coordinates. You might notice that dimensions are also standardised:\n x_dim The dimension for the x-axis (longitude) y_dim The dimension for the y-axis (latitude) t_dim The dimension for the time axis z_dim The dimension for the depth axis.  Wherever possible, the aim is to ensure that all of the above is consistent across the whole COAsT toolbox. Therefore, you will also find the same names and dimensions in, for example observation objects. Future objects, where applicable, will also follow these conventions. If you (as a contributor) add new objects to the toolbox, following the above template is strongly encouraged. This includes using xarray dataset/dataarray objects where possible, adopting an object oriented approach and adhering to naming conventions.\n","excerpt":"** Notes on Object Structure and Loading (for contributors):\nCOAsT is an object-orientated package, …","ref":"/COAsT/docs/contributing_package/python_structure/","title":"Python: Structure"},{"body":"Prerequisites This package requires;\n python version 3.8+ Anaconda version 4.10+  Are there any system requirements for using this project? What languages are supported (if any)? Do users need to already have any software or tools installed?\nBasic use installation via conda or pip This package should be installed by run;\nconda install -c bodc coast However, there is also the option of;\npip install COAsT if you wish to install from source then got to GitHub and follow the README instructions\nThe base package should now be installed on your system. The following packages might be required for some of the advanced plotting features;\n cartopy  Development use installation If you would prefer to work with a clone of the repository in a development python environment do the following. First clone the repoitory in the place where you want to work:\ngit clone https://github.com/British-Oceanographic-Data-Centre/COAsT.git Then start building a python environment. Here (for example) called coast_dev:\nmodule load anaconda/5-2021 # or whatever it takes to activate conda conda config --add channels conda-forge # add conda-forge to your conda channels conda create -n coast_dev python=3.8 # create a new environment. E.g. `coast_dev` conda activate coast_dev # activate new environment Install packages to the environment:\ncd COAsT conda install --file conda_dev_requirements.txt At the time of writing (01/04/2022) the contents of conda_dev_requirements.txt was:\nless COAsT/conda_dev_requirements.txt numpy\u0026gt;=1.22.3 dask\u0026gt;=2022.3.0 dask[complete]\u0026gt;=2022.3.0 xarray\u0026gt;=2022.3.0 matplotlib\u0026gt;=3.5.1 netCDF4\u0026gt;=1.5.8 scipy\u0026gt;=1.8.0 gsw\u0026gt;=3.4.0 utide\u0026gt;=0.2.6 scikit-learn\u0026gt;=1.0.2 scikit-image\u0026gt;=0.19.2 statsmodels\u0026gt;=0.13.2 cartopy\u0026gt;=0.20.2 spyder\u0026gt;=5.1.5 Obtaining Example files In order to try the Examples, example data files and configuration files are recommended.\nExample data files Download example files and link them into a new directory:\nrm -rf coast_demo mkdir coast_demo cd coast_demo wget -c https://linkedsystems.uk/erddap/files/COAsT_example_files/COAsT_example_files.zip \u0026amp;\u0026amp; unzip COAsT_example_files.zip ln -s COAsT_example_files example_files Example configuration files To facilitate loading different types of data, key information is passed to COAsT using configuration files. The config files used in the Examples are in the repository, or can be downloaded as static files:\ncd ../coast_demo wget -c https://github.com/British-Oceanographic-Data-Centre/COAsT/archive/refs/heads/master.zip \u0026amp;\u0026amp; unzip COAsT-master.zip ln -s COAsT-master/config config Preparation for Workshop Package Installation with conda Assuming a linux environment and that you have anaconda on your system:\n## Fresh build in new conda environment module load anaconda/5-2021 # or whatever it takes to activate conda yes | conda env remove --name workshop_env # remove environment \u0026#39;workshop_env\u0026#39; if it exists yes | conda create --name workshop_env python=3.8 # create a new environment conda activate workshop_env # activate new environment yes | conda install -c bodc coast=2.0.3 # install COAsT within new environment yes | conda install -c conda-forge cartopy=0.20.2 # install cartopy Then obtain the Example data and configuration files (as above).\nTest it! The below example works best with the COAsT example data. Start by opening a python terminal and then importing COAsT:\nimport coast Before using coast, we will just check that Anaconda has installed correct package versions. In the python console copy the following:\nimport gsw import matplotlib print(gsw.__version__) print(matplotlib.__version__) The output should be\n3.4.0 3.5.1 If it is, great carry on. If it is not, problems may occur with some functionality in coast. Please get in contact using the contacts in the workshop email.\nTake a look at the example pages for more information on specific objects and methods.\n","excerpt":"Prerequisites This package requires;\n python version 3.8+ Anaconda version 4.10+  Are there any …","ref":"/COAsT/docs/getting-started/","title":"Getting Started"},{"body":"Here we give a short tutorial of how to use the Altimetry object for reading data and comparing to NEMO data.\nBegin by importing coast and other packages\nimport coast And by defining some file paths. There are the example files that can be obtained with the COAsT package:\nfn_nemo_dat = \u0026quot;./example_files/COAsT_example_NEMO_data.nc\u0026quot; fn_nemo_dom = \u0026quot;./example_files/COAsT_example_NEMO_domain.nc\u0026quot; fn_config_t_grid = \u0026quot;./config/example_nemo_grid_t.json\u0026quot; fn_altimetry = './example_files/COAsT_example_altimetry_data.nc' We need to load in a Gridded object for doing things with NEMO data.\nnemo = coast.Gridded(fn_nemo_dat, fn_nemo_dom, config=fn_config_t_grid) And now we can load in our Altimetry data. By default, Altimetry is set up to read in CMEMS netCDF files. However, if no path is supplied, then the object\u0026rsquo;s dataset will be initialised as None. Custom data can then be loaded if desired, as long as it follows the data formatting for Altimetry.\naltimetry = coast.Altimetry(fn_altimetry, config=\u0026quot;./config/example_altimetry.json\u0026quot;) Before going any further, lets just cut out the bit of the altimetry that is over the model domain. This can be done using subset_indices_lonlat_box to find relevant indices and then isel to extract them. The data has also been thinned slightly.\nind = altimetry.subset_indices_lonlat_box([-10,10], [45,60]) ind = ind[::4] altimetry = altimetry.isel(t_dim=ind) Before comparing our observations to the model, we will interpolate a model variable to the same time and geographical space as the altimetry. This is done using the obs_operator() method:\naltimetry.obs_operator(nemo, mod_var_name='ssh', time_interp='nearest') Doing this has created a new interpolated variable called interp_ssh and saved it back into our Altimetry object. Take a look at altimetry.dataset to see for yourself.\nNext we will compare this interpolated variable to an observed variable using some basic metrics. The basic_stats() routine can be used for this, which calculates some simple metrics including differences, RMSE and correlations. NOTE: This may not be a wise choice of variables.\nstats = altimetry.basic_stats(\u0026quot;ocean_tide_standard_name\u0026quot;, \u0026quot;interp_ssh\u0026quot;) Take a look inside stats.dataset to see all of the new variables. When using basic stats, the returned object is also an ALTIMETRY object, so all of the same methods can be applied. Alternatively, if you want to save the new metrics to the original altimetry object, set create_new_object = False.\nNow we will do a more complex comparison using the Continuous Ranked Probability Score (CRPS). For this, we need to hand over the model object, a model variable and an observed variable. We also give it a neighbourhood radius in km (nh_radius).\ncrps = altimetry.crps(nemo, model_var_name =\u0026quot;ssh\u0026quot;, obs_var_name=\u0026quot;ocean_tide_standard_name\u0026quot;, nh_radius=20) Again, take a look inside crps.dataset to see some new variables. Similarly to basic_stats, create_new_object can be set to false to save output to the original altimetry object.\nAltimetry has a ready built quick_plot() routine for taking a look at any of the observed or derived quantities above. So to take a look at the \u0026lsquo;sla_filtered\u0026rsquo; variable:\nfig, ax = altimetry.quick_plot(\u0026quot;ocean_tide_standard_name\u0026quot;) As stats and crps are also Altimetry objects, quick_plot() can also be used:\nfig, ax = crps.quick_plot(\u0026quot;crps\u0026quot;) fig, ax = stats.quick_plot(\u0026quot;absolute_error\u0026quot;) ","excerpt":"Here we give a short tutorial of how to use the Altimetry object for reading data and comparing to …","ref":"/COAsT/docs/examples/altimetry/","title":"Altimetry"},{"body":"For COAsT development we use a Github workflow to manage version control and collaboration. Git allows use to keep track of changes made to the COAsT code base, avoid breaking existing code and work as a group on a single package. Any contributor needs to use this workflow to add their code. Below is some guidance on using git with COAsT, including a typical workflow and cheat sheet.\nFor more information on git, see:\nGithub (https://github.com/)\nThe Github page for this package can be found:\nhere\nKey Ideas   The COAsT repository has two core branches: master and develop. The master branch contains the tested code that you install when using Anaconda. This is updated less frequently, and is the \u0026ldquo;user-facing\u0026rdquo; branch of code. Most contributors do not need to edit this branch. The develop branch is the \u0026lsquo;pre-master\u0026rsquo; branch, where working code is kept. This is the leading branch, with the most up-to-date code, although it is not necessarily user-facing. When writing code into your own branch (see below), it is \u0026lsquo;branched\u0026rsquo; from develop and then eventually merged back into develop. You should never make changes directly to either master or develop.\n  There is a \u0026lsquo;local\u0026rsquo; and \u0026lsquo;remote\u0026rsquo; copy of the COAsT repository. The local repository exists only on your machine. The remote repository is the one you see on the Github website and exists separately. The two versions of the repository can be synchronised at a single point using commands such as git pull git push and git fetch (see below). After cloning (downloading) the repository, all modifications you make/add/commit will only be local until you push them to the remote repository.\n  Typical Workflow A typical workflow for editting COAsT in git might look like:\n  Clone Repository: git clone git@github.com:British-Oceanographic-Data-Centre/COAsT.git. This will create a new copy of COAsT on your local system which you can use to interact with git and view/edit the source code. This only needs to be done once.\n  Checkout develop: git checkout develop. Before creating a new branch for your code, you should checkout the develop branch. This will switch your local repository to the develop branch. You can check what branch your current local repository is in by entering git branch \u0026ndash; it should now say develop\n  Create/checkout your new branch: git checkout -b new_branch_name. This will create and checkout your new branch \u0026ndash; right now it is an identical copy of develop. However, any changes you commit to your local repository will be saved into your branch. Once you have created your branch, you can open it as before, using git checkout new_branch_name.\n  Make changes/additions to code: Make any changes you like to COAsT. At this point it is separate from the main branches and it is safe to do so. If in doubt, enter git branch again to ensure you are within your own branch.\n  Add changes to branch: git add modified_file. Using this command will tell git that you have changed/added this file and you want to save it to the branch you are currently in. Upon entering this command, the file changes/additions are not saved to the branch and won\u0026rsquo;t be until the next step. You can remove an added file by entering git reset modified_file and can check which files have changed by typing git status.\n  Commit changes to branch: git commit -m \u0026quot;type a message in quotations\u0026quot;. Entering this command will \u0026ldquo;save\u0026rdquo; the changes you added using git add  in the step above to the branch you are currently in. Once entered, git will identify what has changed since the previous commit. If this is the first commit in your new branch then since the version of develop that you branch from. This will not change any other branch except the one you are in and you can/should do this often with an appropriate message. At this point, all changes are still only on your local machine and will not change the remote repository. It is also possible to undo a commit using git revert, so nothing is unfixable.\n  Continue modifying code: At this point, you may want to continue modifying the code, repeatedly adding changes and commiting them to your local repository, as above.\n  Push your local repository to the remote: git push origin. This will upload the changes you have made in the branch you are in (and only this branch) to the remote (website) repository. If this is the first time you have pushed this branch then an error may appear telling you to repush with the --set-upstream flag enable. Simply copy and paste this command back into the terminal. This will \u0026ldquo;create\u0026rdquo; your branch in the remote repository. Once pushed, github will do some auto-checks to make sure the code works (which it may not, but that is fine). You can continue to modify the code at any point, and push multiple times. This is encouraged if sharing with other collaboraters.\n  Once you are satisfied with your changes, move onto the next steps.\n Make sure your local branch is up to date with the remote: git pull origin when in your branch. This is to ensure that nobody else has changed your branch, or if they have to update your local branch with the changes on the remote.\n  Update your branch with develop:. Before requesting that your branch and its changes be merged back into the develop branch, it is good practice to first merge develop back into your branch. This is because develop may have changed since you started working on your branch and these changes should be merged into your branch to ensure that conflicts are resolved. To do this, first update develop by entering git checkout develop and git pull. This will update the develop branch on your local machine. Then merge develop back into your branch by entering git checkout your_branch and git merge develop. This may say up-to-date (in which case GREAT), or successful (in which case GREAT) or may say there are some conflicts. This happens when more than one person has changed the same piece of code.\n  Resolve Conflicts: This step may not be necessary if there are no conflicts. If git tells you there are conflicts, it will also tell you which files they occur in. For more information/help with conflict resolution see here\n  Create a pull request for your branch. First your most up to date branch using git push origin, even after merging develop in step 9/10. On the website you may then create a \u0026lsquo;pull request\u0026rsquo; which is a formal way of saying you want to merge your branch back into develop. A pull request allows you to ask people to \u0026lsquo;review\u0026rsquo; your branch, share your code, view the changes in your branch and other things. To make a pull request, go to the website, click on the pull requests tab and click Create new pull request. Then select your branch in the right drop down menu and develop in the left. You may then enter a description of the changes you have made and anything else you would like reviewers to see.\n  Reviewers review the code: Requested reviewers take a look at your changes and run the unit_test. Once they are satisfied, they will approve the pull request, or add comments about any problems.\n  Merge branch into develop: Once reviewers are satisfied, you may click Merge branch at the bottom of the pull request. Now your changes will be added into develop! Again, this is fine as the branch has been inspected by reviewers and any change can be reverted using git revert (although this is not encouraged for the develop branch).\n  **Note: After creating a pull request, Github will automatically apply \u0026ldquo;black formatting\u0026rdquo; to the code. This will commit new (small) changes to the branch so you should always do a git pull on your branch to make sure your local version is up to date with the remote.\nCondensed Workflow  git clone git@github.com:British-Oceanographic-Data-Centre/COAsT.git. git checkout develop git checkout -b new_branch_name Make changes git add changed_file git commit -m \u0026quot;what changes have you made\u0026quot; git push origin If your branch changed by anyone else, git pull Repeat steps 4-8 git checkout develop git pull git checkout your_branch git merge develop git push origin Create pull request from your_branch to develop, include description and request reviewers. Reviewers accept, Merge branch.  ","excerpt":"For COAsT development we use a Github workflow to manage version control and collaboration. Git …","ref":"/COAsT/docs/contributing-docs/github_workflow/","title":"Github Workflow"},{"body":"COAsT utilises Python’s default logging library and includes a simple setup function for those unfamiliar with how to use it.\nimport coast coast.logging_util.setup_logging() This is all you need to enable full logging output to the console.\nBy default, setup_logging will use the \u0026ldquo;DEBUG\u0026rdquo; logging level, if you want to adjust this, you can use the flags from the logging library.\nimport coast import logging coast.logging_util.setup_logging(level=logging.INFO) Alternative logging levels in increasing levels of severity. Note logs are reported at the chosen severity level and higher:\n..., level=logging.DEBUG) # Detailed information, typically of interest only when diagnosing problems. ..., level=logging.INFO) # Confirmation that things are working as expected. ..., level=logging.WARNING) # An indication that something unexpected happened, or indicative of some problem in the near future (e.g. ‘disk space low’). The software is still working as expected. ..., level=logging.ERROR) # Due to a more serious problem, the software has not been able to perform some function ..., level=logging.CRITICAL) # A serious error, indicating that the program itself may be unable to continue running For more info on logging levels, see the relevant Python documentation.\nLogging output will be printed in the console once enabled by default, but output can be directed to any Stream, for instance, to an opened file.\nimport coast file = open(\u0026#34;coast.log\u0026#34;, \u0026#34;w\u0026#34;) coast.logging_util.setup_logging(stream=file) coast.logging_util.info(\u0026#34;Hello World!\u0026#34;) # Your use of COAsT would go here, this line is included as an example file.close() ","excerpt":"COAsT utilises Python’s default logging library and includes a simple setup function for those …","ref":"/COAsT/docs/contributing_package/python_logging/","title":"Logging"},{"body":"Example data are provided for the following tutorial. Download these files and place the example_files directory in your working directory.\nIn addition configuration files are used to pass information about the example data files to COAsT. These can be downloaded or linked to a local version of the COAsT repository. These files should be places in a config diretory in your working directory.\nThe following tutorial is split into sections:\n","excerpt":"Example data are provided for the following tutorial. Download these files and place the …","ref":"/COAsT/docs/examples/","title":"Examples"},{"body":"Here you will find information needed to contribute code changes to the COAsT package.\n","excerpt":"Here you will find information needed to contribute code changes to the COAsT package.","ref":"/COAsT/docs/contributing_package/","title":"Contributing: COAsT"},{"body":"We use Hugo Extended Version to format and generate our website, the Docsy theme for styling and site structure, and GitHub pages to manage the deployment of the site. Hugo is an open-source static site generator that provides us with templates, content organisation in a standard directory structure, and a website generation engine. You write the pages in Markdown (or HTML if you want), and Hugo wraps them up into a website.\nAll submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests.\nUpdating a single page If you\u0026rsquo;ve just spotted something you\u0026rsquo;d like to change while using the docs, Docsy has a shortcut for you:\n Click Edit this page in the top right hand corner of the page. If you don\u0026rsquo;t already have an up to date fork of the project repo, you are prompted to get one - click Fork this repository and propose changes or Update your Fork to get an up to date version of the project to edit. The appropriate page in your fork is displayed in edit mode. make your edit submit a pull request with a summary of the changes  Previewing your changes locally If you want to run your own local Hugo server to preview your changes as you work:\n  Follow the instructions in Getting started to install Hugo and any other tools you need. You\u0026rsquo;ll need at least Hugo version 0.45 (we recommend using the most recent available version), and it must be the extended version, which supports SCSS.\n  Fork the COAsT-site repo repo into your own project, then create a local copy using git clone. Don’t forget to use --recurse-submodules or you won’t pull down some of the code you need to generate a working site.\ngit clone --recurse-submodules --depth 1 https://github.com/British-Oceanographic-Data-Centre/COAsT-site.git   Run npm install to install Node.js dependencies.\n  Run hugo server in the site root directory. By default your site will be available at http://localhost:1313/COAsT. Now that you\u0026rsquo;re serving your site locally, Hugo will watch for changes to the content and automatically refresh your site.\n  Continue with the usual GitHub workflow to edit files, commit them, push the changes up to your fork, and create a pull request.\n  Creating an issue If you\u0026rsquo;ve found a problem in the docs, but you\u0026rsquo;re not sure how to fix it yourself, please create an issue in the COAsT-site repo. You can also create an issue about a specific page by clicking the Create Issue button in the top right hand corner of the page.\nUseful resources  Docsy user guide: All about Docsy, including how it manages navigation, look and feel, and multi-language support. Hugo documentation: Comprehensive reference for Hugo. Github Hello World!: A basic introduction to GitHub concepts and workflow.  ","excerpt":"We use Hugo Extended Version to format and generate our website, the Docsy theme for styling and …","ref":"/COAsT/docs/contributing-docs/","title":"Contributing: Documentation"},{"body":"What is lazy\u0026hellip; \u0026hellip;loading Lazy loading determines if data is read into memory straight away (on that line of code execution) or if the loading is delayed until the data is physical altered by some function (normally mathematical in nature)\n\u0026hellip;evaluation Lazy evaluation is about delaying the execution of a method/function call until the value is physical required, normally as a graph or printed to screen. Lazy evaluation can also help with memory management, useful with large dataset, by allowing for optimisation on the chained methods calls.\nLazy loading and Lazy evaluation are offer used together, though it is not mandatory and always worth checking that both are happening.\nBeing Lazy in COAsT There are two way to be Lazy within the COAsT package.\n xarray Dask  xarray COAsT uses xarray to load NetCDF files in, by default this will be Lazy, the raw data values will not be brought into memory.\nyou can slice and subset the data while still having the lazy loading honoured, it is not until the data is altered, say via a call to NumPy.cumsum, that the required data will be loaded into memory.\nNote the data on disk (in the NetCDF file) is never altered, only the values in memory are changed.\nimport xarray as xr import NumPy as np dataset_domain = xr.open_dataset(fn_domain) e3w_0 = dataset_domain.e3w_0 # still lazy loaded e3w_0_cs = np.cumsum(e3w_0[1:, :, :], axis=0) # now in memory Dask When in use Dask will provide lazy evaluation on top of the lazy loading.\nusing the same example as above, a file loaded in using xarray, this time with the chunks option set, will not only lazy load the data, but will turn on Dask, now using either the xarray or Dask wrapper functions will mean the NumPy cumsum call is not evaluated right way, in fact it will not be evaluated until either the compute function is called, or a greedy method from another library is used.\nimport xarray as xr dataset_domain = xr.open_dataset(fn_domain, chunks={\u0026#34;t\u0026#34;: 1}) e3w_0 = dataset_domain.e3w_0 # still lazy loaded e3w_0_cs = e3w_0[1:, :, :].cumsum(axis=0) # Dask backed Lazy evaluation We discuss Dask even more here.\n","excerpt":"What is lazy\u0026hellip; \u0026hellip;loading Lazy loading determines if data is read into memory straight …","ref":"/COAsT/docs/contributing_package/lazy-loading/","title":"working Lazily"},{"body":"What is Dask Dask is a python library that allows code to be run in parallel based on the hardware your running on. This means Dask works just as well on your laptop as on your large server.\nUsing Dask Dask is included in the xarray library. When loading a data source (file/NumPy array) Dask is automatically initiated with the chunks variable in the config file. However the chunking may not be optimal but you can adjust it before computation are made.\nnemo_t = coast.Gridded( fn_data=dn_files+fn_nemo_grid_t_dat, fn_domain=dn_files+fn_nemo_dom, config=fn_config) chunks = { \u0026#34;x_dim\u0026#34;: 10, \u0026#34;y_dim\u0026#34;: 10, \u0026#34;t_dim\u0026#34;: 10, } # Chunks are prescribed in the config json file, but can be adjusted while the data is lazy loaded. nemo_t.dataset.chunk(chunks) chunks tell Dask where to break your data across the different processor tasks.\nDirect Dask Dask can be imported and used directly\nimport Dask.array as da big_array = da.multiple(array1,array2) Dask arrays follow the NumPy API. This means that most NumPy functions have a Dask version.\nPotential Issues Dask objects are immutable. This means that the classic approach, pre-allocation follow by modification will not work.\nThe following code will error.\nimport Dask.array as da e3w_0 = da.squeeze(dataset_domain.e3w_0) depth_0 = da.zero_like(e3w_0) depth_0[0, :, :] = 0.5 * e3w_0[0, :, :] # this line will error out option 1 Continue using NumPy function but wrapping the final value in a Dask array. This final Dask object will still be in-memory.\ne3w_0 = np.squeeze(dataset_domain.e3w_0) depth_0 = np.zeros_like(e3w_0) depth_0[0, :, :] = 0.5 * e3w_0[0, :, :] depth_0[1:, :, :] = depth_0[0, :, :] + np.cumsum(e3w_0[1:, :, :], axis=0) depth_0 = da.array(depth_0) option 2 Dask offers a feature called delayed. This can be used as a modifier on your complex methods as follows;\n@Dask.delayed def set_timezero_depths(self, dataset_domain): # complex workings these do not return the computed answer, rather it returns a delayed object. These delayed object get stacked, as more delayed methods are called. When the value is needed, it can be computed like so;\nne = coast.Gridded(...) # come complex delayed methods called ne.data_variable.compute() Dask will now work out a computing path via all the required methods using as many processor tasks as possible.\nVisualising the Graph Dask is fundamentally a computational graph library, to understand what is happening in the background it can help to see these graphs (on smaller/simpler problems). This can be achieved by running;\nne = coast.Gridded(...) # come complex delayed methods called ne.data_variable.visualize() this will output a png image of the graph in the calling directory and could look like this;\n  ","excerpt":"What is Dask Dask is a python library that allows code to be run in parallel based on the hardware …","ref":"/COAsT/docs/contributing_package/dask/","title":"Dask"},{"body":"This is a demonstration script for using the Climatology object in the COAsT package. This object has methods for analysing climatological data. Further examples can be found in the COAsT github repository.\nClimatological means This section shows an example of how to use the Climatology.make_climatology() method to calculates mean over a given period of time. This method doesn\u0026rsquo;t take different years into account, unless using the \u0026lsquo;years\u0026rsquo; frequency. (See the Multi-year climatological means section for multi-yeared data.)\nBegin by importing coast:\nimport coast And by defining some file paths for the data:\n# Path to a data file. fn_nemo_dat = \u0026#34;./example_files/coast_example_nemo_data.nc\u0026#34; # Set path for domain file if required. fn_nemo_dom = \u0026#34;./example_files/coast_example_nemo_domain.nc\u0026#34; # Set path for model configuration file config = \u0026#34;./config/example_nemo_grid_t.json\u0026#34; # Read in data (This example uses NEMO data.) nemo = coast.Gridded(fn_nemo_dat, fn_nemo_dom, config=config) Calculate the climatology for temperature and sea surface height (ssh) as an example:\n# Optional (This specifies an output file path.) fn_out = \u0026#34;/path/to/outputfile.nc\u0026#34; # String is appended to \u0026#34;time.\u0026#34; to create a valid xarray time period. (i.e. time.season, time.month...) climatology_frequency = \u0026#34;month\u0026#34; clim = coast.Climatology() # Not writing output to file: clim_mean = clim.make_climatology(nemo[[\u0026#39;temperature\u0026#39;,\u0026#39;ssh\u0026#39;]], climatology_frequency) # Writing output to file (may require a large amount of memory.) clim_mean = clim.make_climatology(nemo[[\u0026#39;temperature\u0026#39;,\u0026#39;ssh\u0026#39;]], climatology_frequency, fn_out=fn_out) Below shows the structure of a dataset returned, containing 1 month worth of meaned temperature and sea surface height data:\n\u0026lt;xarray.Dataset\u0026gt; Dimensions: (y_dim: 375, x_dim: 297, z_dim: 51, month: 1) Coordinates: longitude (y_dim, x_dim) float32 ... latitude (y_dim, x_dim) float32 ... depth_0 (z_dim, y_dim, x_dim) float32 0.5 0.5 0.5 ... 50.5 50.5 50.5 * month (month) int64 1 Dimensions without coordinates: y_dim, x_dim, z_dim Data variables: temperature (month, z_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 51, 375, 297), meta=np.ndarray\u0026gt; ssh (month, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 375, 297), meta=np.ndarray\u0026gt; Multi-year climatological means This section shows an example of how to use the Climatology.multiyear_averages() method to generate annual averages across specified periods of time. This method is designed to be compatible with multi-year datasets, but will work with single year datasets too.\nBegin by importing coast and helpful coast utilities:\nimport coast from coast import seasons And by defining some file paths for the data:\n# Path to a single or multiple NEMO files. fn_nemo_data = \u0026#34;/Path/to/Nemo/*.nc\u0026#34; # Set path for domain file if required. fn_nemo_domain = \u0026#34;/Path/to/domain/domain.nc\u0026#34; fn_config_t_grid = \u0026#34;/Path/to/config/file.json\u0026#34; # Read in multiyear data (This example uses NEMO data from multiple datafiles.) nemo = coast.Gridded(fn_data=fn_nemo_data, fn_domain=fn_nemo_domain, config=fn_config_t_grid, multiple=True) Now calculate temperature and ssh means of each season across multiple years for specified data:\nclim = coast.Climatology() # Using seasons module to specify time period. # SPRING, SUMMER, AUTUMN, WINTER, ALL are valid values for seasons. clim_multiyear = clim.multiyear_averages(nemo[[\u0026#39;temperature\u0026#39;,\u0026#39;ssh\u0026#39;]], seasons.ALL, time_var=\u0026#39;time\u0026#39;, time_dim=\u0026#39;t_dim\u0026#39;) # Or explicitly defining specific month periods. # A list of tuples defining start and end month integers. The start months should be in chronological order. # (you may need to read/load the data again if it gives an error) nemo = coast.Gridded(fn_data=fn_nemo_data, fn_domain=fn_nemo_domain, config=fn_config_t_grid, multiple=True) month_periods = [(1,2), (12,2)] # Specifies January -\u0026gt; February and December -\u0026gt; February for each year of data.  clim_multiyear = clim.multiyear_averages(nemo[[\u0026#39;temperature\u0026#39;,\u0026#39;ssh\u0026#39;]], month_periods , time_var=\u0026#39;time\u0026#39;, time_dim=\u0026#39;t_dim\u0026#39;) Below shows the structure of a dataset returned from this method:\n\u0026lt;xarray.Dataset\u0026gt; Dimensions: (y_dim: 375, x_dim: 297, z_dim: 51, year_period: 2) Coordinates: longitude (y_dim, x_dim) float32 -19.89 -19.78 ... 12.89 13.0 latitude (y_dim, x_dim) float32 40.07 40.07 40.07 ... 65.0 65.0 depth_0 (z_dim, y_dim, x_dim) float32 0.5 0.5 0.5 ... 50.5 50.5 * year_period (year_period) MultiIndex - year_period_level_0 (year_period) int64 2006 2007 - year_period_level_1 (year_period) object 'Dec-Feb' 'Jan-Feb' Dimensions without coordinates: y_dim, x_dim, z_dim Data variables: temperature (year_period, z_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 51, 375, 297), meta=np.ndarray\u0026gt; ssh (year_period, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 375, 297), meta=np.ndarray\u0026gt; Data can be accessed by selecting on the year-period MultiIndex:\n# Selecting temperature data variable based on year: clim_multiyear.sel(year_period=(2006))[\u0026#39;temperature\u0026#39;] # Selecting temperature data variable based on year and period: clim_multiyear.sel(year_period=(2006,\u0026#39;Dec-Feb\u0026#39;))[\u0026#39;temperature\u0026#39;] ","excerpt":"This is a demonstration script for using the Climatology object in the COAsT package. This object …","ref":"/COAsT/docs/examples/climatology/","title":"Climatology"},{"body":"This page will walk you though a simple setup for hugo extended - which is needed if want to view any changes you make to this site locally.\nFor more details please read this.\nInstallation Manual  Download hugo extended from GitHub Unzip into preferred location (I use C:\\hugo) Add to OS PATH  optional but makes usage easier    Via a Package Manager On Windows you can use Chocolately to install with:\nchoco install hugo-extended Or on macOS/Linux you can use Homebrew to install with:\nbrew install hugo Try it out! You should now be able to try the following in a terminal\n$ hugo --help if you have cloned the COAsT-site repo you should also now be able to;\n$ cd COAsT-site $ hugo server the above will start a local hugo powered version of the website. you can edit any of the files under /content and see your changes at http://localhost:1313/COAsT/\n","excerpt":"This page will walk you though a simple setup for hugo extended - which is needed if want to view …","ref":"/COAsT/docs/contributing-docs/hugo/","title":"setting up Hugo"},{"body":"Overview INDEXED type class for storing data from a CTD Profile (or similar down and up observations). The structure of the class is based around having discrete profile locations with independent depth dimensions and coords. The class dataset should contain two dimensions:\n\u0026gt; id_dim :: The profiles dimension. Each element of this dimension contains data (e.g. cast) for an individual location. \u0026gt; z_dim :: The dimension for depth levels. A profile object does not need to have shared depths, so NaNs might be used to pad any depth array.  Alongside these dimensions, the following minimal coordinates should also be available:\n\u0026gt; longitude (id_dim) :: 1D array of longitudes, one for each id_dim \u0026gt; latitude (id_dim) :: 1D array of latitudes, one for each id_dim \u0026gt; time (id_dim) :: 1D array of times, one for each id_dim \u0026gt; depth (id_dim, z_dim) :: 2D array of depths, with different depth levels being provided for each profile. Note that these depth levels need to be stored in a 2D array, so NaNs can be used to pad out profiles with shallower depths. \u0026gt; id_name (id_dim) :: [Optional] Name of id_dim/case or id_dim number.  Example Useage Below is a description of the available example scripts for this class as well as an overview of validation using Profile and ProfileAnalysis.\nExample Scripts Please see COAsT/example_scripts/profile_validation for some scripts which demonstrate how to use the Profile and ProfileAnalysis classes for model validation.\n  analysis_preprocess_en4.py : If you\u0026rsquo;re using EN4 data, this kind of script might be your first step for analysis.\n  analysis_extract_and_compare.py: This script shows you how to extract the nearest model profiles, compare them with EN4 observations and get errors throughout the vertical dimension and averaged in surface and bottom zones\n  analysis_extract_and_compare_single_process.py: This script does the same as number 2. However, it is modified slightly to take a command line argument which helps it figure out which dates to analyse. This means that this script can act as a template for jug type parallel processing on, e.g. JASMIN.\n  analysis_mask_means.py: This script demonstrates how to use boolean masks to obtain regional averages of profiles and errors.\n  analysis_average_into_grid_boxes.py: This script demonstrates how to average the data inside a Profile object into regular grid boxes and seasonal climatologies.\n  Basic useage We can create a new Profile object easily:\nprofile = coast.Profile() Currently, this object is empty, and contains no dataset. There are some reading routines currently available in Profile for reading EN4 or WOD data files. These can be used to easily read data into your new profile object:\n# Read WOD data into profile object (OVERWRITES DATASET) profile.read_wod( filename ) # Read EN4 data into profile object profile.read_en4( filename ) Alternatively, you can pass an xarray.dataset straight to Profile:\nprofile = coast.Profile( dataset = your_dataset, config = config_file [opt] ) We can do some simple spatial and temporal manipulations of this data:\n# Cut out a geographical box profile = profile.subset_indices_lonlat_box(lonbounds = [-15, 15], latbounds = [45, 65]) # Cut out a time window profile = profile.time_slice( date0 = datetime(2004, 1, 1), date1 = datetime(2005,1,1)) If you are using EN4 data, you can use the process_en4() routine to apply quality control flags to the data (replacing with NaNs):\nprocessed_profile = profile.process_en4() Direct Model Comparison There are a number of routines available for interpolating in the horizontal, vertical and in time to do direct comparisons of model and profile data. Profile.obs_operator will do a nearest neighbour spatial interpolation of the data in a Gridded object to profile latitudes/longitudes. It will also do a custom time interpolation. For example:\n# Create gridded object: nemo = coast.Gridded(fn_dat, fn_dom, multiple=True, config=fn_cfg_nemo) # Create a landmask array in Gridded nemo.dataset[\u0026quot;landmask\u0026quot;] = nemo.dataset.bottom_level == 0 nemo.dataset = nemo.dataset.rename({\u0026quot;depth_0\u0026quot;: \u0026quot;depth\u0026quot;}) # Use obs operator for horizontal remapping of Gridded onto Profile. model_profiles = profile.obs_operator(nemo) In the above example we added a landmask variable to the Gridded dataset. When this is present, the obs_operator will use this to interpolation to the nearest wet point. If not present, it will just take the nearest grid point.\nNow that we have interpolated the model onto Profiles, we have a new Profile object called model_profiles. This can be used to do some comparisons with our original processed_profile object, which we created above. First lets make our ProfileAnalysis object:\nanalysis = coast.ProfileAnalysis() We can use ProfileAnalysis.interpolate_vertical to interpolate all variables within a Profile object. This can be done onto a set of reference depths or, matching another object\u0026rsquo;s depth coordinates by passing another profile object. Let\u0026rsquo;s interpolate our model profiles onto observations depths, then interpolate both onto a set of reference depths:\n# Interpolate model profiles onto observation depths model_profiles_interp = analysis.interpolate_vertical(model_profiles, profile, interp_method=\u0026quot;linear\u0026quot;) # Vertical interpolation of model profiles to reference depths model_profiles_interp = panalysis.interpolate_vertical(model_profiles_interp, ref_depth) # Interpolation of obs profiles to reference depths profile_interp = analysis.interpolate_vertical(profile, ref_depth) Now that we have two Profile objects that are horizontally and vertically comparable, we can use difference() to get some basic errors:\ndifferences = analysis.difference(profile_interp, model_profiles_interp) This will return a new Profile object that contains the variable difference, absolute differences and square differences at all depths and means for each profile.\nLayer Averaging We can use the Profile object to get mean values between specific depth levels or for some layer above the bathymetric depth. The former can be done using ProfileAnalysis.depth_means(), for example the following will return a new Profile object containing the means of all variables between 0m and 5m:\nprofile_surface = analysis.depth_means(profile, [0, 5]) This can be done for any arbitrary depth layer defined by two depths. In some cases it may be that one of the depth levels is not defined by a constant, e.g. when calculating bottom means. In this case you may want to calculate averages in some layer above the seabed. This can be done using ProfileAnalysis.bottom_means(). For example:\nbottom_height = [10, 30, 100] # Use bottom heights of 10m, 30m and 100m for... bottom_thresh = [100, 500, np.inf] # ...depths less than 100m, 500m and infinite profile_bottom = analysis.bottom_means(profile, bottom_height, bottom_thresh) This will calculate bottom means differently depending upon the actualy depth. For depths less than 100m, it will take the average of the bottom 10m. For less than 500 m (and greater than 100m), it will take the average of the bottom 30m. And so on. This routine will look for a variable in the input Profile called bathymetry. If this is not present you will need to insert it yourself, e.g:\nprofile.dataset[\u0026quot;bathymetry\u0026quot;] = ([\u0026quot;id_dim\u0026quot;], obs_bathymetry_array) **NOTE1: The bathymetry variable does not actually need to contain bathymetric depths, it can also be used to calculate means above any non-constant surface. For example, it could be mixed layer depth.\n**NOTE2: This can be done for any Profile object. So, you could use this workflow to also average a Profile derived from the difference() routine.\nRegional (Mask) Averaging We can use Profile in combination with MaskMaker to calculate averages over regions defined by masks. For example, to get the mean errors in the North Sea. Start by creating a list of boolean masks we would like to use:\nmm = coast.MaskMaker() # Define Regional Masks regional_masks = [] bath = nemo.dataset.bathymetry.values # Add regional mask for whole domain regional_masks.append(np.ones(lon.shape)) # Add regional mask for North Sea regional_masks.append(mm.region_def_nws_north_sea(lon, lat, bath)) region_names = [\u0026quot;whole_domain\u0026quot;,\u0026quot;north_sea\u0026quot;,] Next, we must make these masks into datasets using MaskMaker.make_mask_dataset. Masks should be 2D datasets defined by booleans. In our example here we have used the latitude/longitude array from the nemo object, however it can be defined however you like.\nmask_list = mm.make_mask_dataset(lon, lat, regional_masks) Then we use ProfileAnalysis.determine_mask_indices to figure out which profiles in a Profile object lie within each regional mask:\nmask_indices = analysis.determine_mask_indices(profile, mask_list) This returns an object called mask_indices, which is required to pass to ProfileAnalysis.mask_means(). This routine will return a new xarray dataset containing averaged data for each region:\nmask_means = analysis.mask_means(profile, mask_indices) Gridding Profile Data If you have large amount of profile data you may want to average it into grid boxes to get, for example, mean error maps or climatologies. This can be done using ProfileAnalysis.average_into_grid_boxes().\nWe can create a gridded dataset of all the data using:\ngrid_lon = np.arange(-15, 15, 0.5) grid_lat = np.arange(45, 65, 0.5) prof_gridded = analysis.average_into_grid_boxes(grid_lon, grid_lat) Alternatively, we can calculate averages for each season:\nprof_gridded_DJF = profile_analysis.average_into_grid_boxes(grid_lon, grid_lat, season=\u0026quot;DJF\u0026quot;, var_modifier=\u0026quot;_DJF\u0026quot;) prof_gridded_MAM = profile_analysis.average_into_grid_boxes(grid_lon, grid_lat, season=\u0026quot;MAM\u0026quot;, var_modifier=\u0026quot;_MAM\u0026quot;) prof_gridded_JJA = profile_analysis.average_into_grid_boxes(grid_lon, grid_lat, season=\u0026quot;JJA\u0026quot;, var_modifier=\u0026quot;_JJA\u0026quot;) prof_gridded_SON = profile_analysis.average_into_grid_boxes(grid_lon, grid_lat, season=\u0026quot;SON\u0026quot;, var_modifier=\u0026quot;_SON\u0026quot;) Here, season specifies which season to average over and var_modifier is added to the end of all variable names in the object\u0026rsquo;s dataset.\nThis function returns a new Gridded object. It also contains a new variable called grid_N, which stores how many profiles were averaged into each grid box. You may want to use this when using the analysis.\n","excerpt":"Overview INDEXED type class for storing data from a CTD Profile (or similar down and up …","ref":"/COAsT/docs/examples/profile/","title":"Profile"},{"body":"","excerpt":"","ref":"/COAsT/docs/reference/","title":"Reference"},{"body":"Overview This is an object for storage and manipulation of tide gauge data in a single dataset. This may require some processing of the observations such as interpolation to a common time step.\nThis object\u0026rsquo;s dataset should take the form (as with Timeseries):\nDimensions: id_dim : The locations dimension. Each time series has an index time : The time dimension. Each datapoint at each port has an index Coordinates: longitude (id_dim) : Longitude values for each port index latitude (id_dim) : Latitude values for each port index time (time) : Time values for each time index (datetime) id_name (id_dim) : Name of index, e.g. port name or mooring id.  An example data variable could be ssh, or ntr (non-tidal residual). This object can also be used for other instrument types, not just tide gauges. For example moorings.\nEvery id index for this object should use the same time coordinates. Therefore, timeseries need to be aligned before being placed into the object. If there is any padding needed, then NaNs should be used. NaNs should also be used for quality control/data rejection.\nExample Useage Please see COAsT/example_scripts/tidegauge_validation for some working example scripts for using the Tidegauge and TidegaugeAnalysis classes.\nTo get started you can use example data detailed in the Getting Started section:\nimport coast dn_files = \u0026quot;./example_files/\u0026quot; fn_dom = dn_files + \u0026quot;coast_example_nemo_domain.nc\u0026quot; fn_dat = dn_files + \u0026quot;coast_example_nemo_data.nc\u0026quot; fn_config = \u0026quot;./config/example_nemo_grid_t.json\u0026quot; fn_multigauge = dn_files + \u0026quot;tg_amm15.nc\u0026quot; fn_tidegauge = dn_files + \u0026quot;tide_gauges/lowestoft-p024-uk-bodc\u0026quot; nemo = coast.Gridded(fn_dat, fn_dom, config=fn_config) Reading and manipulation We can create our empty tidegauge object:\ntidegauge = coast.Tidegauge() The Tidegauge class contains multiple methods for reading different typical tidegauge formats. This includes reading from the GESLA and BODC databases. To read a gesla file between two dates, we can use:\nimport datetime date0 = datetime.datetime(2007,1,10) date1 = datetime.datetime(2007,1,12) tidegauge.read_gesla_v3(fn_tidegauge, date_start = date0, date_end = date1) For the rest of our examples, we will use data from multiple tide gauges on the same time dimension, read in from a simple netCDF file:\nimport xarray as xr dataset = xr.open_dataset( fn_multigauge ) tidegauge = coast.Tidegauge(dataset) tidegauge.dataset = tidegauge.dataset.set_coords('time') Tidegauge has ready made quick plotting routines for viewing time series and tide gauge location. To look at the tide gauge location:\nfig, ax = tidegauge.plot_on_map() Or to look at a time series of the sea_level variable:\nid=1 tidegauge.dataset.ssh[id].rename({'t_dim':'time'}).plot() # rename time dimension to enable automatic x-axis labelling plt.show() Note that start and end dates can also be specified for plot_timeseries().\nWe can do some simple spatial and temporal manipulations of this data:\n# Cut out a geographical box tidegauge = tidegauge.subset_indices_lonlat_box(lonbounds = [-5, 0], latbounds = [50, 55]) # Cut out a time window tidegauge = tidegauge.time_slice( date0 = datetime.datetime(2007, 1, 1), date1 = datetime.datetime(2007,1,31)) We can extract just some variables using, e.g.:\nnemo.dataset = nemo.dataset.rename({\u0026quot;depth_0\u0026quot;: \u0026quot;depth\u0026quot;}) nemo.dataset = nemo.dataset[[\u0026quot;ssh\u0026quot;, \u0026quot;landmask\u0026quot;]] Direct model comparison Before comparing our observations to the model, we will interpolate a model variable to the same time and geographical space as the tidegauge. This is done using the `obs_operator()`` method:\n# Suppose we have created a Gridded object called nemo tidegauge_from_model = tidegauge.obs_operator(nemo, time_interp='nearest') Doing this has created a new interpolated tidegauge called tidegauge_from_model Take a look at tidegauge_from_model.dataset to see for yourself. If a landmask variable is present in the Gridded dataset then the nearest wet points will be taken. Otherwise, just the nearest point is taken. If landmask is required but not present you will need to insert it into the dataset yourself. For nemo data, you could use the bottom_level or mbathy variables to do this. E.g:\nnemo.dataset[\u0026quot;landmask\u0026quot;] = nemo.dataset.bottom_level == 0 For a good comparison, we would like to make sure that both the observed and modelled Tidegauge objects contain the same missing values. TidegaugeAnalysis contains a routine for ensuring this. First create our analysis object:\nanalysis = coast.TidegaugeAnalysis() Then use the match_missing_values() routine:\nobs_ssh, model_ssh = analysis.match_missing_values(tidegauge.dataset.ssh, tidegauge_from_model.dataset.ssh) Although we input data arrays to the above routine, it returns two new Tidegauge objects. Now you have equivalent and comparable sets of time series that can be easily compared.\nThe difference() routine will calculate differences, absolute_differences and squared differenced for all variables:\ndiff = analysis.difference(obs_ssh.dataset, model_ssh.dataset) We can then easily get mean errors, MAE and MSE\nmean_stats = diff.dataset.mean(dim=\u0026quot;t_dim\u0026quot;, skipna=True) Harmonic Analysis \u0026amp; Non-tidal Residuals The Tidegauge object contains some routines which make harmonic analysis and the calculation/comparison of non-tidal residuals easier. Harmonic analysis is done using the utide package. Please see here for more info.\nFirst we can use the TidegaugeAnalysis class to do a harmonic analysis. Suppose we have two Tidegauge objects called tidegauge and tidegauge_from_model generated from tidegauge observations and model simulations respectively.\nWe can subtract means from all time series:\nanalysis = coast.TidegaugeAnalysis() obs_new, model_new = analysis.match_missing_values(tidegauge.dataset.ssh, tidegauge_from_model.dataset.ssh) Then subtract means from all the time series\nmodel_timeseries = analysis.demean_timeseries(model_new.dataset) obs_timeseries = analysis.demean_timeseries(obs_new.dataset) Then we can apply the harmonic analysis (though the example data is too short for this example to be that meaningful):\nha_mod = analysis.harmonic_analysis_utide(model_timeseries.dataset.ssh, min_datapoints = 1) ha_obs = analysis.harmonic_analysis_utide(obs_timeseries.dataset.ssh, min_datapoints = 1) The harmonic_analysis_utide routine returns a list of utide structure object, one for each id_dim in the Tidegauge object. It can be passed any of the arguments that go to utide. It also has an additional argument min_datapoints which determines a minimum number of data points for the harmonics analysis. If a tidegauge id_dim has less than this number, it will not return an analysis.\nNow, create new TidegaugeMultiple objects containing reconstructed tides:\ntide_mod = analysis.reconstruct_tide_utide(model_timeseries.dataset.time, ha_mod) tide_obs = analysis.reconstruct_tide_utide(obs_timeseries.dataset.time, ha_obs) Get new TidegaugeMultiple objects containing non tidal residuals:\nntr_mod = analysis.calculate_non_tidal_residuals(model_timeseries.dataset.ssh, tide_mod.dataset.reconstructed) ntr_obs = analysis.calculate_non_tidal_residuals(obs_timeseries.dataset.ssh, tide_obs.dataset.reconstructed) By default, this routines will apply scipy.signal.savgol_filter to the non-tidal residuals to remove some noise. This can be switched off using apply_filter = False.\nThe Doodson X0 filter is an alternative way of estimating non-tidal residuals:\ndx0 = analysis.doodson_x0_filter(tidegauge.dataset, \u0026quot;ssh\u0026quot;) This will return a new Tidegauge() object containing filtered ssh data.\n6. Threshold Statistics This is a simple extreme value analysis of whatever data you use. It will count the number of peaks and the total time spent over each threshold provided. It will also count the numbers of daily and monthly maxima over each threshold. To this, a Tidegauge object and an array of thresholds (in metres) should be passed:\nimport numpy as np thresh_mod = analysis.threshold_statistics(ntr_mod.dataset, thresholds=np.arange(0, 2, 0.2)) thresh_obs = analysis.threshold_statistics(ntr_obs.dataset, thresholds=np.arange(0, 2, 0.2)) ","excerpt":"Overview This is an object for storage and manipulation of tide gauge data in a single dataset. This …","ref":"/COAsT/docs/examples/tidegauge/","title":"Tidegauge"},{"body":"In this tutorial we take a look the following Isobath Contour Methods:\n1. Extract isbath contour between two points 2. Plot contour on map 3. Calculate pressure along contour 4. Calculate flow across contour 5. Calculate pressure gradient driven flow across contour  Create a contour subset of the example dataset Load packages and define some file paths\nimport coast import matplotlib.pyplot as plt fn_nemo_dat_t = \u0026#34;./example_files/nemo_data_T_grid.nc\u0026#34; fn_nemo_dat_u = \u0026#34;./example_files/nemo_data_U_grid.nc\u0026#34; fn_nemo_dat_v = \u0026#34;./example_files/nemo_data_V_grid.nc\u0026#34; fn_nemo_dom = \u0026#34;./example_files/coast_example_nemo_domain.nc\u0026#34; # Configuration files describing the data files fn_config_t_grid = \u0026#34;./config/example_nemo_grid_t.json\u0026#34; fn_config_f_grid = \u0026#34;./config/example_nemo_grid_f.json\u0026#34; fn_config_u_grid = \u0026#34;./config/example_nemo_grid_u.json\u0026#34; fn_config_v_grid = \u0026#34;./config/example_nemo_grid_v.json\u0026#34; To extract isobath contour between two points and create contour object, first create a gridded object with the grid only.\nnemo_f = coast.Gridded(fn_domain=fn_nemo_dom, config=fn_config_f_grid) Then create a contour object on the 200m isobath\ncontours, no_contours = coast.Contour.get_contours(nemo_f, 200) Extract the indices for the contour in a specified box\ny_ind, x_ind, contour = coast.Contour.get_contour_segment(nemo_f, contours[0], [50, -10], [60, 3]) Extract the contour for the specified indices\ncont_f = coast.ContourF(nemo_f, y_ind, x_ind, 200) Plot contour on map plt.figure() coast.Contour.plot_contour(nemo_f, contour) plt.show()    Calculate pressure along contour Repeat the above procedure but on t-points\nnemo_t = coast.Gridded(fn_data=fn_nemo_dat_t, fn_domain=fn_nemo_dom, config=fn_config_t_grid) contours, no_contours = coast.Contour.get_contours(nemo_t, 200) y_ind, x_ind, contour = coast.Contour.get_contour_segment(nemo_t, contours[0], [50, -10], [60, 3]) cont_t = coast.ContourT(nemo_t, y_ind, x_ind, 200) Now construct pressure along this contour segment\ncont_t.construct_pressure(1027) This creates cont_t.data_contour.pressure_s and cont_t.data_contour.pressure_h_zlevels fields.\nCalculate flow across contour Create the contour segement on f-points again\nnemo_f = coast.Gridded(fn_domain=fn_nemo_dom, config=fn_config_f_grid) nemo_u = coast.Gridded(fn_data=fn_nemo_dat_u, fn_domain=fn_nemo_dom, config=fn_config_u_grid) nemo_v = coast.Gridded(fn_data=fn_nemo_dat_v, fn_domain=fn_nemo_dom, config=fn_config_v_grid) contours, no_contours = coast.Contour.get_contours(nemo_f, 200) y_ind, x_ind, contour = coast.Contour.get_contour_segment(nemo_f, contours[0], [50, -10], [60, 3]) cont_f = coast.ContourF(nemo_f, y_ind, x_ind, 200) To calculate the flow across the contour, pass u- and v- gridded velocity objects\ncont_f.calc_cross_contour_flow(nemo_u, nemo_v) This creates fields cont_f.data_cross_flow.normal_velocities and cont_f.data_cross_flow.depth_integrated_normal_transport\nCalculate pressure gradient driven flow across contour The \u0026ldquo;calc_geostrophic_flow()\u0026rdquo; operates on f-grid objects and requires configuration files for the u- and v- grids\ncont_f.calc_geostrophic_flow(nemo_t, config_u=fn_config_u_grid, config_v=fn_config_v_grid, ref_density=1027) This constructs:\n cont_f.data_cross_flow.normal_velocity_hpg cont_f.data_cross_flow.normal_velocity_spg cont_f.data_cross_flow.transport_across_AB_hpg cont_f.data_cross_flow.transport_across_AB_spg ","excerpt":"In this tutorial we take a look the following Isobath Contour Methods:\n1. Extract isbath contour …","ref":"/COAsT/docs/examples/contour/","title":"Contour subsetting"},{"body":"In this tutorial we take a look at subsetting the model data along a transect (a custom straight line) and creating some bespoke diagnostics along it. We look at:\n1. Creating a Transect object, defined between two points. 2. Plotting data along a transect. 3. Calculating flow normal to the transect  Create a transect subset of the example dataset Load packages and define some file paths\nimport coast import xarray as xr import matplotlib.pyplot as plt fn_nemo_dat_t = \u0026#34;./example_files/nemo_data_T_grid.nc\u0026#34; fn_nemo_dat_u = \u0026#34;./example_files/nemo_data_U_grid.nc\u0026#34; fn_nemo_dat_v = \u0026#34;./example_files/nemo_data_V_grid.nc\u0026#34; fn_nemo_dom = \u0026#34;./example_files/coast_example_nemo_domain.nc\u0026#34; # Configuration files describing the data files fn_config_t_grid = \u0026#34;./config/example_nemo_grid_t.json\u0026#34; fn_config_f_grid = \u0026#34;./config/example_nemo_grid_f.json\u0026#34; fn_config_u_grid = \u0026#34;./config/example_nemo_grid_u.json\u0026#34; fn_config_v_grid = \u0026#34;./config/example_nemo_grid_v.json\u0026#34; Load data variables that are on the NEMO t-grid\nnemo_t = coast.Gridded( fn_data = fn_nemo_dat_t, fn_domain = fn_nemo_dom, config=fn_config_t_grid ) Now create a transect between the points (54 N 15 W) and (56 N, 12 W) using the coast.TransectT object. This needs to be passed the corresponding Gridded object and transect end points. The model points closest to these coordinates will be selected as the transect end points.\ntran_t = coast.TransectT( nemo_t, (54,-15), (56,-12) ) tran_t.data where r_dim is the dimension along the transect. It is simple to plot a scalar such as temperature along the transect:\ntemp_mean = tran_t.data.temperature.mean(dim=\u0026#39;t_dim\u0026#39;) temp_mean.plot.pcolormesh(y=\u0026#39;depth_0\u0026#39;, yincrease=False )    Flow across the transect With NEMO’s staggered grid, the first step is to define the transect on the f-grid so that the velocity components are between f-points. We do not need any model data on the f-grid, just the grid information, so create a nemo f-grid object\nnemo_f = coast.Gridded( fn_domain = fn_nemo_dom, config=fn_config_f_grid ) and a transect on the f-grid\ntran_f = coast.TransectF( nemo_f, (54,-15), (56,-12) ) tran_f.data We also need the i- and j-components of velocity so (lazy) load the model data on the u- and v-grid grids\nnemo_u = coast.Gridded( fn_data = fn_nemo_dat_u, fn_domain = fn_nemo_dom, config=fn_config_u_grid ) nemo_v = coast.Gridded( fn_data = fn_nemo_dat_v, fn_domain = fn_nemo_dom, config=fn_config_v_grid ) Now we can calculate the flow across the transect with the method\ntran_f.calc_flow_across_transect(nemo_u,nemo_v) The flow across the transect is stored in a new dataset where the variables are all defined at the points between f-points.\ntran_f.data_cross_tran_flow For example, to plot the time averaged velocity across the transect, we can plot the ‘normal_velocities’ variable\ncross_velocity_mean = tran_f.data_cross_tran_flow.normal_velocities.mean(dim=\u0026#39;t_dim\u0026#39;) cross_velocity_mean.rolling(r_dim=2).mean().plot.pcolormesh(yincrease=False,y=\u0026#39;depth_0\u0026#39;,cbar_kwargs={\u0026#39;label\u0026#39;: \u0026#39;m/s\u0026#39;})    or the volume transport across the transect, we can plot the ‘normal_transports’ variable\ncross_transport_mean = tran_f.data_cross_tran_flow.normal_transports.mean(dim=\u0026#39;t_dim\u0026#39;) cross_transport_mean.rolling(r_dim=2).mean().plot() plt.ylabel(\u0026#39;Sv\u0026#39;)    ","excerpt":"In this tutorial we take a look at subsetting the model data along a transect (a custom straight …","ref":"/COAsT/docs/examples/transect/","title":"Transect subsetting"},{"body":"A short script to install COAsT in a conda environment, download and run some build tests.\n# Fresh build module load anaconda/3-5.1.0 # or whatever it takes to activate conda yes | conda env remove --name test_env yes | conda create -n test_env python=3.8 # create a new environment conda activate test_env yes | conda install -c conda-forge -c bodc coast yes | conda install -c conda-forge cartopy=0.18.0 # used for some of the map plotting # Download bits and bobs rm -rf coast_test mkdir coast_test cd coast_test git clone https://github.com/British-Oceanographic-Data-Centre/COAsT.git wget -c https://linkedsystems.uk/erddap/files/COAsT_example_files/COAsT_example_files.zip \u0026amp;\u0026amp; unzip COAsT_example_files.zip ln -s COAsT/unit_testing/ . ln -s COAsT_example_files example_files # Run unit tests python COAsT/unit_testing/unit_test.py \u0026gt; coast_test.txt ## If OK then clean up cd .. rm -rf coast_test ","excerpt":"A short script to install COAsT in a conda environment, download and run some build tests.\n# Fresh …","ref":"/COAsT/docs/contributing_package/build_test/","title":"Build test"},{"body":"To date the workflow has been to unit test anything and everything that goes into the develop branch and then periodically push to master less frequently and issue a new github release.\nWith the push to master Git Actions build the conda and pip packages and the package receives a zenodo update (https://zenodo.org/account/settings/github/repository/British-Oceanographic-Data-Centre/COAsT) and DOI.\n1. Push to master Any push to master initiates the Git Actions to build and release the package. It is advisable then to prepare the release in develop and only ever pull into master from develop. (Pulling from master to develop could bring unexpected Git Actions to develop). In order for the package builds to work the version of the package must be unique. The version of the package is set in file setup.py. E.g. shown as 2.0.1 below:\n# setup.py ... PACKAGE = SimpleNamespace(**{ \u0026#34;name\u0026#34;: \u0026#34;COAsT\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;2.0.1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;This is the Coast Ocean Assessment Tool\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://www.bodc.ac.uk\u0026#34;, \u0026#34;download_url\u0026#34;: \u0026#34;https://github.com/British-Oceanographic-Data-Centre/COAsT/\u0026#34;, .... Package version also appears in CITATION.cff file, which therefore also needs updating. E.g.:\n... title: British-Oceanographic-Data-Centre/COAsT: v2.0.1 version: v2.0.1 date-released: 2022-04-07 Version numbering follows the semantic versioning convention. Briefly, given a version number MAJOR.MINOR.PATCH, increment the:\n MAJOR version when you make incompatible API changes, MINOR version when you add functionality in a backwards compatible manner, and PATCH version when you make backwards compatible bug fixes. Additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format.  2. Issue new release Then issue a new release, with the new version label, and annotate the major changes.\n","excerpt":"To date the workflow has been to unit test anything and everything that goes into the develop branch …","ref":"/COAsT/docs/contributing_package/push_to_master/","title":"Push to master"},{"body":"Using COAsT to compute the Empirical Orthogonal Functions (EOFs) of your data.\nLoad data and compute EOFs Load packages and define some file paths\nimport coast import xarray as xr import matplotlib.pyplot as plt fn_nemo_dat_t = \u0026#34;./example_files/nemo_data_T_grid.nc\u0026#34; fn_nemo_dom = \u0026#34;./example_files/COAsT_example_NEMO_domain.nc\u0026#34; fn_nemo_config = \u0026#34;./config/example_nemo_grid_t.json\u0026#34; Load data variables that are on the NEMO t-grid\nnemo_t = coast.Gridded( fn_data = fn_nemo_dat_t, fn_domain = fn_nemo_dom, config = fn_nemo_config ) For a variable (or subset of a variable) with two spatial dimensions and one temporal dimension, i.e. (x,y,t), the EOFs, temporal projections and variance explained can be computed by calling the ‘eofs’ method, and passing in the ssh DataArray as an argument. For example, for the sea surface height field, we can do\neof_data = coast.compute_eofs( nemo_t.dataset.ssh ) The method returns an xarray dataset that contains the EOFs, temporal projections and variance as DataArrays\neof_data The variance explained of the first four modes is\neof_data.variance.sel(mode=[1,2,3,4]) And the EOFs and temporal projections can be quick plotted:\neof_data.EOF.sel(mode=[1,2,3,4]).plot.pcolormesh(col=\u0026#39;mode\u0026#39;,col_wrap=2,x=\u0026#39;longitude\u0026#39;,y=\u0026#39;latitude\u0026#39;)    eof_data.temporal_proj.sel(mode=[1,2,3,4]).plot(col=\u0026#39;mode\u0026#39;,col_wrap=2,x=\u0026#39;time\u0026#39;)    The more exotic hilbert complex EOFs can also be computed to investigate the propagation of variability, for example:\nheof_data = coast.compute_hilbert_eofs( nemo_t.dataset.ssh ) heof_data now with the modes expressed by their amplitude and phase, the spatial propagation of the variability can be examined through the EOF_phase.\n","excerpt":"Using COAsT to compute the Empirical Orthogonal Functions (EOFs) of your data.\nLoad data and compute …","ref":"/COAsT/docs/examples/eofs/","title":"Empirical Orthogonal Functions"},{"body":"A demonstration of pycnocline depth and thickness diagnostics. The first and second depth moments of stratification are computed as proxies for pycnocline depth and thickness, suitable for a nearly two-layer fluid.\nNote that in the AMM7 example data the plots are not particularly spectacular as the internal tide is poorly resolved at 7km.\nimport coast import numpy as np import os import xarray as xr import dask import matplotlib.pyplot as plt import matplotlib.colors as colors # colormap fiddling Load in the Data set some aliases and load the t-grid data:\n# set some paths config = \u0026#39;AMM7\u0026#39; fn_nemo_grid_t_dat = \u0026#39;./example_files/nemo_data_T_grid_Aug2015.nc\u0026#39; fn_nemo_dom = \u0026#39;./example_files/coast_example_nemo_domain.nc\u0026#39; config_t = \u0026#34;./config/example_nemo_grid_t.json\u0026#34; config_w = \u0026#34;./config/example_nemo_grid_w.json\u0026#34; Create a Gridded object and load in the data:\nnemo_t = coast.Gridded(fn_nemo_grid_t_dat, fn_nemo_dom, config=config_t) The stratification variables are computed as centred differences of the t-grid variables. These will become w-grid variables. So, create an empty w-grid object, to store stratification. Note how we do not pass a NEMO data file for this load.\nnemo_w = coast.Gridded(fn_domain=fn_nemo_dom, config=config_w) Subset the Domain We are not interested in the whole doman so it is computationally efficient to subset the data for the region of interest. Here we will look at the North Sea between (51N: 62N) and (-4E:15E). We will great subset objects for both the t- and w-grids:\nind_2d = nemo_t.subset_indices([51,-4], [62,15]) nemo_nwes_t = nemo_t.isel(y_dim=ind_2d[0], x_dim=ind_2d[1]) #nwes = northwest european shelf ind_2d = nemo_w.subset_indices([51,-4], [62,15]) nemo_nwes_w = nemo_w.isel(y_dim=ind_2d[0], x_dim=ind_2d[1]) #nwes = northwest european shelf nemo_nwes_t.dataset Diagnostic calculations and plotting We can use a COAsT method to construct the in-situ density:\nnemo_nwes_t.construct_density( eos=\u0026#39;EOS10\u0026#39; ) Then we construct stratification using a COAsT method to take the vertical derivative. Noting that the inputs are on t-pts and the outputs are on w-pt\nnemo_nwes_w = nemo_nwes_t.differentiate( \u0026#39;density\u0026#39;, dim=\u0026#39;z_dim\u0026#39;, out_var_str=\u0026#39;rho_dz\u0026#39;, out_obj=nemo_nwes_w ) # --\u0026gt; sci_nwes_w.rho_dz This has created a variable called nemo_nwes_w.rho_dz.\nWe can now use the InternalTide class to construct the first and second moments (over depth) of density. In the limit of an idealised two-layer fluid these converge to the depth and thickness of the interface. I.e. the pycnocline depth and thickness respectively.\n#%% Create internal tide diagnostics object IT = coast.InternalTide(nemo_nwes_t, nemo_nwes_w) #%% Construct pycnocline variables: depth and thickness IT.construct_pycnocline_vars( nemo_nwes_t, nemo_nwes_w ) Finally we plot pycnocline variables (depth and thickness) using an InternalTide method:\nIT.quick_plot()    ","excerpt":"A demonstration of pycnocline depth and thickness diagnostics. The first and second depth moments of …","ref":"/COAsT/docs/examples/stratification/","title":"Stratification diagnostics"},{"body":"This is a demonstration for how to export intermediate data from COAsT to netCDF files for later analysis or storage. The tutorial showcases the xarray.to_netcdf() method. http://xarray.pydata.org/en/stable/generated/xarray.Dataset.to_netcdf.html\nLoad in example data Begin by importing coast and other packages\nimport coast and by defining some file paths\nfn_nemo_dat = \u0026#34;./example_files/coast_example_nemo_data.nc\u0026#34; fn_nemo_dom = \u0026#34;./example_files/coast_example_nemo_domain.nc\u0026#34; config = \u0026#34;./config/example_nemo_grid_t.json\u0026#34; ofile = \u0026#34;example_export_output.nc\u0026#34; # The target filename for output We need to load in a Gridded object to get started.\nnemo = coast.Gridded(fn_nemo_dat, fn_nemo_dom, config=config) Export to netCDF We can export the whole xr.DataSet to a netCDF file\nnemo.dataset.to_netcdf(ofile, mode=\u0026#34;w\u0026#34;, format=\u0026#34;NETCDF4\u0026#34;) Other file formats are available. From the documentation:\nformat: NETCDF4: Data is stored in an HDF5 file, using netCDF4 API features. NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only netCDF 3 compatible API features. NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format, which fully supports 2+ GB files, but is only compatible with clients linked against netCDF version 3.6.0 or later. NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not handle 2+ GB files very well. Similarly other modes are available; \u0026lsquo;w\u0026rsquo; (write) is the default. From the documentation:\nmode ({\u0026quot;w\u0026quot;, \u0026quot;a\u0026quot;}, default: \u0026quot;w\u0026quot;) – Write (‘w’) or append (‘a’) mode. If mode=’w’, any existing file at this location will be overwritten. If mode=’a’, existing variables will be overwritten. Alternatively a single variable (an xr.DataArray object) can be exported\nnemo.dataset[\u0026#39;temperature\u0026#39;].to_netcdf(ofile, format=\u0026#34;NETCDF4\u0026#34;) Similarly xr.DataSets collections of variables or xr.DataArray variables can be exported to netCDF for objects in the TRANSECT, TIDEGAUGE, etc classes.\nCheck the exported file Check the exported file is as you expect. Perhaps using ncdump -h example_export_output.nc Or load file as see that the xarray structure is preserved.\nimport xarray as xr object = xr.open_dataset(ofile) object.close() # close file associated with this object ","excerpt":"This is a demonstration for how to export intermediate data from COAsT to netCDF files for later …","ref":"/COAsT/docs/examples/export_netcdf/","title":"Export to netCDF"},{"body":"Unit testing is a key component of the COAsT contribution workflow. A \u0026ldquo;unit test\u0026rdquo; is a small, singular piece of code that tests an element of COAsT to ensure that it works as expected. This might include a small check that a method actually runs without error and returns an expected value or dataset. There is then a sequence of unit tests which should, in theory, test all parts of the COAsT codebase. When pushing a change to COAsT, we can use this system to ensure that any changes that have been made do not break the existing package. If a modification results in a unit test failing then we must not merge changes into the develop or merge branches.\nThe COAsT unit testing system uses Python\u0026rsquo;s own unittest library. On this page, the system is outlined, alongside guidance on how to use it.\nThe unittest library Unittest is a library that comes with Python. You can find more information here.\nWe have used the TestSuite functionality of the library alongside the TestCase class. This works by creating a class that inherits from TestCase. Then, any method within this class acts as a unit test, so long as it begins with test_. We can create a Test Suite (just a collection of the TestCase classes) and then run it. When we do this, the package will search for all methods beginning with test_ and run them.\nSystem Overview There are several files that make up the unit testing system in COAsT:\n unit_testing/unit_test.py. This is the main unit testing file that controls the import of test classes as well as the creation and execution of the Test Suite. unit_testing/unit_test_files.py. This file simply contains paths which point towards example files, scripts and configs. These are small files that can be obtained here. Ideally, the example files should be in COAsT/example_files, the scripts in COAsT/example_scripts and the configs in COAsT/configs. It is easy to obtain these files from within a test file. Simply import unit_test_files as files at the top of a test file. A file can then be obtained anywhere by using files.file_name_variable. unit_testing/test_*.py. These are the files that contain the actual unit tests, inside the TestCase classes mentioned above. There is a well commented example/template file that comes with COAsT called test_TEMPLATE.py. Please see this file for a better idea of what these files look like.  Running the unit test You can run the unit testing system simply by running the unit_test.py file. For example, within Spyder you could use run unit_testing/unit_test.py. The script should be run from the main COAsT directory to work correctly. Be careful if using Spyder\u0026rsquo;s play button that your working directory is also the highest level COAsT directory. For the easiest time, your example files should be in COAsT/example_files, however you can put them anywhere you wish. If the unit test cannot find the files, it will prompt you to enter the path in the terminal.\nIf you are testing new additions to the system, you may not wish to run all tests. You can easily change which tests you are running by modifying the tests_to_run list variable in unit_testing/unit_test.py. Comment out the names of the test classes you do not wish to run (that\u0026rsquo;s all you have to do).\nAs the test runs, it will return the names of the test methods it is currently running and a message telling you whether it was ok, ERROR or FAILURE. When all tests are complete, it will tell you how many ERRORs and FAILUREs occurred and how long the test took. A successful unit testing run will have 0 ERRORS and 0 FAILURES. If there are errors, it will return information on which unit tests failed as well as traceback on the error raised.\nAdding your own unit test When changing COAsT, you should add your own unit tests for the features you have added. You can do this either by adding a method to an existing test class in one of the test_*.py files or by creating your own test class and adding unit tests to this. You should carefully consider whether your new features fit within the context of any of the existing test classes.\nIf creating a new test class, your workflow may look something like this:\n  Create new test_your_name.py file: You might want to start by copying test_TEMPLATE.py to a new filename of your choice. Delete the methods inside of it and rename to class from test_TEMPLATE to test_\u0026lt;\u0026gt;. The name of the class does not have to match the name of the file, but make sure you know the difference. Ensure this new file is saved in the COAsT/unit_testing directory.\n  Add unit tests to your file: You do this simply by creating new methods inside the class you created above. Each method that you want to act as a unit test must being with \u0026ldquo;test_\u0026quot;. You should name your test to appropriately reflect its function. If there are any problems with your unit test when running the system then the name of the failed method will be returned. Using a long descriptive name in this case is useful.\n  Add new test file to unit_test.py: You need to let the main unit_test.py script know about your new file and class. At the top of this file is a selection of import statement. Follow the other lines here by importing the names of your test class(es) from your file. For example, to import the test_TEMPLATE class from test_TEMPLATE.py, you can add the line from test_TEMPLATE.py import test_TEMPLATE. Next, you must add this imported classes to the tests_to_do list below. And that\u0026rsquo;s it!\n  Unit testing coverage Code coverage is a useful statistic that tells us how much of the package (and individual classes) are \u0026lsquo;covered\u0026rsquo; by the unit testing system. Coverage means what percentage of the code is \u0026lsquo;touched\u0026rsquo; by the unit testing system. This is not built in to the system used by COAsT, however it is easy to set up and use.\nThe Coverage python tool can be used to do this. For more information see here. The tool can be easily installed using pip:\npip install coverage\nThen we use the tool from the command line/terminal. From the main COAsT directory, enter:\ncoverage run unit_testing/unit_test.py This will run the unit test as usual, but with the coverage tool in the background (takes a little longer than usual). The result is a new output file in the COAsT directory called .coverage. We can\u0026rsquo;t read this but can transform it into a readable report using:\ncoverage report Which outputs statistics to the screen for each file and the whole package. For example:\n------------------------------------------------------------------ coast/__init__.py 28 0 100% coast/altimetry.py 169 139 18% coast/argos.py 24 14 42% coast/climatology.py 85 67 21% coast/coast.py 205 147 28% coast/config_parser.py 51 9 82% coast/config_structure.py 53 0 100% coast/contour.py 485 443 9% coast/crps_util.py 117 56 52% coast/eof.py 83 76 8% coast/general_utils.py 153 88 42% coast/glider.py 16 8 50% coast/gridded.py 377 267 29% coast/index.py 49 25 49% coast/internal_tide.py 81 72 11% coast/lagrangian.py 3 0 100% coast/logging_util.py 50 3 94% coast/mask_maker.py 66 46 30% coast/oceanparcels.py 16 8 50% coast/plot_util.py 129 94 27% coast/profile.py 345 314 9% coast/stats_util.py 67 43 36% coast/tidegauge.py 453 113 75% coast/tidegauge_analysis.py 189 15 92% coast/timeseries.py 35 29 17% coast/track.py 3 0 100% coast/transect.py 420 385 8% coast/xesmf_convert.py 32 21 34% ------------------------------------------------------------------ TOTAL 4851 3071 37% This table shows statistics for each file in the COAsT package. There are three key statistics returns: stmts, Miss and Cover. stmts tells us how many executable statements are reached, Miss tells us how many were missed and Cover tells us the percentage coverage.\nThere are plenty of extra features, such as outputting to various file types and flags which control the coverage calculations. See the website for more information.\n","excerpt":"Unit testing is a key component of the COAsT contribution workflow. A \u0026ldquo;unit test\u0026rdquo; is a …","ref":"/COAsT/docs/examples/unit_testing/","title":"Unit testing"},{"body":"Configuration file code can be found in coast/config within the COAsT github repository. This code is used internally within the package.\nConfiguration file usage Configuration files are passed into a COAsT class on the instantiation of a new object. For example the Gridded class __init__ method takes an argument config. This argument must be a String or Path object representing a path to the configuration file. E.g.\nconfig_file = \u0026#34;./config/example_nemo_grid_t.json\u0026#34; # path to json config file fn_nemo_dat = \u0026#34;coast_example_nemo_data.nc\u0026#34; fn_nemo_dom = \u0026#34;coast_example_nemo_domain.nc\u0026#34; gridded_obj = coast.Gridded(fn_data=fn_nemo_dat, fn_domain=fn_nemo_dom, config=config_file) For convenience, as indicated above, the path to the configuration file could be alternatively expressed as a path object. E.g.:\nfrom pathlib import Path config_file = Path(\u0026#34;path/to/config_file.json\u0026#34;) Configuration file structure Configuration files must follow a standard structure so that the ConfigParser class can parse the file correctly.\nDepending on the type of configuration file, there are a number of required keys:\nGridded configuration    Key Description     type A string value representing the type of configuration file. In the case of gridded config this will always be \u0026ldquo;gridded\u0026rdquo;.   dimensionality An integer value representing the number of dimensions within the data files.   grid_ref A dictionary containing the type of grid, and a list of grid variables defining the mapping from the domain file to NEMO file.   chunks A dictionary defining a dask chunk shape, used when loading in data files. JSON doesn\u0026rsquo;t support integer keys, and so the dimensions name should be provided as the key instead. An empty dictionary will result in auto chunking. Rechunking can be applied subsequently with the standardised dimension names.   dataset Parent key for holding configuration specific to the dataset files.   domain Parent key for holding configuration specific to domain files. This is an optional key depending on whether a domain file is required or not.   dimension_map Child key of dataset/domain. A dictionary defining the mappings between input data dimension names and the framework\u0026rsquo;s standardised dimension names.   variable_map Child key of dataset/domain. A dictionary defining the mappings between input data variable names and the framework\u0026rsquo;s standardised variable names.   keep_all_vars Optional child key of dataset/domain. If \u0026ldquo;True\u0026rdquo;, all variables from the input datafile will be carried over to the Gridded dataset. If \u0026ldquo;False\u0026rdquo;, only mapped variables will be carried over. \u0026ldquo;False\u0026rdquo; is assumed if the key is not present.   coord_vars Child key of dataset. A list of dataset coordinate variables to apply once dataset is loaded.   static_variables Parent key for holding configuration used for merging domain variables into the main dataset.   not_grid_vars Child key of static_variables. A list of grid independant variables to pull across from the domain file.   delete_vars Child key of static_variables. A list of variables to drop following the merge of domain and dataset.   processing_flags A list of strings referring to any preliminary processing methods to be carried out on the data.    Indexed configuration    Key Description     type A string value representing the type of configuration file. In the case of indexed config this will always be \u0026ldquo;indexed\u0026rdquo;.   dimensionality An integer value representing the number of dimensions within the data files.   chunks A dictionary defining a dask chunk shape, used when loading in data files. JSON doesn\u0026rsquo;t support integer keys, and so the dimensions name should be provided as the key instead. An empty dictionary will result in auto chunking. Rechunking can be applied subsequently with the standardised dimension names.   dataset Parent key for holding configuration specific to the dataset files.   dimension_map Child key of dataset. A dictionary defining the mappings between input data dimension names and the framework\u0026rsquo;s standardised dimension names.   variable_map Child key of dataset. A dictionary defining the mappings between input data variable names and the framework\u0026rsquo;s standardised variable names.   keep_all_vars Optional child key of dataset/domain. If \u0026ldquo;True\u0026rdquo;, all variables from the input datafile will be carried over to the Indexed dataset. If \u0026ldquo;False\u0026rdquo;, only mapped variables will be carried over. \u0026ldquo;False\u0026rdquo; is assumed if the key is not present.   coord_vars Child key of dataset. A list of dataset coordinate variables to apply once dataset is loaded.   processing_flags A list of strings referring to any preliminary processing methods to be carried out on the data.    Example configuration file Below is the template of a gridded configuration file:\n{ \u0026#34;type\u0026#34;: \u0026#34;gridded\u0026#34;, \u0026#34;dimensionality\u0026#34;: 4, \u0026#34;chunks\u0026#34;: { \u0026#34;time_counter\u0026#34;:2, \u0026#34;x\u0026#34;:4, \u0026#34;y\u0026#34;:4 }, \u0026#34;grid_ref\u0026#34;: { \u0026#34;t-grid\u0026#34;: [ \u0026#34;glamt\u0026#34;, \u0026#34;gphit\u0026#34;, \u0026#34;e1t\u0026#34;, \u0026#34;e2t\u0026#34;, \u0026#34;e3t_0\u0026#34;, \u0026#34;deptht_0\u0026#34;, \u0026#34;tmask\u0026#34;, \u0026#34;bottom_level\u0026#34;, \u0026#34;hbatt\u0026#34; ] }, \u0026#34;dataset\u0026#34;: { \u0026#34;dimension_map\u0026#34;: { \u0026#34;time_counter\u0026#34;: \u0026#34;t_dim\u0026#34;, \u0026#34;deptht\u0026#34;: \u0026#34;z_dim\u0026#34;, \u0026#34;y\u0026#34;: \u0026#34;y_dim\u0026#34;, \u0026#34;x\u0026#34;: \u0026#34;x_dim\u0026#34;, \u0026#34;x_grid_T\u0026#34;: \u0026#34;x_dim\u0026#34;, \u0026#34;y_grid_T\u0026#34;: \u0026#34;y_dim\u0026#34; }, \u0026#34;variable_map\u0026#34;: { \u0026#34;time_counter\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;votemper\u0026#34;: \u0026#34;temperature\u0026#34;, \u0026#34;thetao\u0026#34;: \u0026#34;temperature\u0026#34;, \u0026#34;temp\u0026#34;: \u0026#34;temperature\u0026#34;, \u0026#34;toce\u0026#34;: \u0026#34;temperature\u0026#34;, \u0026#34;so\u0026#34;: \u0026#34;salinity\u0026#34;, \u0026#34;vosaline\u0026#34;: \u0026#34;salinity\u0026#34;, \u0026#34;soce\u0026#34;: \u0026#34;salinity\u0026#34;, \u0026#34;sossheig\u0026#34;: \u0026#34;ssh\u0026#34;, \u0026#34;zos\u0026#34;: \u0026#34;ssh\u0026#34; }, \u0026#34;coord_vars\u0026#34;: [ \u0026#34;longitude\u0026#34;, \u0026#34;latitude\u0026#34;, \u0026#34;time\u0026#34;, \u0026#34;depth_0\u0026#34; ] }, \u0026#34;domain\u0026#34;: { \u0026#34;dimension_map\u0026#34;: { \u0026#34;t\u0026#34;: \u0026#34;t_dim0\u0026#34;, \u0026#34;x\u0026#34;: \u0026#34;x_dim\u0026#34;, \u0026#34;y\u0026#34;: \u0026#34;y_dim\u0026#34;, \u0026#34;z\u0026#34;: \u0026#34;z_dim\u0026#34; }, \u0026#34;variable_map\u0026#34;: { \u0026#34;time_counter\u0026#34;: \u0026#34;time0\u0026#34;, \u0026#34;glamt\u0026#34;: \u0026#34;longitude\u0026#34;, \u0026#34;gphit\u0026#34;: \u0026#34;latitude\u0026#34;, \u0026#34;e1t\u0026#34;: \u0026#34;e1\u0026#34;, \u0026#34;e2t\u0026#34;: \u0026#34;e2\u0026#34;, \u0026#34;e3t_0\u0026#34;: \u0026#34;e3_0\u0026#34;, \u0026#34;tmask\u0026#34;:\u0026#34;mask\u0026#34;, \u0026#34;deptht_0\u0026#34;: \u0026#34;depth_0\u0026#34;, \u0026#34;bottom_level\u0026#34;: \u0026#34;bottom_level\u0026#34;, \u0026#34;hbatt\u0026#34;:\u0026#34;bathymetry\u0026#34; } }, \u0026#34;static_variables\u0026#34;: { \u0026#34;not_grid_vars\u0026#34;: [ \u0026#34;jpiglo\u0026#34;, \u0026#34;jpjglo\u0026#34;, \u0026#34;jpkglo\u0026#34;, \u0026#34;jperio\u0026#34;, \u0026#34;ln_zco\u0026#34;, \u0026#34;ln_zps\u0026#34;, \u0026#34;ln_sco\u0026#34;, \u0026#34;ln_isfcav\u0026#34; ], \u0026#34;delete_vars\u0026#34;: [ \u0026#34;nav_lat\u0026#34;, \u0026#34;nav_lon\u0026#34;, \u0026#34;deptht\u0026#34; ] }, \u0026#34;processing_flags\u0026#34;: [ \u0026#34;example_flag1\u0026#34;, \u0026#34;example_flag2\u0026#34; ] } Example configuration files can be found in the config/ directory within the COAsT github repository.\n","excerpt":"Configuration file code can be found in coast/config within the COAsT github repository. This code …","ref":"/COAsT/docs/examples/configuration_files/","title":"Configuration files"},{"body":"AMM15 - 1.5km resolution Atlantic Margin Model \u0026#34;\u0026#34;\u0026#34; AMM15_example_plot.py Make simple AMM15 SST plot. \u0026#34;\u0026#34;\u0026#34; #%% import coast import numpy as np import xarray as xr import matplotlib.pyplot as plt import matplotlib.colors as colors # colormap fiddling ################################################# #%% Loading data ################################################# config = \u0026#39;AMM15\u0026#39; dir_nam = \u0026#34;/projectsa/NEMO/gmaya/2013p2/\u0026#34; fil_nam = \u0026#34;20130415_25hourm_grid_T.nc\u0026#34; dom_nam = \u0026#34;/projectsa/NEMO/gmaya/AMM15_GRID/amm15.mesh_mask.cs3x.nc\u0026#34; config = \u0026#34;/work/jelt/GitHub/COAsT/config/example_nemo_grid_t.json\u0026#34; sci_t = coast.Gridded(dir_nam + fil_nam, dom_nam, config=config) # , chunks=chunks) chunks = { \u0026#34;x_dim\u0026#34;: 10, \u0026#34;y_dim\u0026#34;: 10, \u0026#34;t_dim\u0026#34;: 10, } # Chunks are prescribed in the config json file, but can be adjusted while the data is lazy loaded. sci_t.dataset.chunk(chunks) # create an empty w-grid object, to store stratification sci_w = coast.Gridded(fn_domain=dom_nam, config=config.replace(\u0026#34;t_nemo\u0026#34;, \u0026#34;w_nemo\u0026#34;)) sci_w.dataset.chunk({\u0026#34;x_dim\u0026#34;: 10, \u0026#34;y_dim\u0026#34;: 10}) # Can reset after loading config json print(\u0026#39;* Loaded \u0026#39;,config, \u0026#39; data\u0026#39;) ################################################# #%% subset of data and domain ## ################################################# # Pick out a North Sea subdomain print(\u0026#39;* Extract North Sea subdomain\u0026#39;) ind_sci = sci_t.subset_indices([51,-4], [62,15]) sci_nwes_t = sci_t.isel(y_dim=ind_sci[0], x_dim=ind_sci[1]) #nwes = northwest europe shelf ind_sci = sci_w.subset_indices([51,-4], [62,15]) sci_nwes_w = sci_w.isel(y_dim=ind_sci[0], x_dim=ind_sci[1]) #nwes = northwest europe shelf #%% Apply masks to temperature and salinity if config == \u0026#39;AMM15\u0026#39;: sci_nwes_t.dataset[\u0026#39;temperature_m\u0026#39;] = sci_nwes_t.dataset.temperature.where( sci_nwes_t.dataset.mask.expand_dims(dim=sci_nwes_t.dataset[\u0026#39;t_dim\u0026#39;].sizes) \u0026gt; 0) sci_nwes_t.dataset[\u0026#39;salinity_m\u0026#39;] = sci_nwes_t.dataset.salinity.where( sci_nwes_t.dataset.mask.expand_dims(dim=sci_nwes_t.dataset[\u0026#39;t_dim\u0026#39;].sizes) \u0026gt; 0) else: # Apply fake masks to temperature and salinity sci_nwes_t.dataset[\u0026#39;temperature_m\u0026#39;] = sci_nwes_t.dataset.temperature sci_nwes_t.dataset[\u0026#39;salinity_m\u0026#39;] = sci_nwes_t.dataset.salinity #%% Plots fig = plt.figure() plt.pcolormesh( sci_t.dataset.longitude, sci_t.dataset.latitude, sci_t.dataset.temperature.isel(z_dim=0).squeeze()) #plt.xlabel(\u0026#39;longitude\u0026#39;) #plt.ylabel(\u0026#39;latitude\u0026#39;) #plt.colorbar() plt.axis(\u0026#39;off\u0026#39;) plt.show() fig.savefig(\u0026#39;AMM15_SST_nocolorbar.png\u0026#39;, dpi=120)    India subcontinent maritime domain. WCSSP India configuration #%% import coast import numpy as np import xarray as xr import dask import matplotlib.pyplot as plt import matplotlib.colors as colors # colormap fiddling ################################################# #%% Loading data ################################################# dir_nam = \u0026#34;/projectsa/COAsT/NEMO_example_data/MO_INDIA/\u0026#34; fil_nam = \u0026#34;ind_1d_cat_20180101_20180105_25hourm_grid_T.nc\u0026#34; dom_nam = \u0026#34;domain_cfg_wcssp.nc\u0026#34; config_t = \u0026#34;/work/jelt/GitHub/COAsT/config/example_nemo_grid_t.json\u0026#34; sci_t = coast.Gridded(dir_nam + fil_nam, dir_nam + dom_nam, config=config_t) #%% Plot fig = plt.figure() plt.pcolormesh( sci_t.dataset.longitude, sci_t.dataset.latitude, sci_t.dataset.temperature.isel(t_dim=0).isel(z_dim=0)) plt.xlabel(\u0026#39;longitude\u0026#39;) plt.ylabel(\u0026#39;latitude\u0026#39;) plt.title(\u0026#39;WCSSP India SST\u0026#39;) plt.colorbar() plt.show() fig.savefig(\u0026#39;WCSSP_India_SST.png\u0026#39;, dpi=120)    South East Asia, 1/12 deg configuration (ACCORD: SEAsia_R12) #%% import coast import numpy as np import xarray as xr import dask import matplotlib.pyplot as plt import matplotlib.colors as colors # colormap fiddling ################################################# #%% Loading data ################################################# dir_nam = \u0026#34;/projectsa/COAsT/NEMO_example_data/SEAsia_R12/\u0026#34; fil_nam = \u0026#34;SEAsia_R12_5d_20120101_20121231_gridT.nc\u0026#34; dom_nam = \u0026#34;domain_cfg_ORCA12_adj.nc\u0026#34; config_t = \u0026#34;/work/jelt/GitHub/COAsT/config/example_nemo_grid_t.json\u0026#34; sci_t = coast.Gridded(dir_nam + fil_nam, dir_nam + dom_nam, config=config_t) #%% Plot fig = plt.figure() plt.pcolormesh( sci_t.dataset.longitude, sci_t.dataset.latitude, sci_t.dataset.soce.isel(t_dim=0).isel(z_dim=0)) plt.xlabel(\u0026#39;longitude\u0026#39;) plt.ylabel(\u0026#39;latitude\u0026#39;) plt.title(\u0026#39;SE Asia, surface salinity (psu)\u0026#39;) plt.colorbar() plt.show() fig.savefig(\u0026#39;SEAsia_R12_SSS.png\u0026#39;, dpi=120)    ","excerpt":"AMM15 - 1.5km resolution Atlantic Margin Model \u0026#34;\u0026#34;\u0026#34; AMM15_example_plot.py Make simple …","ref":"/COAsT/docs/examples/configs_gallery/","title":"Configuration Gallery"},{"body":"__________________________________________________________________________________________ ______ ___ _ _________ .' ___ | .' `. / \\ | _ _ | / .' \\_|/ .-. \\ / _ \\ .--.|_/ | | \\_| | | | | | | / ___ \\ ( (`\\] | | \\ `.___.'\\\\ `-' /_/ / \\ \\_ `'.'. _| |_ `.____ .' `.___.'|____| |____|[\\__) )|_____| Coastal Ocean Assessment Toolbox __________________________________________________________________________________________ COAsT is a Python package for managing and analysing high resolution NEMO output. Here you can find information on obtaining, installing and using COAsT as well as guidelines for contributing to the project.\nThis documentation site is still under construction but you can still find guidelines for contributing to the package and this website. See below for description of each section.\n","excerpt":"__________________________________________________________________________________________ ______ …","ref":"/COAsT/docs/","title":"Documentation"},{"body":"","excerpt":"","ref":"/COAsT/docs/reference/parameter-reference/","title":"Parameter Reference"},{"body":"  #td-cover-block-0 { background-image: url(/COAsT/about/featured-background_hu14d69772da4446f8c45afbc4cad362c8_132726_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/COAsT/about/featured-background_hu14d69772da4446f8c45afbc4cad362c8_132726_1920x1080_fill_q75_catmullrom_top.jpg); } }  About COAsT A site using the Docsy Hugo theme. --        COAsT is a Python package for managing and analysing high resolution NEMO output Read more here     This site was based off the Docsy Hugo theme.    ","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/COAsT/about/","title":"About Goldydocs"},{"body":"  #td-cover-block-0 { background-image: url(/COAsT/featured-background_hu14d69772da4446f8c45afbc4cad362c8_132726_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/COAsT/featured-background_hu14d69772da4446f8c45afbc4cad362c8_132726_1920x1080_fill_q75_catmullrom_top.jpg); } }  Welcome to the documentation: A Docsy site for COAsT Learn More   Download   COAsT\n\n        This is a single web UI providing visibility into the COAsT python framework.       Download from Anaconda.org Get the COAsT framework!\nRead more …\n   Contributions welcome! We do a Pull Request contributions workflow on GitHub. New users are always welcome!\nRead more …\n   Follow us on Twitter! For announcement of latest features etc.\nRead more …\n    ","excerpt":"#td-cover-block-0 { background-image: …","ref":"/COAsT/","title":"COAsT"},{"body":"","excerpt":"","ref":"/COAsT/community/","title":"Community"},{"body":"","excerpt":"","ref":"/COAsT/search/","title":"Search Results"}]