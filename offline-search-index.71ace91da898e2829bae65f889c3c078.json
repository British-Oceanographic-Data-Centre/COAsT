[{"body":"Bug reports are an important part of making COAsT more stable. Having a complete bug report will allow others to reproduce the bug and provide insight into fixing. See this stackoverflow article for tips on writing a good bug report.\nTrying if the bug is ocurring in the lastest version of the package is worthwhile exercise to confirm the bug still exists. It is also worth searching existing bug reports and pull requests to see if the issue has already been reported and/or fixed.\nBug reports must:\nInclude a short, self contained Python snippet reproducing the problem. You can format the code nicely by using GitHub Flavored Markdown: .```python import coast fn_data = \"\u003cpath to T-grid data file(s)\u003e\" fn_domain = \"\u003cpath to domain file\u003e\" fn_config = \"\u003cpath to json config file\u003e\" data = coast.Gridded(fn_data, fn_domain, fn_config) ```. Include the full version string of COAsT and its dependencies. Explain why the current behavior is wrong/not desired and what you expect instead. The issue will then show up to the COAsT community and be open to comments/ideas from others. Click here to open an issue.\n","categories":"","description":"Python object structure guidance.\n","excerpt":"Python object structure guidance.\n","ref":"/COAsT/docs/contributing/bug_report/","tags":"","title":"Bug Report"},{"body":"Python as a language comes with more stringent recommendations than most when it comes to code styling. This is advantageous in our case as it gives us an obvious set of guidelines to adopt.\nWhen it comes to simple code styling, much of what’s recommended here will be copied from Python Enhancement Proposal (PEP) 8, an officially proposed and accepted Python style guide.\nIn our code, we use pylint and black applied on CI/CD codes in order to fix some code styling and to check the code styling. Your pull request will only be accepted if the new code have a styling score equal or higher than the actual code.\nCode Styling Conventions Let’s keep things simple to start with…\nIndentation should be achieved with spaces rather than tabs and each new level of indentation should be indented by four columns (i.e four spaces).\nAny single line, including its indentation characters, should not exceed 79 characters in length.\nTop-level (i.e at the module/file level rather than inside a function or class) function and class definitions should be separated by two blank lines.\nMethod (functions within a class) definitions are separated by a single blank line.\nUsually, “import” statements should be on separate lines, that is to say that you should have one line per distinct module or package import. An exception to this rule is when multiple objects are imported from a single module or package, using a “from” statement, in which case individual objects can be imported on the same line, separated by commas.\nPEP 8 does not make a recommendation relating to the use of double or single quotes in general use, but for the sake of consistency, this document suggests the use of double quotes wherever practical. This recommendation is intended for the sake of consistency with triple-quoted strings, as per Docstring Conventions (PEP 257).\nOperators should be separated by single columns (i.e one space) either side, unless inside parentheses, in which case no whitespace is required.\nComments (beginning with the # character) should be indented as if they were code. In the case of inline comments, separate the comment with two spaces following the code it shares the line with.\nAll functions should contain a docstring, which provides basic information on its usage. For this project, the reStructuredText docstring format is suggested.\nWhen it comes to naming variables and functions, snake case (lower_case_words_separated_by_underscores) is preferred. There are however a few exceptions to this rule: Class names should be styled as camel case (EveryNewWordIsCapitalised). Constants (Variables that should not be changed) can be indicated by the use of screaming snake case (UPPER_CASE_WORDS_SEPARATED_BY_UNDERSCORES). Note that this library currently targets Python 3.7, so the use of typing.Final official support for constant variables, new as of Python 3.8: is not currently supported.\nIn general, it is suggested to avoid the use of single-character variable names, but this is acceptable in certain cases, such as when defining coordinates (such as x, y and z), as these will be commonly recognized and enforcing different rules could cause confusion. PEP 8 advises the following regarding names to avoid: “Never use the characters ’l’ (lowercase letter el), ‘O’ (uppercase letter oh), or ‘I’ (uppercase letter eye) as single character variable names.” These specific characters should be avoided because they present an accessibility issue, as under many fonts these characters may be difficult to distinguish or completely indistinguishable from numerals one (1) and zero (0).\nIn the interest of readability, where named iterator variables are required, this document suggests the use of double characters (e.g. “ii” rather than “i”).\n","categories":"","description":"Python style guidance.\n","excerpt":"Python style guidance.\n","ref":"/COAsT/docs/contributing/contributing-package/code_styling/","tags":"","title":"Code styling"},{"body":"","categories":"","description":"Examples Gridded scripts for the COAsT package.\n","excerpt":"Examples Gridded scripts for the COAsT package.\n","ref":"/COAsT/docs/examples/notebooks/gridded/","tags":"","title":"Gridded"},{"body":"COAsT utilises Python’s default logging library and includes a simple setup function for those unfamiliar with how to use it.\nimport coast coast.logging_util.setup_logging() This is all you need to enable full logging output to the console.\nBy default, setup_logging will use the “CRITICAL” logging level, if you want to adjust this, you can use the flags from the logging library.\nimport coast import logging coast.logging_util.setup_logging(level=logging.INFO) Alternative logging levels in increasing levels of severity. Note logs are reported at the chosen severity level and higher:\n..., level=logging.DEBUG) # Detailed information, typically of interest only when diagnosing problems. ..., level=logging.INFO) # Confirmation that things are working as expected. ..., level=logging.WARNING) # An indication that something unexpected happened, or indicative of some problem in the near future (e.g. ‘disk space low’). The software is still working as expected. ..., level=logging.ERROR) # Due to a more serious problem, the software has not been able to perform some function ..., level=logging.CRITICAL) # A serious error, indicating that the program itself may be unable to continue running For more info on logging levels, see the relevant Python documentation.\nLogging output will be printed in the console once enabled by default, but output can be directed to any Stream, for instance, to an opened file.\nimport coast file = open(\"coast.log\", \"w\") coast.logging_util.setup_logging(stream=file) coast.logging_util.info(\"Hello World!\") # Your use of COAsT would go here, this line is included as an example file.close() ","categories":"","description":"Python logging diagnostics guidance.\n","excerpt":"Python logging diagnostics guidance.\n","ref":"/COAsT/docs/general-information/python_logging/","tags":"","title":"Logging"},{"body":"The Coastal Ocean Assessment Toolbox (COAsT) is a valuable Python package specifically designed to assist in the assessment, management, and analysis of high-resolution regional ocean model outputs. It provides a comprehensive set of tools and functionalities for analyzing and visualizing various aspects of coastal ocean data, delivering novel diagnostics for processes that emerge within these models.\nKey Features High-Resolution Ocean Models: COAsT is tailored to work with high-resolution regional ocean models\nNEMO Integration: The initial focus of COAsT is on delivering a limited number of novel diagnostics for NEMO configurations, a widely used ocean model. However, the toolbox is designed to be expanded to include other diagnostics and support for additional ocean models.\nxarray Framework: COAsT leverages the capabilities of the xarray library to provide efficient and user-friendly data handling and analysis.\nCommunity-Ready and Flexible: The aim of COAsT is to create a toolbox that is ready for collaboration with the research community. It is designed to be flexible, allowing users to extend and adapt its functionalities to suit their specific research needs.\nFunctionalities Observation Data Co-processing and Management: COAsT includes an expanding array of functions for reading and processing of ocean observational data types for co-analysis with simulation data. These data sources include satellite altimetry, tide gauges and in-situ profile data.\nVisualization and Mapping: COAsT offers tools for creating visual representations of your data through maps, graphs, and charts. It seamlessly integrates with popular libraries such as cartopy and matplotlib\nSpatial Analysis: COAsT provides robust spatial analysis tools for geospatial data analysis and statistical computations. For example flows across contours or transect, and computations over geographic regions using masks.\nStatistical Analysis: The package also offers a suite of statistical analysis capabilities\n","categories":"","description":"What is COAsT?\n","excerpt":"What is COAsT?\n","ref":"/COAsT/docs/overview/","tags":"","title":"Overview"},{"body":"","categories":"","description":"These tutorials were automatically rendered from the python notebooks in [COAsT:examples_scripts/notebook_tutorials:runnable_notebooks](https://github.com/British-Oceanographic-Data-Centre/COAsT/tree/master/example_scripts/notebook_tutorials/runnable_notebooks). These can be downloaded and run locally with some example data and example configuration files, that you can download following the procedures described on the [Getting Started section](https://british-oceanographic-data-centre.github.io/COAsT/docs/examples/notebooks/). Example scripts rendered from python notebooks [available within the COAsT package](https://github.com/British-Oceanographic-Data-Centre/COAsT/tree/master/example_scripts/notebook_tutorials).\n","excerpt":"These tutorials were automatically rendered from the python notebooks …","ref":"/COAsT/docs/examples/notebooks/","tags":"","title":"Tutorials - Notebooks"},{"body":"If you’re not a developer, your contributions to the documentation are still of huge value, even if you’re not an expert in COAsT. In fact, some sections of our docs may benefit from your fresh perspective. If you come across something that doesn’t make sense to you, updating that section once you figure it out can greatly assist the next person.\nAll submissions, including those from project members, require review. We use GitHub pull requests for this purpose. For more information on using pull requests, consult GitHub Help.\nAbout COAsT Documentation We rely on the following technologies to create our Documentation:\nHugo Extended Version for formatting and generating our Documentation website. The Docsy theme for styling and site structure. GitHub Pages for site deployment. Hugo, an open-source static site generator, provides templates, a standardized content organization, and website generation. You write pages in Markdown (or HTML if you prefer), and Hugo transforms them into a website.\nHere are some key points about our documentation:\nCOAsT documentation consists of three parts: tutorial notebooks within the code itself, docstrings within the code, and the documentation repository.\nThe tutorial notebooks provide guidance on using the code for analysis, docstrings explain individual function usage, and the documentation offers topic-based overviews along with other information (overviews, installation, usage, etc).\nAll functions should contain a docstring, which provides basic information on its usage. For this project, the reStructuredText docstring format is suggested.\nUpdating a Single Page If you come across something you’d like to change while using the docs, Docsy provides a convenient way to do so:\nClick Edit this page in the top right-hand corner of the page.\nIf you don’t already have an up-to-date fork of the project repo, you’ll be prompted to get one. Click Fork this repository and propose changes or Update your Fork to obtain an up-to-date version of the project to edit. The relevant page in your fork will be displayed in edit mode.\nMake your edits.\nSubmit a pull request with a summary of the changes.\nPreviewing Your Changes Locally To preview your changes locally, run your own Hugo server. Ensure you have the following prerequisites:\nNode.js and npm Installation: Download and install Node.js, including npm, from the official Node.js website. We recommend using version 18 or later.\nHugo Installation: You’ll need an extended version of Hugo, preferably version 0.120.1 or later, for local builds and previews of sites using Docsy. To ensure you have the correct Hugo version, follow these steps:\nIf you install Hugo from the release page, select the extended Hugo version to support SCSS.\nAfter installation, verify your Hugo version with hugo version. It should resemble the following:\nhugo v0.120.2-9c2b2414d231ec1bdaf3e3a030bf148a45c7aa17+extended linux/amd64 BuildDate=2023-10-31T16:27:18Z VendorInfo=gohugoio Ensure the version name includes “extended.”\nFor more details, see this installation guide.\nNext, fork the COAsT-site repo into your project and create a local copy using git clone:\ngit clone --recurse-submodules --depth 1 git@github.com:British-Oceanographic-Data-Centre/COAsT-site.git cd COAsT-site Now, install the necessary dependencies and scripts:\nnpm install # Install package dependencies npm run prepare # Fetch the submodule used as the theme and install its dependencies After cloning the site repository, navigate to the root folder and execute the following command to serve the website locally:\nhugo server The hugo server command builds and serves the site at http://localhost:1313/COAsT by default. While serving your site locally, Hugo will monitor content changes and refresh your site automatically.\nFollow the standard GitHub workflow to edit files, commit changes, push them to your fork, and create a pull request.\nCreating an Issue If you’ve identified a problem in the documentation but aren’t sure how to fix it yourself, please create an issue in the COAsT-site repo. You can also create an issue about a specific page by clicking the Create Issue button in the top right-hand corner of the page.\nUseful Resources Docsy User Guide: All about Docsy, including navigation, design, and multi-language support. Hugo Documentation: A comprehensive reference for Hugo. GitHub Hello World!: A basic introduction to GitHub concepts and workflow. ","categories":"","description":"Guidelines for contributing to the COAsT documentation site.\n","excerpt":"Guidelines for contributing to the COAsT documentation site.\n","ref":"/COAsT/docs/contributing/contributing-docs/","tags":"","title":"Contributing: Documentation"},{"body":"COAsT is an object-orientated package, meaning that data is stored within Python object structures. In addition to data storage, these objects contain methods (subroutines) which allow for manipulation of this data.\nThe fundamental concepts of Object-Oriented Programming (OOP) are well-established and thoroughly documented. However, we believe it is important to focus certain guidelines that are particularly relevant to this programming language and its specific application:\nIn Python, all class attributes are technically public, but semantically, attributes can be designated as non-public by including leading underscores in the name. For instance, “my_variable” becomes “_my_variable”. These attributes are generally referred to as “protected”.\nWhen you define a Python class, it is a best practice to inherit from the base object type. This convention stems from Python 2.X, as classes and types were not originally synonymous. This behaviour is implicit in Python 3.X but the convention has persisted nonetheless. Classes defined this way are referred to as “new-style” classes.\nWhen defining a class that inherits from another, it is important to remember that overridden methods (in particular, this behaviour is important when dealing with __init__ methods) do not implicitly call the parent method. What this means is that unless you want to deliberately prevent the behaviour of the parent class (this is a very niche use-case), it is important to include a reference to the parent method. An example of this is: super().__init__() This functionality is advantageous as it prevents unnecessary duplication of code, which is a key tenet of object-oriented software.\nAn example of such an object in COAsT is the Gridded object, which allows for the storage and manipulation of e.g. NEMO output and domain data. It is important to understand how to load data using COAsT and the structure of the resulting objects.\nA Gridded object is created and initialised by passing it the paths of the domain and data files. Ideally, the grid type should also be specified (T, U, V or F in the case of NEMO). For example, to load in data from a file containing data on a NEMO T-grid:\nimport coast fn_data = \"\u003cpath to T-grid data file(s)\u003e\" fn_domain = \"\u003cpath to domain file\u003e\" fn_config = \"\u003cpath to json config file\u003e\" data = coast.Gridded(fn_data, fn_domain, fn_config) Ideally, Gridded model output data should be in grid-specific files, i.e. containing output variables situated on a NEMO T, U, V or F grid, whereas the grid variables are in a single domain file. On loading into COAsT, only the grid specific variables appropriate for the paired data are placed into the Gridded object. A Gridded object therefore contains grid-specific data and all corresponding grid variables. One of the file names can be omitted (to get a data-only or grid only object), however functionality in this case will be limited.\nOnce loaded, data is stored inside the object using an xarray.dataset object. Following on from the previous code example, this can be viewed by calling:\ndata.dataset This reveals all netcdf-type aspects of the data and domain variables that were loaded, including dimensions, coordinates, variables and attributes. For example:\n\u003cxarray.Dataset\u003e Dimensions: (axis_nbounds: 2, t_dim: 7, x_dim: 297, y_dim: 375, z_dim: 51) Coordinates: time (t_dim) datetime64[ns] 2007-01-01T11:58:56 ... 2007-01-31T11:58:56 longitude (y_dim, x_dim) float32 ... latitude (y_dim, x_dim) float32 ... Dimensions without coordinates: axis_nbounds, t_dim, x_dim, y_dim, z_dim Data variables: deptht_bounds (z_dim, axis_nbounds) float32 ... sossheig (t_dim, y_dim, x_dim) float32 ... time_counter_bounds (t_dim, axis_nbounds) datetime64[ns] ... time_instant (t_dim) datetime64[ns] ... temperature (t_dim, z_dim, y_dim, x_dim) float32 ... e1 (y_dim, x_dim) float32 ... e2 (y_dim, x_dim) float32 ... e3_0 (z_dim, y_dim, x_dim) float32 1.0 1.0 1.0 ... 1.0 1.0 Variables may be obtained in a number of ways. For example, to get temperature data, the following are all equivalent:\ntemp = data.dataset.temperature temp = data.dataset['temperature'] temp = data['temperature'] These commands will all return an xarray.dataarray object. Manipulation of this object can be done using xarray commands, for example indexing using [] or `xarray.isel``. Be aware that indexing will preserve lazy loading, however and direct access or modifying of the data will not. For this reason, if you require a subset of the data, it is best to index first.\nThe names of common grid variables are standardised within the COAsT package using JSON configuration files. For example, the following lists COAsT internal variable followed by the typical NEMO variable names:\nlongitude [glamt / glamu / glamv / glamf] latitude [gphit / gphiu / gphiv / gphif] time [time_counter] e1 [e1t / e1u / e1v / e1f] (dx variable) e2 [e1t / e1u / e1v / e1f] (dy variable) e3_0 [e3t_0 / e3u_0 / e3v_0 / e3f_0] (dz variable at time 0) Longitude, latitude and time are also set as coordinates. You might notice that dimensions are also standardised:\nx_dim The dimension for the x-axis (longitude) y_dim The dimension for the y-axis (latitude) t_dim The dimension for the time axis z_dim The dimension for the depth axis. Wherever possible, the aim is to ensure that all of the above is consistent across the whole COAsT toolbox. Therefore, you will also find the same names and dimensions in, for example observation objects. Future objects, where applicable, will also follow these conventions. If you (as a contributor) add new objects to the toolbox, following the above template is strongly encouraged. This includes using xarray dataset/dataarray objects where possible, adopting an object oriented approach and adhering to naming conventions.\n","categories":"","description":"Python style guidance.\n","excerpt":"Python style guidance.\n","ref":"/COAsT/docs/contributing/contributing-package/code_structure/","tags":"","title":"Code structure"},{"body":"","categories":"","description":"Examples scripts for general utility and analysis tools within COAsT.\n","excerpt":"Examples scripts for general utility and analysis tools within COAsT.\n","ref":"/COAsT/docs/examples/notebooks/general/","tags":"","title":"General utility and analysis tools"},{"body":"Prerequisites This package requires;\na linux environment or docker for Windows python version 3.8.10 Miniconda (if you are planning to install it using conda) Basic use installation via conda or pip This package should be installed by run;\nconda install -c bodc coast However, there is also the option of;\npip install COAsT (Optional): Extra python packages In order to try the Examples described in this documentation, you may also need to install the following python packages (as they are not natively part of the COAsT package):\n# cartopy pip install cartopy # xesmf pip install xesmf Development use installation If you would prefer to work with a clone of the repository in a development python environment do the following. First clone the repository in the place where you want to work:\ngit clone https://github.com/British-Oceanographic-Data-Centre/COAsT.git cd COAsT Then build a python environment:\nconda env update --prune --file environment.yml conda activate coast The last step is to install the coast package on this environment:\npip install -e . Building the docker image and executing an interactive environment Warning: Building the image is resource heavy\nAfter cloning the repo (as above):\ndocker build . --tag coast docker compose up -d docker compose exec coast bash You can now start a python session and import coast. Docker compose mounts 3 directories from you host machine onto the docker container:\n./example_files:/example_files ./config:/config ./example_scripts:/example_scripts Check the installation! Start by opening a python terminal and then importing COAsT:\nimport coast Before using coast, we will just check if the installation process (Anaconda or pip) has installed correct package versions. In the python console copy the following:\nimport gsw import matplotlib print(gsw.__version__) print(matplotlib.__version__) The output should be\n3.4.0 3.5.1 or later. If it is, great carry on. If it is not, problems may occur with some functionality in coast. In this case, please update these packages versions.\nTest it! In order to test the package, example data files and configuration files are recommended.\nExample data files Download example files and link them into a new directory:\nwget -c https://linkedsystems.uk/erddap/files/COAsT_example_files/COAsT_example_files.zip \u0026\u0026 unzip COAsT_example_files.zip \u0026\u0026 rm -f COAsT_example_files.zip Example configuration files To facilitate loading different types of data, key information is passed to COAsT using configuration files. The config files used in the Examples are in the repository, or can be downloaded as static files:\nwget -c https://github.com/British-Oceanographic-Data-Centre/COAsT/archive/refs/heads/master.zip \u0026\u0026 unzip master.zip \u0026\u0026 rm -f master.zip mv COAsT-master/config ./ \u0026\u0026 rm -rf COAsT-master Explore the API Reference Page to access detailed information about specific objects and methods. Additionally, you can find practical usage examples on the example pages.\nIMPORTANT: If you are utilizing COAsT at the National Oceanography Centre (NOC) on Liverpool Servers, kindly access this link on the NOC Intranet for additional details.\n","categories":"","description":"Download, install and use.\n","excerpt":"Download, install and use.\n","ref":"/COAsT/docs/getting-started/","tags":"","title":"Getting Started"},{"body":"What is lazy… …loading Lazy loading determines if data is read into memory straight away (on that line of code execution) or if the loading is delayed until the data is physical altered by some function (normally mathematical in nature)\n…evaluation Lazy evaluation is about delaying the execution of a method/function call until the value is physical required, normally as a graph or printed to screen. Lazy evaluation can also help with memory management, useful with large dataset, by allowing for optimisation on the chained methods calls.\nLazy loading and Lazy evaluation are offer used together, though it is not mandatory and always worth checking that both are happening.\nBeing Lazy in COAsT There are two way to be Lazy within the COAsT package.\nxarray Dask xarray COAsT uses xarray to load NetCDF files in, by default this will be Lazy, the raw data values will not be brought into memory.\nyou can slice and subset the data while still having the lazy loading honoured, it is not until the data is altered, say via a call to NumPy.cumsum, that the required data will be loaded into memory.\nNote the data on disk (in the NetCDF file) is never altered, only the values in memory are changed.\nimport xarray as xr import NumPy as np dataset_domain = xr.open_dataset(fn_domain) e3w_0 = dataset_domain.e3w_0 # still lazy loaded e3w_0_cs = np.cumsum(e3w_0[1:, :, :], axis=0) # now in memory Dask When in use Dask will provide lazy evaluation on top of the lazy loading.\nusing the same example as above, a file loaded in using xarray, this time with the chunks option set, will not only lazy load the data, but will turn on Dask, now using either the xarray or Dask wrapper functions will mean the NumPy cumsum call is not evaluated right way, in fact it will not be evaluated until either the compute function is called, or a greedy method from another library is used.\nimport xarray as xr dataset_domain = xr.open_dataset(fn_domain, chunks={\"t\": 1}) e3w_0 = dataset_domain.e3w_0 # still lazy loaded e3w_0_cs = e3w_0[1:, :, :].cumsum(axis=0) # Dask backed Lazy evaluation We discuss Dask even more here.\n","categories":"","description":"Information on using lazy loading and/or evaluation within the COAsT package\n","excerpt":"Information on using lazy loading and/or evaluation within the COAsT …","ref":"/COAsT/docs/general-information/lazy-loading/","tags":"","title":"Working Lazily"},{"body":"Anyone interested in helping to develop COAsT need to have in mind that for COAsT development we use a Github workflow to manage version control and collaboration. Git allows use to keep track of changes made to the COAsT code base, avoid breaking existing code and work as a group on a single package.\nBefore understand more about the workflow of how to contribute to the package, you need to have in mind some key ideas.\nKey Ideas The COAsT repository has two core branches: master and develop. The master branch contains the tested code that you install when using Anaconda. This is updated less frequently, and is the “user-facing” branch of code. Most contributors do not need to edit this branch. The develop branch is the ‘pre-master’ branch, where working code is kept. This is the leading branch, with the most up-to-date code, although it is not necessarily user-facing. When writing code into your own branch (see below), it is ‘branched’ from develop and then eventually merged back into develop. You should never make changes directly to either master or develop.\nThere is a ’local’ and ‘remote’ copy of the COAsT repository. The local repository exists only on your machine. The remote repository is the one you see on the Github website and exists separately. The two versions of the repository can be synchronised at a single point using commands such as git pull git push and git fetch (see below). After cloning (downloading) the repository, all modifications you make/add/commit will only be local until you push them to the remote repository.\nTypical Workflow for contribution A typical workflow for editting COAsT in git might look like:\nClone Repository: git clone git@github.com:British-Oceanographic-Data-Centre/COAsT.git. This will create a new copy of COAsT on your local system which you can use to interact with git and view/edit the source code. This only needs to be done once. The branch that will be clonned will be the develop;\nCreate/checkout your new branch: git checkout -b new_branch_name. This will create and checkout your new branch – right now it is an identical copy of develop. However, any changes you commit to your local repository will be saved into your branch. Once you have created your branch, you can open it as before, using git checkout new_branch_name.\nMake changes/additions to code: Make any changes you like to COAsT. At this point it is separate from the main branches and it is safe to do so. If in doubt, enter git branch again to ensure you are within your own branch.\nAdd changes to branch: git add modified_file. Using this command will tell git that you have changed/added this file and you want to save it to the branch you are currently in. Upon entering this command, the file changes/additions are not saved to the branch and won’t be until the next step. You can remove an added file by entering git reset modified_file and can check which files have changed by typing git status.\nCommit changes to branch: git commit -m \"type a message in quotations\". Entering this command will “save” the changes you added using git add in the step above to the branch you are currently in. Once entered, git will identify what has changed since the previous commit. If this is the first commit in your new branch then since the version of develop that you branch from. This will not change any other branch except the one you are in and you can/should do this often with an appropriate message. At this point, all changes are still only on your local machine and will not change the remote repository. It is also possible to undo a commit using git revert, so nothing is unfixable.\nContinue modifying code: At this point, you may want to continue modifying the code, repeatedly adding changes and commiting them to your local repository, as above.\nPush your local repository to the remote: git push origin. This will upload the changes you have made in the branch you are in (and only this branch) to the remote (website) repository. If this is the first time you have pushed this branch then an error may appear telling you to repush with the --set-upstream flag enable. Simply copy and paste this command back into the terminal. This will “create” your branch in the remote repository. Once pushed, github will do some auto-checks to make sure the code works (which it may not, but that is fine). You can continue to modify the code at any point, and push multiple times. This is encouraged if sharing with other collaboraters.\nOnce you are satisfied with your changes, move onto the next steps.\nMake sure your local branch is up to date with the remote: git pull origin when in your branch. This is to ensure that nobody else has changed your branch, or if they have to update your local branch with the changes on the remote.\nUpdate your branch with develop:. Before requesting that your branch and its changes be merged back into the develop branch, it is good practice to first merge develop back into your branch. This is because develop may have changed since you started working on your branch and these changes should be merged into your branch to ensure that conflicts are resolved. To do this, first update develop by entering git checkout develop and git pull. This will update the develop branch on your local machine. Then merge develop back into your branch by entering git checkout your_branch and git merge develop. This may say up-to-date (in which case GREAT), or successful (in which case GREAT) or may say there are some conflicts. This happens when more than one person has changed the same piece of code.\nResolve Conflicts: This step may not be necessary if there are no conflicts. If git tells you there are conflicts, it will also tell you which files they occur in. For more information/help with conflict resolution see here\nCreate a pull request for your branch. First your most up to date branch using git push origin, even after merging develop in step 9/10. On the website you may then create a ‘pull request’ which is a formal way of saying you want to merge your branch back into develop. A pull request allows you to ask people to ‘review’ your branch, share your code, view the changes in your branch and other things. To make a pull request, go to the website, click on the pull requests tab and click Create new pull request. Then select your branch in the right drop down menu and develop in the left. You may then enter a description of the changes you have made and anything else you would like reviewers to see.\nCheck Github actions results. Some github actions runs everytime you open a pull request. Please make sure that all of then passed. Otherwise, you will not be able to continue with the pull request. In this step, please pay attention on the code formating and styling, following PEP8 guide.\nReviewers review the code: Requested reviewers take a look at your changes and run the unit_test. Once they are satisfied, they will approve the pull request, or add comments about any problems.\nMerge branch into develop: Once reviewers are satisfied, you may click Merge branch at the bottom of the pull request. Now your changes will be added into develop! Again, this is fine as the branch has been inspected by reviewers and any change can be reverted using git revert (although this is not encouraged for the develop branch).\n**Note: After creating a pull request, Github will automatically apply “black formatting” to the code and update “pylint score”. This will commit new (small) changes to the branch so you should always do a git pull on your branch to make sure your local version is up to date with the remote.\nCondensed Workflow git clone git@github.com:British-Oceanographic-Data-Centre/COAsT.git. git checkout -b new_branch_name Make changes git add changed_file git commit -m \"what changes have you made\" git push origin If your branch changed by anyone else, git pull Repeat steps 4-8 git checkout develop git pull git checkout your_branch git merge develop git push origin Create pull request from your_branch to develop, include description and request reviewers. Reviewers accept, Merge branch. ","categories":"","description":"Guidelines for contributing to the COAsT documentation site.\n","excerpt":"Guidelines for contributing to the COAsT documentation site.\n","ref":"/COAsT/docs/contributing/contributing-package/","tags":"","title":"Contributing to the Package"},{"body":"What is Dask Dask is a python library that allows code to be run in parallel based on the hardware your running on. This means Dask works just as well on your laptop as on your large server.\nUsing Dask Dask is included in the xarray library. When loading a data source (file/NumPy array) Dask is automatically initiated with the chunks variable in the config file. However the chunking may not be optimal but you can adjust it before computation are made.\nnemo_t = coast.Gridded( fn_data=dn_files+fn_nemo_grid_t_dat, fn_domain=dn_files+fn_nemo_dom, config=fn_config) chunks = { \"x_dim\": 10, \"y_dim\": 10, \"t_dim\": 10, } # Chunks are prescribed in the config json file, but can be adjusted while the data is lazy loaded. nemo_t.dataset.chunk(chunks) chunks tell Dask where to break your data across the different processor tasks.\nDirect Dask Dask can be imported and used directly\nimport Dask.array as da big_array = da.multiple(array1,array2) Dask arrays follow the NumPy API. This means that most NumPy functions have a Dask version.\nPotential Issues Dask objects are immutable. This means that the classic approach, pre-allocation follow by modification will not work.\nThe following code will error.\nimport Dask.array as da e3w_0 = da.squeeze(dataset_domain.e3w_0) depth_0 = da.zero_like(e3w_0) depth_0[0, :, :] = 0.5 * e3w_0[0, :, :] # this line will error out Option 1 Continue using NumPy function but wrapping the final value in a Dask array. This final Dask object will still be in-memory.\ne3w_0 = np.squeeze(dataset_domain.e3w_0) depth_0 = np.zeros_like(e3w_0) depth_0[0, :, :] = 0.5 * e3w_0[0, :, :] depth_0[1:, :, :] = depth_0[0, :, :] + np.cumsum(e3w_0[1:, :, :], axis=0) depth_0 = da.array(depth_0) Option 2 Dask offers a feature called delayed. This can be used as a modifier on your complex methods as follows;\n@Dask.delayed def set_timezero_depths(self, dataset_domain): # complex workings these do not return the computed answer, rather it returns a delayed object. These delayed object get stacked, as more delayed methods are called. When the value is needed, it can be computed like so;\nne = coast.Gridded(...) # come complex delayed methods called ne.data_variable.compute() Dask will now work out a computing path via all the required methods using as many processor tasks as possible.\nVisualising the Graph Dask is fundamentally a computational graph library, to understand what is happening in the background it can help to see these graphs (on smaller/simpler problems). This can be achieved by running;\nne = coast.Gridded(...) # come complex delayed methods called ne.data_variable.visualize() this will output a png image of the graph in the calling directory and could look like this;\n","categories":"","description":"Information on using Dask within the COAsT package\n","excerpt":"Information on using Dask within the COAsT package\n","ref":"/COAsT/docs/general-information/dask/","tags":"","title":"Dask"},{"body":"\nIn order to describe some of the usages of this repository, we prepared a list of examples and tutorials. This section is split into: ","categories":"","description":"Demonstrations and tutorials to help you get to know COAsT.\n","excerpt":"Demonstrations and tutorials to help you get to know COAsT.\n","ref":"/COAsT/docs/examples/","tags":"","title":"Usage"},{"body":"Your contributions to this repository are highly encouraged. We welcome bug reports, bug fixes, documentation enhancements, new features, and creative ideas.\nIf you’re new to COAsT or open-source development, we recommend visiting the GitHub “issues” tab to explore issues that pique your interest. You’ll find various issues listed under Documentation and Good first issues to help you get started. Once you’ve identified an issue that intrigues you, return here to set up your development environment.\nIf you have questions or need assistance, please refrain from filing an issue. Instead, visit our Discussion forum. We’re here to help!\n","categories":"","description":"Guidelines for contributing to the COAsT Python package, Includes Python style and structure.\n","excerpt":"Guidelines for contributing to the COAsT Python package, Includes …","ref":"/COAsT/docs/contributing/","tags":"","title":"Contributing to COAsT"},{"body":"Here you’ll discover valuable details regarding the project’s libraries, features, tests, and CI/CD setup.\n","categories":"","description":"General information about some characteristics of the package.\n","excerpt":"General information about some characteristics of the package.\n","ref":"/COAsT/docs/general-information/","tags":"","title":"General Information"},{"body":"In order to run Build Tests locally, you need to follow the procedures of Getting Started related to Development use installation and Example data files.\nAfter that you can run:\npip install . \u0026\u0026 pytest unit_testing/unit_test.py -s It will ask you for the path of your example files.\n","categories":"","description":"Build testing.\n","excerpt":"Build testing.\n","ref":"/COAsT/docs/general-information/build_test/","tags":"","title":"Build test"},{"body":"This is a demonstration script for using the Altimetry object in the COAsT package. This object has strict data formatting requirements, which are outlined in altimetry.py.\nRelevant imports and filepath configuration # Begin by importing coast and other packages import coast root = \"./\" # And by defining some file paths dn_files = root + \"./example_files/\" fn_nemo_dat = dn_files + \"coast_example_nemo_data.nc\" fn_nemo_dom = dn_files + \"coast_example_nemo_domain.nc\" fn_nemo_config = root + \"./config/example_nemo_grid_t.json\" fn_altimetry = dn_files + \"coast_example_altimetry_data.nc\" fn_altimetry_config = root + \"./config/example_altimetry.json\" /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pydap/lib.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.responses')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.handlers')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.tests')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages Load data # We need to load in a NEMO object for doing NEMO things. nemo = coast.Gridded(fn_nemo_dat, fn_nemo_dom, config=fn_nemo_config) # And now we can load in our Altimetry data. By default, Altimetry is set up # to read in CMEMS netCDF files. However, if no path is supplied, then the # object's dataset will be initialised as None. Custom data can then be loaded # if desired, as long as it follows the data formatting for Altimetry. # altimetry = coast.Altimetry(fn_altimetry) altimetry = coast.Altimetry(fn_altimetry, config=fn_altimetry_config) /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/xarray/core/dataset.py:278: UserWarning: The specified chunks separate the stored chunks along dimension \"time_counter\" starting at index 2. This could degrade performance. Instead, consider rechunking after loading. ././config/example_altimetry.json Altimetry object at 0x560d5d3e2980 initialised /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/xarray/core/dataset.py:278: UserWarning: The specified chunks separate the stored chunks along dimension \"time\" starting at index 1000. This could degrade performance. Instead, consider rechunking after loading. Subsetting # Before going any further, lets just cut out the bit of the altimetry that # is over the model domain. This can be done using `subset_indices_lonlat_box` # to find relevant indices and then `isel` to extract them. The data here is then also # also thinned slightly. ind = altimetry.subset_indices_lonlat_box([-10, 10], [45, 60]) ind = ind[::4] altimetry = altimetry.isel(t_dim=ind) Subsetting Altimetry object at 0x560d5d3e2980 indices in [-10, 10], [45, 60] Model interpolation # Before comparing our observations to the model, we will interpolate a model # variable to the same time and geographical space as the altimetry. This is # done using the obs_operator() method: altimetry.obs_operator(nemo, mod_var_name=\"ssh\", time_interp=\"nearest\") # Doing this has created a new interpolated variable called interp_ssh and # saved it back into our Altimetry object. Take a look at altimetry.dataset # to see for yourself. Interpolating Gridded object at 0x560d5d3e2980 \"ssh\" with time_interp \"nearest\" #altimetry.dataset # uncomment to print data object summary Interpolated vs observed # Next we will compare this interpolated variable to an observed variable # using some basic metrics. The basic_stats() routine can be used for this, # which calculates some simple metrics including differences, RMSE and # correlations. NOTE: This may not be a wise choice of variables. stats = altimetry.basic_stats(\"ocean_tide_standard_name\", \"interp_ssh\") Altimetry object at 0x560d5d3e2980 initialised # Take a look inside stats.dataset to see all of the new variables. When using # basic stats, the returned object is also an Altimetry object, so all of the # same methods can be applied. Alternatively, if you want to save the new # metrics to the original altimetry object, set 'create_new_object = False'. #stats.dataset # uncomment to print data object summary # Now we will do a more complex comparison using the Continuous Ranked # Probability Score (CRPS). For this, we need to hand over the model object, # a model variable and an observed variable. We also give it a neighbourhood # radius in km (nh_radius). crps = altimetry.crps(nemo, model_var_name=\"ssh\", obs_var_name=\"ocean_tide_standard_name\", nh_radius=20) # Again, take a look inside `crps.dataset` to see some new variables. Similarly # to basic_stats, `create_new_object` keyword arg can be set to `false` to save output to # the original altimetry object. #crps.dataset # uncomment to print data object summary Altimetry object at 0x560d5d3e2980 initialised Plotting data # Altimetry has a ready built quick_plot() routine for taking a look at any # of the observed or derived quantities above. So to take a look at the # 'ocean_tide_standard_name' variable: fig, ax = altimetry.quick_plot(\"ocean_tide_standard_name\") /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/cartopy/io/__init__.py:241: DownloadWarning: Downloading: https://naturalearth.s3.amazonaws.com/50m_physical/ne_50m_coastline.zip # As stats and crps are also `altimetry` objects, quick_plot() can also be used: fig, ax = crps.quick_plot(\"crps\") # stats quick_plot: fig, ax = stats.quick_plot(\"absolute_error\") ","categories":"","description":"Altimetry tutorial example.\n","excerpt":"Altimetry tutorial example.\n","ref":"/COAsT/docs/examples/notebooks/altimetry/altimetry_tutorial/","tags":"","title":"Altimetry tutorial"},{"body":"","categories":"","description":"Examples Altimety scripts for the COAsT package.\n","excerpt":"Examples Altimety scripts for the COAsT package.\n","ref":"/COAsT/docs/examples/notebooks/altimetry/","tags":"","title":"Altimety"},{"body":"This demonstration has two parts:\nClimatology.make_climatology(): This demonstration uses the COAsT package to calculate a climatological mean of an input dataset at a desired output frequency. Output can be written straight to file.\nClimatology.make_multiyear_climatology(): This demonstrations uses the COAsT package to calculate a climatological mean of an input dataset at a desired output frequency, over multiple years, but will work with single year datasets too.\nCOAsT and xarray should preserve any lazy loading and chunking. If defined properly in the read function, memory issues can be avoided and parallel processes will automatically be used.\nimport coast /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pydap/lib.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.responses')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.handlers')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.tests')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages Usage of coast.Climatology.make_climatology(). Calculates mean over a given period of time. This doesn’t take different years into account, unless using the ‘years’ frequency.\nroot = \"./\" # Paths to a single or multiple data files. dn_files = root + \"./example_files/\" fn_nemo_dat = dn_files + \"coast_example_nemo_data.nc\" fn_nemo_config = root + \"./config/example_nemo_grid_t.json\" # Set path for domain file if required. fn_nemo_dom = dn_files + \"coast_example_nemo_domain.nc\" # Define output filepath (optional: None or str) fn_out = None # Read in multiyear data (This example uses NEMO data from a single file.) nemo_data = coast.Gridded(fn_data=fn_nemo_dat, fn_domain=fn_nemo_dom, config=fn_nemo_config, ).dataset /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/xarray/core/dataset.py:278: UserWarning: The specified chunks separate the stored chunks along dimension \"time_counter\" starting at index 2. This could degrade performance. Instead, consider rechunking after loading. Calculate the climatology for temperature and sea surface height (ssh) as an example:\n# Select specific data variables. data = nemo_data[[\"temperature\", \"ssh\"]] # Define frequency -- Any xarray time string: season, month, etc climatology_frequency = \"month\" # Calculate the climatology and write to file. clim = coast.Climatology() clim_mean = clim.make_climatology(data, climatology_frequency, fn_out=fn_out) Below shows the structure of a dataset returned, containing 1 month worth of meaned temperature and sea surface height data:\n#clim_mean # uncomment to print data object summary Usage of coast.Climatology.multiyear_averages(). Calculates the mean over a specified period and groups the data by year-period. Here a fully working example is not available as multi-year example data is not in the example_files. However a working example using synthetic data is given in: tests/test_climatology.py. This method is designed to be compatible with multi-year datasets, but will work with single year datasets too.\n# Paths to a single or multiple data files. fn_nemo_data = \"/path/to/nemo/*.nc\" # Set path for domain file if required. fn_nemo_domain = None # Set path to configuration file fn_nemo_config = \"/path/to/nemo/*.json\" # Read in multiyear data (This example uses NEMO data from multiple datafiles.) nemo_data = coast.Gridded(fn_data=fn_nemo_data, fn_domain=fn_nemo_domain, config=fn_nemo_config, multiple=True).dataset Now calculate temperature and ssh means of each season across multiple years for specified data, using seasons module to specify time period.\nfrom coast._utils import seasons # Select specific data variables. data = nemo_data[[\"temperature\", \"ssh\"]] clim = coast.Climatology() # SPRING, SUMMER, AUTUMN, WINTER, ALL are valid values for seasons. clim_multiyear = clim.multiyear_averages(data, seasons.ALL, time_var='time', time_dim='t_dim') # Or explicitly defining specific month periods. # A list of tuples defining start and end month integers. The start months should be in chronological order. # (you may need to read/load the data again if it gives an error) month_periods = [(1,2), (12,2)] # Specifies January -\u003e February and December -\u003e February for each year of data. clim_multiyear = clim.multiyear_averages(data, month_periods , time_var='time', time_dim='t_dim') ","categories":"","description":"Climatology tutorial example.\n","excerpt":"Climatology tutorial example.\n","ref":"/COAsT/docs/examples/notebooks/general/climatology_tutorial/","tags":"","title":"Climatology tutorial"},{"body":"Contour subsetting (a vertical slice of data along a contour).\nThis is a demonstration script for using the Contour class in the COAsT package. This object has strict data formatting requirements, which are outlined in contour.py.\nThe code is taken directly from unit_tesing/unit_test.py\nIn this tutorial we take a look the following Isobath Contour Methods:\na. Extract isbath contour between two points b. Plot contour on map c. Calculate pressure along contour d. Calculate flow across contour e. Calculate pressure gradient driven flow across contour Load packages and define some file paths. import coast import matplotlib.pyplot as plt # Define some file paths root = \"./\" dn_files = root + \"./example_files/\" fn_nemo_dat_t = dn_files + \"nemo_data_T_grid.nc\" fn_nemo_dat_u = dn_files + \"nemo_data_U_grid.nc\" fn_nemo_dat_v = dn_files + \"nemo_data_V_grid.nc\" fn_nemo_dom = dn_files + \"coast_example_nemo_domain.nc\" # Configuration files describing the data files fn_config_t_grid = root + \"./config/example_nemo_grid_t.json\" fn_config_f_grid = root + \"./config/example_nemo_grid_f.json\" fn_config_u_grid = root + \"./config/example_nemo_grid_u.json\" fn_config_v_grid = root + \"./config/example_nemo_grid_v.json\" /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pydap/lib.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.responses')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.handlers')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.tests')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages Extract isobath contour between two points and create contour object. Create a gridded object with the grid only.\nnemo_f = coast.Gridded(fn_domain=fn_nemo_dom, config=fn_config_f_grid) Then create a contour object on the 200m isobath.\ncontours, no_contours = coast.Contour.get_contours(nemo_f, 200) Extract the indices for the contour in a specified box.\ny_ind, x_ind, contour = coast.Contour.get_contour_segment(nemo_f, contours[0], [50, -10], [60, 3]) Extract the contour for the specified indices.\ncont_f = coast.ContourF(nemo_f, y_ind, x_ind, 200) Plot contour on map plt.figure() coast.Contour.plot_contour(nemo_f, contour) plt.show() \u003cFigure size 640x480 with 0 Axes\u003e Calculate pressure along contour. Repeat the above procedure but on t-points.\nnemo_t = coast.Gridded(fn_data=fn_nemo_dat_t, fn_domain=fn_nemo_dom, config=fn_config_t_grid) contours, no_contours = coast.Contour.get_contours(nemo_t, 200) y_ind, x_ind, contour = coast.Contour.get_contour_segment(nemo_t, contours[0], [50, -10], [60, 3]) cont_t = coast.ContourT(nemo_t, y_ind, x_ind, 200) Now contruct pressure along this contour segment.\ncont_t.construct_pressure(1027) # This creates ``cont_t.data_contour.pressure_s`` and ``cont_t.data_contour.pressure_h_zlevels`` fields. Calculate flow across contour. Create the contour segement on f-points again.\nnemo_f = coast.Gridded(fn_domain=fn_nemo_dom, config=fn_config_f_grid) nemo_u = coast.Gridded(fn_data=fn_nemo_dat_u, fn_domain=fn_nemo_dom, config=fn_config_u_grid) nemo_v = coast.Gridded(fn_data=fn_nemo_dat_v, fn_domain=fn_nemo_dom, config=fn_config_v_grid) contours, no_contours = coast.Contour.get_contours(nemo_f, 200) y_ind, x_ind, contour = coast.Contour.get_contour_segment(nemo_f, contours[0], [50, -10], [60, 3]) cont_f = coast.ContourF(nemo_f, y_ind, x_ind, 200) Calculate the flow across the contour, pass u- and v- gridded velocity objects.\ncont_f.calc_cross_contour_flow(nemo_u, nemo_v) # This creates fields ``cont_f.data_cross_flow.normal_velocities`` and ## ``cont_f.data_cross_flow.depth_integrated_normal_transport`` Calculate pressure gradient driven flow across contour. The “calc_geostrophic_flow()” operates on f-grid objects and requires configuration files for the u- and v- grids.\ncont_f.calc_geostrophic_flow(nemo_t, config_u=fn_config_u_grid, config_v=fn_config_v_grid, ref_density=1027) \"\"\" This constructs: cont_f.data_cross_flow.normal_velocity_hpg cont_f.data_cross_flow.normal_velocity_spg cont_f.data_cross_flow.transport_across_AB_hpg cont_f.data_cross_flow.transport_across_AB_spg \"\"\" '\\n This constructs:\\n cont_f.data_cross_flow.normal_velocity_hpg\\n cont_f.data_cross_flow.normal_velocity_spg\\n cont_f.data_cross_flow.transport_across_AB_hpg\\n cont_f.data_cross_flow.transport_across_AB_spg\\n' ","categories":"","description":"Contour tutorial example.\n","excerpt":"Contour tutorial example.\n","ref":"/COAsT/docs/examples/notebooks/gridded/contour_tutorial/","tags":"","title":"Contour tutorial"},{"body":"The notebook proves a template and some instruction on how to create a dask wrapper\nMotivation Start with an xarray.DataArray object called myDataArray, that we want to pass into a function. That function will perform eager evaluation and return a numpy array, but we want lazy evaluation with the possibility to allow dask parallelism. See worked example in Process_data.seasonal_decomposition.\nImport dependencies import dask.array as da from dask import delayed import xarray as xr import numpy as np Step 1. (optional: allows dask to distribute computation across multiple cores, if not interested see comment 2) Partition data in myDataArray by chunking it up as desired. Note that chunking dimensions need to make sense for your particular problem! Here we just chunk along dim_2\nmyDataArray = myDataArray.chunk({\"dim_1\": myDataArray.dim_1.size, \"dim_2\": chunksize}) # can be more dimensions Then create a list containing all the array chunks as dask.delayed objects (e.g. 4 chunks =\u003e list contain 4 delayed objects)\nmyDataArray_partitioned = myDataArray.data.to_delayed().ravel() Comment 1 There are different ways to partition your data. For example, if you start off with a numpy array rather than an xarray DataArray you can just iterate over the array and partition it that way (the partitions do NOT need to be dask.delayed objects). For example see the very simple case here: https://docs.dask.org/en/stable/delayed.html\nThe method described in 1 is just very convenient for DataArrays where the multi-dimensional chunks may be the desired way to partition the data.\nStep 2. Call your eager evaluating function using dask.delayed and pass in your data. This returns a list containing the outputs from the function as dask.delayed objects. The list will have the same length as myDataArray_partitioned\ndelayed_myFunction_output = [ delayed(myFunction)(aChunk, other_args_for_myFunction) for aChunk in myDataArray_partitioned ] Step 3. Convert the lists of delayed objects to lists of dask arrays to allow array operations. It’s possible this step is not necessary!\ndask_array_list = [] for chunk_idx, aChunk in enumerate(delayed_myFunction_output): # When converting from dask.delayed to dask.array, you must know the shape of the # array. In this example we know this from the chunk sizes of the original DataArray chunk_shape = (myDataArray.chunks[0][0], myDataArray.chunks[1][chunk_idx]) dask_array_list.append(da.from_delayed(aChunk, shape=chunk_shape, dtype=float)) Step 4. Concatenate the array chunks together to get a single dask.array. This can be assigned to a new DataArray as desired.\nmyOutputArray = da.concatenate(dask_array_list, axis=1) Comment 2 If you skipped step 1., i.e. just want a lazy operation and no parallelism, you can just do this\nmyOutputArray = da.from_delayed( delayed(myFunction)(myDataArray, other_args_for_myFunction), shape=myDataArray.shape, dtype=float ) ","categories":"","description":"Dask wrapper template tutorial example.\n","excerpt":"Dask wrapper template tutorial example.\n","ref":"/COAsT/docs/examples/notebooks/general/dask_wrapper_template_tutorial/","tags":"","title":"Dask wrapper template tutorial"},{"body":"Using COAsT to compute the Empirical Orthogonal Functions (EOFs) of your data\nRelevant imports and filepath configuration # Begin by importing coast and other packages import coast import xarray as xr import matplotlib.pyplot as plt # Define some file paths root = \"./\" dn_files = root + \"./example_files/\" fn_nemo_dat_t = dn_files + \"nemo_data_T_grid.nc\" fn_nemo_dom = dn_files + \"coast_example_nemo_domain.nc\" fn_nemo_config = root + \"./config/example_nemo_grid_t.json\" /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pydap/lib.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.responses')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.handlers')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.tests')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages Loading data # Load data variables that are on the NEMO t-grid nemo_t = coast.Gridded( fn_data = fn_nemo_dat_t, fn_domain = fn_nemo_dom, config = fn_nemo_config ) Compute EOF For a variable (or subset of a variable) with two spatial dimensions and one temporal dimension, i.e. (x,y,t), the EOFs, temporal projections and variance explained can be computed by calling the ‘eofs’ method, and passing in the ssh DataArray as an argument. For example, for the sea surface height field, we can do\neof_data = coast.compute_eofs( nemo_t.dataset.ssh ) The method returns an xarray dataset that contains the EOFs, temporal projections and variance as DataArrays\n#eof_data # uncomment to print data object summary Inspect EOFs The variance explained of the first four modes is\n# eof_data.variance.sel(mode=[1,2,3,4]) ## uncomment Plotting And the EOFs and temporal projections can be quick plotted:\neof_data.EOF.sel(mode=[1,2,3,4]).plot.pcolormesh(col='mode',col_wrap=2,x='longitude',y='latitude') \u003cxarray.plot.facetgrid.FacetGrid at 0x7fb34d977520\u003e eof_data.temporal_proj.sel(mode=[1,2,3,4]).plot(col='mode',col_wrap=2,x='time') \u003cxarray.plot.facetgrid.FacetGrid at 0x7fb2f86bc760\u003e Complex EOFs The more exotic hilbert complex EOFs can also be computed to investigate the propagation of variability, for example:\nheof_data = coast.compute_hilbert_eofs( nemo_t.dataset.ssh ) #heof_data # uncomment to print data object summary now with the modes expressed by their amplitude and phase, the spatial propagation of the variability can be examined through the EOF_phase.\n","categories":"","description":"Eof tutorial example.\n","excerpt":"Eof tutorial example.\n","ref":"/COAsT/docs/examples/notebooks/general/eof_tutorial/","tags":"","title":"Eof tutorial"},{"body":"This is a demonstration script for how to export intermediate data from COAsT to netCDF files for later analysis or storage. The tutorial showcases the xarray.to_netcdf() method. http://xarray.pydata.org/en/stable/generated/xarray.Dataset.to_netcdf.html\nBegin by importing COAsT and other packages import coast import xarray as xr /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pydap/lib.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.responses')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.handlers')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.tests')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages Now define some file paths root = \"./\" # And by defining some file paths dn_files = root + \"./example_files/\" fn_nemo_dat = dn_files + \"coast_example_nemo_data.nc\" fn_nemo_dom = dn_files + \"coast_example_nemo_domain.nc\" config = root + \"./config/example_nemo_grid_t.json\" ofile = \"example_export_output.nc\" # The target filename for output We need to load in a NEMO object for doing NEMO things nemo = coast.Gridded(fn_nemo_dat, fn_nemo_dom, config=config) /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/xarray/core/dataset.py:278: UserWarning: The specified chunks separate the stored chunks along dimension \"time_counter\" starting at index 2. This could degrade performance. Instead, consider rechunking after loading. We can export the whole xr.DataSet to a netCDF file Other file formats are available. From the documentation:\nNETCDF4: Data is stored in an HDF5 file, using netCDF4 API features. NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only netCDF 3 compatible API features. NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format, which fully supports 2+ GB files, but is only compatible with clients linked against netCDF version 3.6.0 or later. NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not handle 2+ GB files very well. Mode - ‘w’ (write) is the default. Other options from the documentation:\nmode ({“w”, “a”}, default: “w”) – Write (‘w’) or append (‘a’) mode. If mode=’w’, any existing file at this location will be overwritten. If mode=’a’, existing variables will be overwritten. Similarly xr.DataSets collections of variables or xr.DataArray variables can be exported to netCDF for objects in the TRANSECT, TIDEGAUGE, etc classes.\nnemo.dataset.to_netcdf(ofile, mode=\"w\", format=\"NETCDF4\") Alternatively a single variable (an xr.DataArray object) can be exported nemo.dataset[\"temperature\"].to_netcdf(ofile, format=\"NETCDF4\") Check the exported file is as you expect Perhaps by using ncdump -h example_export_output.nc, or load the file and see that the xarray structure is preserved.\nobject = xr.open_dataset(ofile) object.close() # close file associated with this object ","categories":"","description":"Export to netcdf tutorial example.\n","excerpt":"Export to netcdf tutorial example.\n","ref":"/COAsT/docs/examples/notebooks/general/export_to_netcdf_tutorial/","tags":"","title":"Export to netcdf tutorial"},{"body":"GitHub actions diagram This is a collection of flowcharts for all the GitHub actions used across the COAsT and COAsT-site repos\nCOAsT building Packages graph LR; subgraph publish_package - runs on push to master A1[Setup python]-- 3.8 --\u003eB1; B1[Install dependencies]--\u003eC1; C1[Setup Enviroment]--\u003eD1; D1[Build package]--\u003eE1; E1[Test Package Install]--\u003eF1 F1[Publish to pypi]--\u003eG1 G1[Generate Conda Metadata]--\u003eH1 H1[Publish to Anaconda] end; subgraph build_package - runs on push to non-master A[Setup python]-- 3.8 and 3.9 --\u003eB; B[Install dependencies]--\u003eC; C[Setup Enviroment]--\u003eD; D[Build package]--\u003eE; E[Test Package Install]--\u003eF F[Generate Conda Metadata] end; Verification, Formatting ans Pylint graph LR subgraph formatting - runs on pull requests A[Setup python]-- 3.9 --\u003eB; B[Install black]--\u003eC; C[Check formatting]--\u003e D; D[Apply formatting] end; subgraph pylint - runs on pull requests A1[Setup python]-- 3.9 --\u003eB1; B1[Install pylint]--\u003eC1; C1[Check Score]-- if test pass --\u003e D1; D1[Update Score] end; subgraph verifiy_package - runs for every push A2[Setup python]-- 3.8 and 3.9 --\u003eB2; B2[Install dependencies]--\u003eC2; C2[Lint]--\u003eD2; D2[Test] end; click B1 \"https://www.github.com\" \"tooltip\" interactions with other repos flowchart LR subgraph b1[push_notebooks - runs on push to develop] direction LR subgraph b2[COAsT site - markdown ] direction TB a[checkout docsy site] --\u003eb b[checkout coast] --\u003ec c[create environment] --\u003ed d[execute notebooks] --\u003ee e[covert notebooks to MD] --\u003ef f[move images to static dir] --\u003eg g[commit changes] end t[Repository Dispatch] -- event pushed --\u003e b2 end click a \"https://github.com/British-Oceanographic-Data-Centre/COAsT-site\" \"Docsy site for COAsT repo\" flowchart LR subgraph b3[push_docstrings - runs on push to master] direction LR subgraph b4[COAsT site - docstrings ] direction TB a1[checkout docsy site] --\u003eb1 b1[checkout coast] --\u003ec1 c1[add python] --\u003ed1 d1[covert docstrings] --\u003ee1 e1[commit changes] end r[Repository Dispatch] -- event pushed --\u003e b4 end click a1 \"https://github.com/British-Oceanographic-Data-Centre/COAsT-site\" \"Docsy site for COAsT repo\" Generate unit test contents file graph LR subgraph generate-test-contents - runs on pull_request A[checkout COAsT]--\u003eB; B[install package]--\u003eC; C[make example files dir]--\u003e D; D[run generate_unit_test_contents.py]--\u003eE E[commit changes] end; COAsT-site These are the actions used on the COAsT-site repo.\nConvert to markdown See Interactions with other repos for the related markdown and docstring workflows\nBuild site graph LR subgraph hugo - runs on push to master A[checkout site]--\u003eB; B[Setup Hugo] -- v0.70.0 --\u003eC; C[Setup Nodejs]-- v12 --\u003e D; D[Build]--\u003eE E[Deploy] end; ","categories":"","description":"Flowchart of Github actions active on the COAsT and COAsT-Site repositories.\n","excerpt":"Flowchart of Github actions active on the COAsT and COAsT-Site …","ref":"/COAsT/docs/general-information/github_actions_flowchart/","tags":"","title":"Github Actions Flowchart"},{"body":"An introduction to the Gridded class. Loading variables and grid information.\nThis is designed to be a brief introduction to the Gridded class including: 1. Creation of a Gridded object 2. Loading data into the Gridded object. 3. Combining Gridded output and Gridded domain data. 4. Interrogating the Gridded object. 5. Basic manipulation ans subsetting 6. Looking at the data with matplotlib\nLoading and Interrogating Begin by importing COAsT and define some file paths for NEMO output data and a NEMO domain, as an example of model data suitable for the Gridded object.\nimport coast import matplotlib.pyplot as plt import datetime import numpy as np # Define some file paths root = \"./\" dn_files = root + \"./example_files/\" fn_nemo_dat = dn_files + \"coast_example_nemo_data.nc\" fn_nemo_dom = dn_files + \"coast_example_nemo_domain.nc\" fn_config_t_grid = root + \"./config/example_nemo_grid_t.json\" /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pydap/lib.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.responses')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.handlers')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.tests')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages We can create a new Gridded object by simple calling coast.Gridded(). By passing this a NEMO data file and a NEMO domain file, COAsT will combine the two into a single xarray dataset within the Gridded object. Each individual Gridded object should be for a specified NEMO grid type, which is specified in a configuration file which is also passed as an argument. The Dask library is switched on by default, chunking can be specified in the configuration file.\nnemo_t = coast.Gridded(fn_data = fn_nemo_dat, fn_domain = fn_nemo_dom, config=fn_config_t_grid) /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/xarray/core/dataset.py:278: UserWarning: The specified chunks separate the stored chunks along dimension \"time_counter\" starting at index 2. This could degrade performance. Instead, consider rechunking after loading. Our new Gridded object nemo_t contains a variable called dataset, which holds information on the two files we passed. Let’s have a look at this:\n#nemo_t.dataset # uncomment to print data object summary This is an xarray dataset, which has all the information on netCDF style structures. You can see dimensions, coordinates and data variables. At the moment, none of the actual data is loaded to memory and will remain that way until it needs to be accessed.\nAlong with temperature (which has been renamed from votemper) a number of other things have happen under the hood:\nThe dimensions have been renamed to t_dim, x_dim, y_dim, z_dim The coordinates have been renamed to time, longitude, latitude and depth_0. These are the coordinates for this grid (the t-grid). Also depth_0 has been calculated as the 3D depth array at time zero. The variables e1, e2 and e3_0 have been created. These are the metrics for the t-grid in the x-dim, y-dim and z_dim (at time zero) directions. So we see that the Gridded class has standardised some variable names and created an object based on this discretisation grid by combining the appropriate grid information with all the variables on that grid.\nWe can interact with this as an xarray Dataset object. So to extract a specific variable (say temperature):\nssh = nemo_t.dataset.ssh #ssh # uncomment to print data object summary Or as a numpy array:\nssh_np = ssh.values #ssh_np.shape # uncomment to print data object summary Then lets plot up a single time snapshot of ssh using matplotlib:\nplt.pcolormesh(nemo_t.dataset.longitude, nemo_t.dataset.latitude, nemo_t.dataset.ssh[0]) \u003cmatplotlib.collections.QuadMesh at 0x7fac889f3910\u003e Some Manipulation There are currently some basic subsetting routines for Gridded objects, to cut out specified regions of data. Fundamentally, this can be done using xarray’s isel or sel routines to index the data. In this case, the Gridded object will pass arguments straight through to xarray.isel.\nLets get the indices of all model points within 111km km of (5W, 55N):\nind_y, ind_x = nemo_t.subset_indices_by_distance(centre_lon=-5, centre_lat=55, radius=111) #ind_x.shape # uncomment to print data object summary Now create a new, smaller subsetted Gridded object by passing those indices to isel.\nnemo_t_subset = nemo_t.isel(x_dim=ind_x, y_dim=ind_y) #nemo_t_subset.dataset # uncomment to print data object summary Alternatively, xarray.isel can be applied directly to the xarray.Dataset object. A longitude/latitude box of data can also be extracted using Gridded.subset_indices().\nPlotting example for NEMO-ERSEM biogechemical variables Import COAsT, define some file paths for NEMO-ERSEM output data and a NEMO domain, and read/load your NEMO-ERSEM data into a gridded object, example:\nimport coast import matplotlib.pyplot as plt # Define some file paths root = \"./\" dn_files = root + \"./example_files/\" fn_bgc_dat = dn_files + \"coast_example_SEAsia_BGC_1990.nc\" fn_bgc_dom = dn_files + \"coast_example_domain_SEAsia.nc\" fn_config_bgc_grid = root + \"./config/example_nemo_bgc.json\" nemo_bgc = coast.Gridded(fn_data = fn_bgc_dat, fn_domain = fn_bgc_dom, config=fn_config_bgc_grid) #nemo_bgc.dataset # uncomment to print data object summary As an example plot a snapshot of dissolved inorganic carbon at the sea surface\nfig = plt.figure() plt.pcolormesh( nemo_bgc.dataset.longitude, nemo_bgc.dataset.latitude, nemo_bgc.dataset.dic.isel(t_dim=0).isel(z_dim=0), cmap=\"RdYlBu_r\", vmin=1600, vmax=2080, ) plt.colorbar() plt.title(\"DIC, mmol/m^3\") plt.xlabel(\"longitude\") plt.ylabel(\"latitude\") plt.show() /tmp/ipykernel_2672/2498690501.py:2: UserWarning: The input coordinates to pcolormesh are interpreted as cell centers, but are not monotonically increasing or decreasing. This may lead to incorrectly calculated cell edges, in which case, please supply explicit cell edges to pcolormesh. ","categories":"","description":"Introduction to gridded class example.\n","excerpt":"Introduction to gridded class example.\n","ref":"/COAsT/docs/examples/notebooks/gridded/introduction_to_gridded_class/","tags":"","title":"Introduction to gridded class"},{"body":"Example useage of Profile object. Overview INDEXED type class for storing data from a CTD Profile (or similar down and up observations). The structure of the class is based around having discrete profile locations with independent depth dimensions and coords. The class dataset should contain two dimensions:\n\u003e id_dim :: The profiles dimension. Each element of this dimension contains data (e.g. cast) for an individual location. \u003e z_dim :: The dimension for depth levels. A profile object does not need to have shared depths, so NaNs might be used to pad any depth array. Alongside these dimensions, the following minimal coordinates should also be available:\n\u003e longitude (id_dim) :: 1D array of longitudes, one for each id_dim \u003e latitude (id_dim) :: 1D array of latitudes, one for each id_dim \u003e time (id_dim) :: 1D array of times, one for each id_dim \u003e depth (id_dim, z_dim) :: 2D array of depths, with different depth levels being provided for each profile. Note that these depth levels need to be stored in a 2D array, so NaNs can be used to pad out profiles with shallower depths. \u003e id_name (id_dim) :: [Optional] Name of id_dim/case or id_dim number. Introduction to Profile and ProfileAnalysis Below is a description of the available example scripts for this class as well as an overview of validation using Profile and ProfileAnalysis.\nExample Scripts Please see COAsT/example_scripts/notesbooks/runnable_notebooks/profile_validation/*.ipynb and COAsT/example_scripts/profile_validation/*.py for some notebooks and equivalent scripts which demonstrate how to use the Profile and ProfileAnalysis classes for model validation.\nanalysis_preprocess_en4.py : If you’re using EN4 data, this kind of script might be your first step for analysis.\nanalysis_extract_and_compare.py: This script shows you how to extract the nearest model profiles, compare them with EN4 observations and get errors throughout the vertical dimension and averaged in surface and bottom zones\nanalysis_extract_and_compare_single_process.py: This script does the same as number 2. However, it is modified slightly to take a command line argument which helps it figure out which dates to analyse. This means that this script can act as a template for jug type parallel processing on, e.g. JASMIN.\nanalysis_mask_means.py: This script demonstrates how to use boolean masks to obtain regional averages of profiles and errors.\nanalysis_average_into_grid_boxes.py: This script demonstrates how to average the data inside a Profile object into regular grid boxes and seasonal climatologies.\nLoad and preprocess profile and model data Start by loading python packages\nimport coast from os import path import numpy as np import matplotlib.pyplot as plt /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pydap/lib.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.responses')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.handlers')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.tests')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages We can create a new Profile object easily:\nprofile = coast.Profile() Currently, this object is empty, and contains no dataset. There are some reading routines currently available in Profile for reading EN4 or WOD data files. These can be used to easily read data into your new profile object:\n# Read WOD data into profile object fn_prof = path.join(\"example_files\",\"WOD_example_ragged_standard_level.nc\") profile.read_wod( fn_prof ) # Read EN4 data into profile object (OVERWRITES DATASET) fn_prof = path.join(\"example_files\", \"coast_example_en4_201008.nc\") fn_cfg_prof = path.join(\"config\",\"example_en4_profiles.json\") profile = coast.Profile(config=fn_cfg_prof) profile.read_en4( fn_prof ) config/example_en4_profiles.json Alternatively, you can pass an xarray.dataset straight to Profile:\nprofile = coast.Profile( dataset = your_dataset, config = config_file [opt] ) If you are using EN4 data, you can use the process_en4() routine to apply quality control flags to the data (replacing with NaNs):\nprocessed_profile = profile.process_en4() profile = processed_profile We can do some simple spatial and temporal manipulations of this data:\n# Cut out a geographical box profile = profile.subset_indices_lonlat_box(lonbounds = [-15, 15], latbounds = [45, 65]) # Cut out a time window profile = profile.time_slice( date0 = np.datetime64('2010-01-01'), date1 = np.datetime64(\"2010-01-20\")) Inspect profile locations Have a look inside the profile.py class to see what it can do\nprofile.plot_map() /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/cartopy/io/__init__.py:241: DownloadWarning: Downloading: https://naturalearth.s3.amazonaws.com/50m_physical/ne_50m_coastline.zip (\u003cFigure size 640x480 with 2 Axes\u003e, \u003cGeoAxes: \u003e) Direct Model comparison using obs_operator() method There are a number of routines available for interpolating in the horizontal, vertical and in time to do direct comparisons of model and profile data. Profile.obs_operator will do a nearest neighbour spatial interpolation of the data in a Gridded object to profile latitudes/longitudes. It will also do a custom time interpolation.\nFirst load some model data: root = \"./\" # And by defining some file paths dn_files = root + \"./example_files/\" fn_nemo_dat = path.join(dn_files, \"coast_example_nemo_data.nc\") fn_nemo_dom = path.join(dn_files, \"coast_example_nemo_domain.nc\") fn_nemo_config = path.join(root, \"./config/example_nemo_grid_t.json\") # Create gridded object: nemo = coast.Gridded(fn_nemo_dat, fn_nemo_dom, multiple=True, config=fn_nemo_config) /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/xarray/core/dataset.py:278: UserWarning: The specified chunks separate the stored chunks along dimension \"time_counter\" starting at index 2. This could degrade performance. Instead, consider rechunking after loading. Create a landmask array in Gridded In this example we add a landmask variable to the Gridded dataset. When this is present, the obs_operator will use this to interpolation to the nearest wet point. If not present, it will just take the nearest grid point (not implemented).\nWe also rename the depth at initial time coordinate depth_0 to depth as this is expected by Profile()\nnemo.dataset[\"landmask\"] = nemo.dataset.bottom_level == 0 nemo.dataset = nemo.dataset.rename({\"depth_0\": \"depth\"}) # profile methods will expect a `depth` coordinate Interpolate model to horizontal observation locations using obs_operator() method # Use obs operator for horizontal remapping of Gridded onto Profile. model_profiles = profile.obs_operator(nemo) /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/coast/data/profile.py:456: UserWarning: Converting non-nanosecond precision timedelta values to nanosecond precision. This behavior can eventually be relaxed in xarray, as it is an artifact from pandas which is now beginning to support non-nanosecond precision values. This warning is caused by passing non-nanosecond np.datetime64 or np.timedelta64 values to the DataArray or Variable constructor; it can be silenced by converting the values to nanosecond precision ahead of time. /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/coast/data/profile.py:456: UserWarning: Converting non-nanosecond precision timedelta values to nanosecond precision. This behavior can eventually be relaxed in xarray, as it is an artifact from pandas which is now beginning to support non-nanosecond precision values. This warning is caused by passing non-nanosecond np.datetime64 or np.timedelta64 values to the DataArray or Variable constructor; it can be silenced by converting the values to nanosecond precision ahead of time. Now that we have interpolated the model onto Profiles, we have a new Profile object called model_profiles. This can be used to do some comparisons with our original processed_profile object, which we created above.\nDiscard profiles where the interpolation distance is too large However maybe we first want to restrict the set of model profiles to those that were close to the observations; perhaps, for example, the observational profiles are beyond the model domain. The model resolution would be an appropriate scale to pick\ntoo_far = 7 # distance km keep_indices = model_profiles.dataset.interp_dist \u003c= too_far model_profiles = model_profiles.isel(id_dim=keep_indices) # Also drop the unwanted observational profiles profile = profile.isel(id_dim=keep_indices) Profile analysis Create an object for Profile analysis Let’s make our ProfileAnalysis object:\nanalysis = coast.ProfileAnalysis() We can use ProfileAnalysis.interpolate_vertical to interpolate all variables within a Profile object. This can be done onto a set of reference depths or, matching another object’s depth coordinates by passing another profile object. Let’s interpolate our model profiles onto observations depths, then interpolate both onto a set of reference depths:\n### Set depth averaging settings ref_depth = np.concatenate((np.arange(1, 100, 2), np.arange(100, 300, 5), np.arange(300, 1000, 50))) # Interpolate model profiles onto observation depths model_profiles_interp = analysis.interpolate_vertical(model_profiles, profile, interp_method=\"linear\") # Vertical interpolation of model profiles to reference depths model_profiles_interp_ref = analysis.interpolate_vertical(model_profiles_interp, ref_depth) # Interpolation of obs profiles to reference depths profile_interp_ref = analysis.interpolate_vertical(profile, ref_depth) However, there is a problem here as the interpolate_vertical() method tries to map the whole contents of profile to the ref_depth and the profile object contains some binary data from the original qc flags. The data from the qc flags was mapped using process_en4() so the original qc entries can be removed.\n## Strip out old QC variables profile.dataset = profile.dataset.drop_vars(['qc_potential_temperature','qc_practical_salinity', 'qc_depth','qc_time', 'qc_flags_profiles','qc_flags_levels']) # Interpolation of obs profiles to reference depths profile_interp_ref = analysis.interpolate_vertical(profile, ref_depth) Differencing Now that we have two Profile objects that are horizontally and vertically comparable, we can use difference() to get some basic errors:\ndifferences = analysis.difference(profile_interp_ref, model_profiles_interp_ref) This will return a new Profile object that contains the variable difference, absolute differences and square differences at all depths and means for each profile.\nType\ndifferences.dataset to see what it returns\n# E.g. plot the differences on ind_dim vs z_dim axes differences.dataset.diff_temperature.plot() \u003cmatplotlib.collections.QuadMesh at 0x7f55f059ec50\u003e # or a bit prettier on labelled axes cmap=plt.get_cmap('seismic') fig = plt.figure(figsize=(8, 3)) plt.pcolormesh( differences.dataset.time, ref_depth, differences.dataset.diff_temperature.T, label='abs_diff', cmap=cmap, vmin=-5, vmax=5) plt.ylim([0,200]) plt.gca().invert_yaxis() plt.ylabel('depth') plt.colorbar( label='temperature diff (obs-model)') \u003cmatplotlib.colorbar.Colorbar at 0x7f55f03581c0\u003e Layer Averaging We can use the Profile object to get mean values between specific depth levels or for some layer above the bathymetric depth. The former can be done using ProfileAnalysis.depth_means(), for example the following will return a new Profile object containing the means of all variables between 0m and 5m:\nprofile_surface = analysis.depth_means(profile, [0, 5]) # 0 - 5 metres But since this can work on any Profile object it would be more interesting to apply it to the differences between the interpolated observations and model points\nsurface_def = 10 # in metres model_profiles_surface = analysis.depth_means(model_profiles_interp_ref, [0, surface_def]) obs_profiles_surface = analysis.depth_means(profile_interp_ref, [0, surface_def]) surface_errors = analysis.difference(obs_profiles_surface, model_profiles_surface) # Plot (observation - model) upper 10m averaged temperatures surface_errors.plot_map(var_str=\"diff_temperature\") (\u003cFigure size 640x480 with 2 Axes\u003e, \u003cGeoAxes: \u003e) This can be done for any arbitrary depth layer defined by two depths.\nHowever, in some cases it may be that one of the depth levels is not defined by a constant, e.g. when calculating bottom means. In this case you may want to calculate averages over a height from the bottom that is conditional on the bottom depth. This can be done using ProfileAnalysis.bottom_means(). For example:\nbottom_height = [10, 50, 100] # Average over bottom heights of 10m, 30m and 100m for... bottom_thresh = [100, 500, np.inf] # ...bathymetry depths less than 100m, 100-500m and 500-infinite model_profiles_bottom = analysis.bottom_means(model_profiles_interp_ref, bottom_height, bottom_thresh) similarly compute the same for the observations… though first we have to patch in a bathymetry variable that will be expected by the method. Grab it from the model dataset.\nprofile_interp_ref.dataset[\"bathymetry\"] = ([\"id_dim\"], model_profiles_interp_ref.dataset[\"bathymetry\"].values) obs_profiles_bottom = analysis.bottom_means(profile_interp_ref, bottom_height, bottom_thresh) Now the difference can be calculated\nbottom_errors = analysis.difference( obs_profiles_bottom, model_profiles_bottom) # Plot (observation - model) upper 10m averaged temperatures bottom_errors.plot_map(var_str=\"diff_temperature\") (\u003cFigure size 640x480 with 2 Axes\u003e, \u003cGeoAxes: \u003e) NOTE1: The bathymetry variable does not actually need to contain bathymetric depths, it can also be used to calculate means above any non-constant surface. For example, it could be mixed layer depth.\nNOTE2: This can be done for any Profile object. So, you could use this workflow to also average a Profile derived from the difference() routine.\n# Since they are indexed by 'id_dim' they can be plotted against time fig = plt.figure(figsize=(8, 3)) plt.plot( surface_errors.dataset.time, surface_errors.dataset.diff_temperature, '.', label='surf T' ) plt.plot( bottom_errors.dataset.time, bottom_errors.dataset.diff_temperature, '.', label='bed T' ) plt.xlabel('time') plt.ylabel('temperature errors') plt.legend() plt.title(\"Temperature diff (obs-model)\") Text(0.5, 1.0, 'Temperature diff (obs-model)') Regional (Mask) Averaging We can use Profile in combination with MaskMaker to calculate averages over regions defined by masks. For example, to get the mean errors in the North Sea. Start by creating a list of boolean masks we would like to use:\nmm = coast.MaskMaker() # Define Regional Masks regional_masks = [] # Define convenient aliases based on nemo data lon = nemo.dataset.longitude.values lat = nemo.dataset.latitude.values bathy = nemo.dataset.bathymetry.values # Add regional mask for whole domain regional_masks.append(np.ones(lon.shape)) # Add regional mask for English Channel regional_masks.append(mm.region_def_nws_english_channel(lon, lat, bathy)) region_names = [\"whole_domain\",\"english_channel\",] Next, we must make these masks into datasets using MaskMaker.make_mask_dataset. Masks should be 2D datasets defined by booleans. In our example here we have used the latitude/longitude array from the nemo object, however it can be defined however you like.\nmask_list = mm.make_mask_dataset(lon, lat, regional_masks) Then we use ProfileAnalysis.determine_mask_indices to figure out which profiles in a Profile object lie within each regional mask:\nmask_indices = analysis.determine_mask_indices(profile, mask_list) This returns an object called mask_indices, which is required to pass to ProfileAnalysis.mask_means(). This routine will return a new xarray dataset containing averaged data for each region:\nmask_means = analysis.mask_means(profile, mask_indices) which can be visualised or further processed\nfor count_region in range(len(region_names)): plt.plot( mask_means.profile_mean_temperature.isel(dim_mask=count_region), mask_means.profile_mean_depth.isel(dim_mask=count_region), label=region_names[count_region], marker=\".\", linestyle='none') plt.ylim([10,1000]) plt.yscale(\"log\") plt.gca().invert_yaxis() plt.xlabel('temperature'); plt.ylabel('depth') plt.legend() \u003cmatplotlib.legend.Legend at 0x7f55f03eadd0\u003e Gridding Profile Data If you have large amount of profile data you may want to average it into grid boxes to get, for example, mean error maps or climatologies. This can be done using ProfileAnalysis.average_into_grid_boxes().\nWe can create a gridded dataset shape (y_dim, x_dim) from all the data using:\ngrid_lon = np.arange(-15, 15, 0.5) grid_lat = np.arange(45, 65, 0.5) prof_gridded = analysis.average_into_grid_boxes(profile, grid_lon, grid_lat) # NB this method does not separately treat `z_dim`, see docstr lat = prof_gridded.dataset.latitude lon = prof_gridded.dataset.longitude temperature = prof_gridded.dataset.temperature plt.pcolormesh( lon, lat, temperature) plt.title('gridded mean temperature') plt.colorbar() \u003cmatplotlib.colorbar.Colorbar at 0x7f55e0dfaad0\u003e Alternatively, we can calculate averages for each season:\nprof_gridded_DJF = analysis.average_into_grid_boxes(profile, grid_lon, grid_lat, season=\"DJF\", var_modifier=\"_DJF\") prof_gridded_MAM = analysis.average_into_grid_boxes(profile, grid_lon, grid_lat, season=\"MAM\", var_modifier=\"_MAM\") prof_gridded_JJA = analysis.average_into_grid_boxes(profile, grid_lon, grid_lat, season=\"JJA\", var_modifier=\"_JJA\") prof_gridded_SON = analysis.average_into_grid_boxes(profile, grid_lon, grid_lat, season=\"SON\", var_modifier=\"_SON\") Here, season specifies which season to average over and var_modifier is added to the end of all variable names in the object’s dataset.\nNB with the example data only DJF has any data.\nThis function returns a new Gridded object. It also contains a new variable called grid_N, which stores how many profiles were averaged into each grid box. You may want to use this when using or extending the analysis. E.g. use it with plot symbol size\ntemperature = prof_gridded_DJF.dataset.temperature_DJF N = prof_gridded_DJF.dataset.grid_N_DJF plt.scatter( lon, lat, c=temperature, s=N) plt.title('DJF gridded mean temperature') plt.colorbar() \u003cmatplotlib.colorbar.Colorbar at 0x7f55e0d09ea0\u003e ","categories":"","description":"Introduction to profile class example.\n","excerpt":"Introduction to profile class example.\n","ref":"/COAsT/docs/examples/notebooks/profile/introduction_to_profile_class/","tags":"","title":"Introduction to profile class"},{"body":"A demonstration of the MaskMaker class to build and use regional masking\nMaskMasker is a class of methods to assist with making regional masks within COAsT. Presently the mask generated are external to MaskMaker. Masks are constructed as gridded boolean numpy array for each region, which are stacked over a dim_mask dimension. The mask arrays are generated on a supplied horizontal grid. The masks are then stored in xarray objects along with regions names.\nExamples are given working with Gridded and Profile data.\nRelevant imports and filepath configuration import coast import numpy as np from os import path import matplotlib.pyplot as plt import matplotlib.colors as colors # colormap fiddling import xarray as xr /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pydap/lib.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.responses')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.handlers')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.tests')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages # set some paths root = \"./\" dn_files = root + \"./example_files/\" fn_nemo_grid_t_dat = dn_files + \"nemo_data_T_grid_Aug2015.nc\" fn_nemo_dom = dn_files + \"coast_example_nemo_domain.nc\" config_t = root + \"./config/example_nemo_grid_t.json\" Loading data # Create a Gridded object and load in the data: nemo = coast.Gridded(fn_nemo_grid_t_dat, fn_nemo_dom, config=config_t) Initialise MaskMaker and define target grid mm = coast.MaskMaker() # Define Regional Masks regional_masks = [] # Define convenient aliases based on nemo data lon = nemo.dataset.longitude.values lat = nemo.dataset.latitude.values bathy = nemo.dataset.bathymetry.values Use MaskMaker to define new regions MaskMaker can build a stack of boolean masks in an xarray dataset for regional analysis. Regions can be supplied by providing vertices coordiates to the make_region_from_vertices method. (Vertices coordinates can be passed as xarray objects or as numpy arrays). The method returns a numpy array of booleans.\n# Draw and fill a square vertices_lon = [-5, -5, 5, 5] vertices_lat = [40, 60, 60, 40] # input lat/lon as xr.DataArray filled1 = mm.make_region_from_vertices(nemo.dataset.longitude, nemo.dataset.latitude, vertices_lon, vertices_lat) # input lat/lon as np.ndarray filled2 = mm.make_region_from_vertices( nemo.dataset.longitude.values, nemo.dataset.latitude.values, vertices_lon, vertices_lat ) check = (filled1 == filled2).all() print(f\"numpy array outputs are the same? {check}\") numpy array outputs are the same? True The boolean numpy array can then be converted to an xarray object using make_mask_dataset() for improved interactions with other xarray objects.\nmask_xr = mm.make_mask_dataset(nemo.dataset.longitude.values, nemo.dataset.latitude.values, filled1) Use MaskMaker for predefined regions The NWS has a number of predefined regions. These are numpy boolean arrays as functions of the specified latitude, longitude and bathymetry. They can be appended into a list of arrays, which can be similarly converted into an xarray object.\nmasks_list = [] # Add regional mask for whole domain masks_list.append(np.ones(lon.shape)) # Add regional mask for English Channel masks_list.append(mm.region_def_nws_north_north_sea(lon, lat, bathy)) masks_list.append(mm.region_def_nws_south_north_sea(lon, lat, bathy)) masks_list.append(mm.region_def_nws_outer_shelf(lon, lat, bathy)) masks_list.append(mm.region_def_nws_norwegian_trench(lon, lat, bathy)) masks_list.append(mm.region_def_nws_english_channel(lon, lat, bathy)) masks_list.append(mm.region_def_nws_off_shelf(lon, lat, bathy)) masks_list.append(mm.region_def_nws_irish_sea(lon, lat, bathy)) masks_list.append(mm.region_def_nws_kattegat(lon, lat, bathy)) masks_list.append(mm.region_def_nws_fsc(lon, lat, bathy)) masks_names = [\"whole domain\", \"northern north sea\", \"southern north sea\", \"outer shelf\", \"norwegian trench\", \"english_channel\", \"off shelf\", \"irish sea\", \"kattegat\", \"fsc\"] As before the numpy arrays (here as a list) can be converted into an xarray dataset where each mask is separated along the dim_mask dimension\nmask_xr = mm.make_mask_dataset(lon, lat, masks_list, masks_names) Inspect mask xarray object structure\nmask_xr\nPlot masks Inspect the mask with a quick_plot() method.\nmm.quick_plot(mask_xr) NB overlapping regions are not given special treatment, the layers are blindly superimposed on each other. E.g. as demonstrated with “Norwegian Trench” and “off shelf”, “FSC” and “off shelf”, or “whole domain” and any other region.\nplt.subplot(2,2,1) mm.quick_plot(mask_xr.sel(dim_mask=[0,3,9])) plt.subplot(2,2,2) mm.quick_plot(mask_xr.sel(dim_mask=[1,2,4,5,6,7,8])) plt.tight_layout() # Show overlap mask_xr.mask.sum(dim='dim_mask').plot(levels=(1,2,3,4)) # Save if required #plt.savefig('tmp.png') \u003cmatplotlib.collections.QuadMesh at 0x7fbf32be4d00\u003e Regional analysis with Profile data Apply the regional masks to average SST\n# Read EN4 data into profile object fn_prof = path.join(dn_files, \"coast_example_en4_201008.nc\") fn_cfg_prof = path.join(\"config\",\"example_en4_profiles.json\") profile = coast.Profile(config=fn_cfg_prof) profile.read_en4( fn_prof ) config/example_en4_profiles.json Then we use ProfileAnalysis.determine_mask_indices() to figure out which profiles in a Profile object lie within each regional mask:\nanalysis = coast.ProfileAnalysis() mask_indices = analysis.determine_mask_indices(profile, mask_xr) This returns an object called mask_indices, which is required to pass to ProfileAnalysis.mask_means(). This routine will return a new xarray dataset containing averaged data for each region:\nprofile_mask_means = analysis.mask_means(profile, mask_indices) This routine operates over all variables in the profile object. It calculates means by region preserving depth information (profile_mean_*) and also averaging over depth information (all_mean_*). The variables are returned with these prefixes accordingly.\nprofile_mask_means \u003cxarray.Dataset\u003e Dimensions: (dim_mask: 8, z_dim: 400) Coordinates: region_names (dim_mask) \u003cU18 'whole domain' … 'k… Dimensions without coordinates: dim_mask, z_dim Data variables: profile_mean_depth (dim_mask, z_dim) float32 3.802 … nan profile_mean_potential_temperature (dim_mask, z_dim) float32 4.629 … nan profile_mean_temperature (dim_mask, z_dim) float32 4.629 … nan profile_mean_practical_salinity (dim_mask, z_dim) float32 29.08 … nan profile_mean_qc_flags_profiles (dim_mask) float64 4.422e+05 … 1.93… profile_mean_qc_flags_levels (dim_mask, z_dim) float64 1.693e+07 …. all_mean_depth (dim_mask) float32 219.3 48.17 … 86.48 all_mean_potential_temperature (dim_mask) float32 7.458 6.68 … 7.266 all_mean_temperature (dim_mask) float32 7.48 6.685 … 7.275 all_mean_practical_salinity (dim_mask) float32 34.57 34.86 … 33.76 all_mean_qc_flags_profiles (dim_mask) float64 4.422e+05 … 1.93… all_mean_qc_flags_levels (dim_mask) float64 3.272e+07 … 3.68…xarray.DatasetDimensions:dim_mask: 8z_dim: 400Coordinates: (1)region_names(dim_mask)\u003cU18'whole domain' … 'kattegat'array(['whole domain', 'northern north sea', 'southern north sea', 'outer shelf', 'norwegian trench', 'english_channel', 'off shelf', 'kattegat'], dtype='\u003cU18')Data variables: (12)profile_mean_depth(dim_mask, z_dim)float323.802 12.43 12.75 … nan nan nanarray([[3.8015647e+00, 1.2431897e+01, 1.2752748e+01, …, 6.8166663e+02, 4.4625000e+02, 4.8479999e+02], [5.3978572e+00, 2.4058212e+01, 1.9938543e+01, …, nan, nan, nan], [4.4636950e+00, 1.6393627e+01, 1.4392214e+01, …, nan, nan, nan], …, [1.2563381e+00, 3.3999999e+00, 6.0253515e+00, …, nan, nan, nan], [1.0883763e+01, 1.9575655e+01, 3.1419313e+01, …, 1.1761000e+03, nan, nan], [1.1508474e+00, 5.1322031e+00, 8.6844826e+00, …, nan, nan, nan]], dtype=float32)profile_mean_potential_temperature(dim_mask, z_dim)float324.629 4.718 4.732 … nan nan nanarray([[ 4.629049 , 4.7177176 , 4.7318735 , …, 4.4791145 , 7.0863085 , 7.4710603 ], [ 5.4062243 , 5.538929 , 5.3454785 , …, nan, nan, nan], [ 4.690185 , 4.7648296 , 4.290611 , …, nan, nan, nan], …, [ 7.777337 , 7.743324 , 7.730106 , …, nan, nan, nan], [10.741058 , 10.7420845 , 10.740048 , …, -0.8672274 , nan, nan], [ 0.6519121 , 0.89932823, 1.4384961 , …, nan, nan, nan]], dtype=float32)profile_mean_temperature(dim_mask, z_dim)float324.629 4.719 4.733 … nan nan nanarray([[ 4.629366 , 4.7186904 , 4.7328353 , …, 4.523333 , 7.13 , 7.52 ], [ 5.4066286 , 5.540855 , 5.3469286 , …, nan, nan, nan], [ 4.69051 , 4.76605 , 4.291559 , …, nan, nan, nan], …, [ 7.777464 , 7.7436633 , 7.7307053 , …, nan, nan, nan], [10.742379 , 10.744476 , 10.743904 , …, -0.82 , nan, nan], [ 0.6519491 , 0.89944816, 1.4387244 , …, nan, nan, nan]], dtype=float32)profile_mean_practical_salinity(dim_mask, z_dim)float3229.08 29.49 30.4 … nan nan nanarray([[29.07752 , 29.488913, 30.403212, …, 35.102665, 35.194 , 35.191 ], [34.215603, 34.549515, 34.572178, …, nan, nan, nan], [34.39857 , 34.319733, 34.34744 , …, nan, nan, nan], …, [34.69485 , 34.745506, 34.778313, …, nan, nan, nan], [35.41276 , 35.531857, 35.523746, …, 34.912 , nan, nan], [22.681366, 23.651918, 24.719212, …, nan, nan, nan]], dtype=float32)profile_mean_qc_flags_profiles(dim_mask)float644.422e+05 1.501e+04 … 1.93e+06array([ 442235.24343675, 15008.98928571, 80146.01273885, 376662.06666667, 938649.58108108, 236414.92957746, 351245.56410256, 1930156.61016949])profile_mean_qc_flags_levels(dim_mask, z_dim)float641.693e+07 1.56e+07 … 3.356e+07array([[1.69308660e+07, 1.55996508e+07, 1.91944702e+07, …, 3.35092258e+07, 3.35359264e+07, 3.35359264e+07], [2.41202110e+06, 1.67795573e+06, 1.43838659e+07, …, 3.35626270e+07, 3.35626270e+07, 3.35626270e+07], [7.82982090e+06, 9.03196737e+06, 2.22584483e+07, …, 3.35626270e+07, 3.35626270e+07, 3.35626270e+07], …, [1.43590767e+07, 1.24096114e+07, 1.32338878e+07, …, 3.35626270e+07, 3.35626270e+07, 3.35626270e+07], [2.49180806e+06, 2.49194591e+06, 1.91836366e+06, …, 3.32757669e+07, 3.35626270e+07, 3.35626270e+07], [1.07067776e+08, 1.03513576e+08, 9.04351003e+07, …, 3.35626270e+07, 3.35626270e+07, 3.35626270e+07]])all_mean_depth(dim_mask)float32219.3 48.17 17.66 … 492.1 86.48array([219.31877 , 48.17008 , 17.655499, 54.261852, 147.37276 , 21.17876 , 492.1379 , 86.48219 ], dtype=float32)all_mean_potential_temperature(dim_mask)float327.458 6.68 5.08 … 8.609 7.266array([ 7.4579477, 6.6803527, 5.0804653, 10.605105 , 7.5581946, 7.9912376, 8.609107 , 7.266073 ], dtype=float32)all_mean_temperature(dim_mask)float327.48 6.685 5.082 … 8.658 7.275array([ 7.479547 , 6.684928 , 5.0818586, 10.611826 , 7.572891 , 7.993422 , 8.657866 , 7.2747602], dtype=float32)all_mean_practical_salinity(dim_mask)float3234.57 34.86 34.49 … 35.33 33.76array([34.574173, 34.85523 , 34.486984, 35.177284, 34.745213, 34.871284, 35.334602, 33.764072], dtype=float32)all_mean_qc_flags_profiles(dim_mask)float644.422e+05 1.501e+04 … 1.93e+06array([ 442235.24343675, 15008.98928571, 80146.01273885, 376662.06666667, 938649.58108108, 236414.92957746, 351245.56410256, 1930156.61016949])all_mean_qc_flags_levels(dim_mask)float643.272e+07 3.244e+07 … 3.683e+07array([32717180.43962013, 32440148.97097322, 33282693.25738854, 32052298.20066667, 38954525.82148649, 32985344.45144366, 26732339.47348291, 36830323.95974576])Indexes: (0)Attributes: (0)\nNotice that the number of mask dimensions is not necessarily preserved between the mask and the mask averaged variables. This happens if, for example, there are no profiles in one of the mask regions\ncheck1 = mask_indices.dims[\"dim_mask\"] == profile_mask_means.dims[\"dim_mask\"] print(check1) False The mean profiles can be visualised or further processed (notice the Irish Sea region and FSC are missing because there were no profiles in the example dataset)\nfor count_region in range(profile_mask_means.sizes['dim_mask']): plt.plot( profile_mask_means.profile_mean_temperature.isel(dim_mask=count_region), profile_mask_means.profile_mean_depth.isel(dim_mask=count_region), label=profile_mask_means.region_names[count_region].values, marker=\".\", linestyle='none') plt.ylim([10,1000]) plt.yscale(\"log\") plt.gca().invert_yaxis() plt.xlabel('temperature'); plt.ylabel('depth') plt.legend() \u003cmatplotlib.legend.Legend at 0x7fbf3191e260\u003e Regional analysis with Gridded data Apply the regional masks to average SST. This is done manually as there are not yet COAsT methods to broadcast the operations across all variables.\n# Syntax: xr.where(if \u003cfirst\u003e, then \u003c2nd\u003e, else \u003c3rd\u003e) mask_SST = xr.where( mask_xr.mask, nemo.dataset.temperature.isel(z_dim=0), np.NaN) # Take the mean over space for each region mask_mean_SST = mask_SST.mean(dim=\"x_dim\").mean(dim=\"y_dim\") # Inspect the processed data mask_mean_SST.plot() \u003cmatplotlib.collections.QuadMesh at 0x7fbf32b91c90\u003e # Plot timeseries per region for count_region in range(mask_mean_SST.sizes['dim_mask']): plt.plot( mask_mean_SST.isel(dim_mask=count_region), label=mask_mean_SST.region_names[count_region].values, marker=\".\", linestyle='none') plt.xlabel('time'); plt.ylabel('SST') plt.legend() \u003cmatplotlib.legend.Legend at 0x7fbf35cf2230\u003e ","categories":"","description":"Mask maker tutorial example.\n","excerpt":"Mask maker tutorial example.\n","ref":"/COAsT/docs/examples/notebooks/general/mask_maker_tutorial/","tags":"","title":"Mask maker tutorial"},{"body":"A demonstration to calculate the Potential Energy Anomaly and demonstrate regional masking with MaskMaker\nRelevant imports and filepath configuration import coast import numpy as np import os import matplotlib.pyplot as plt import matplotlib.colors as colors # colormap fiddling import xarray as xr /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pydap/lib.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.responses')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.handlers')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.tests')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages # set some paths root = \"./\" dn_files = root + \"./example_files/\" fn_nemo_grid_t_dat = dn_files + \"nemo_data_T_grid_Aug2015.nc\" fn_nemo_dom = dn_files + \"coast_example_nemo_domain.nc\" config_t = root + \"./config/example_nemo_grid_t.json\" Loading data # Create a Gridded object and load in the data: nemo = coast.Gridded(fn_nemo_grid_t_dat, fn_nemo_dom, config=config_t) Calculates Potential Energy Anomaly The density and depth averaged density can be supplied within gridded_t as density and density_bar DataArrays, respectively. If they are not supplied they will be calculated. density_bar is calcuated using depth averages of temperature and salinity.\n# Compute a vertical max to exclude depths below 200m Zd_mask, kmax, Ikmax = nemo.calculate_vertical_mask(200.) # Initiate a stratification diagnostics object strat = coast.GriddedStratification(nemo) # calculate PEA for unmasked depths strat.calc_pea(nemo, Zd_mask) make a plot strat.quick_plot('PEA') (\u003cFigure size 1000x1000 with 2 Axes\u003e, \u003cAxes: title={'center': '01 Aug 2015: Potential Energy Anomaly (J / m^3)'}, xlabel='longitude', ylabel='latitude'\u003e) strat.dataset \u003cxarray.Dataset\u003e Dimensions: (t_dim: 7, y_dim: 375, x_dim: 297) Coordinates: time (t_dim) datetime64[ns] 2015-08-01T12:00:00 … 2015-08-07T12:0… latitude (y_dim, x_dim) float32 40.07 40.07 40.07 40.07 … 65.0 65.0 65.0 longitude (y_dim, x_dim) float32 -19.89 -19.78 -19.67 … 12.78 12.89 13.0 Dimensions without coordinates: t_dim, y_dim, x_dim Data variables: PEA (t_dim, y_dim, x_dim) float64 nan nan nan nan … nan nan nan nanxarray.DatasetDimensions:t_dim: 7y_dim: 375x_dim: 297Coordinates: (3)time(t_dim)datetime64[ns]2015-08-01T12:00:00 … 2015-08-…array(['2015-08-01T12:00:00.000000000', '2015-08-02T12:00:00.000000000', '2015-08-03T12:00:00.000000000', '2015-08-04T12:00:00.000000000', '2015-08-05T12:00:00.000000000', '2015-08-06T12:00:00.000000000', '2015-08-07T12:00:00.000000000'], dtype='datetime64[ns]')latitude(y_dim, x_dim)float3240.07 40.07 40.07 … 65.0 65.0array([[40.066406, 40.066406, 40.066406, …, 40.066406, 40.066406, 40.066406], [40.13379 , 40.13379 , 40.13379 , …, 40.13379 , 40.13379 , 40.13379 ], [40.200195, 40.200195, 40.200195, …, 40.200195, 40.200195, 40.200195], …, [64.868164, 64.868164, 64.868164, …, 64.868164, 64.868164, 64.868164], [64.93457 , 64.93457 , 64.93457 , …, 64.93457 , 64.93457 , 64.93457 ], [65.00098 , 65.00098 , 65.00098 , …, 65.00098 , 65.00098 , 65.00098 ]], dtype=float32)longitude(y_dim, x_dim)float32-19.89 -19.78 -19.67 … 12.89 13.0array([[-19.888672, -19.777344, -19.666992, …, 12.777344, 12.888672, 13. ], [-19.888672, -19.777344, -19.666992, …, 12.777344, 12.888672, 13. ], [-19.888672, -19.777344, -19.666992, …, 12.777344, 12.888672, 13. ], …, [-19.888672, -19.777344, -19.666992, …, 12.777344, 12.888672, 13. ], [-19.888672, -19.777344, -19.666992, …, 12.777344, 12.888672, 13. ], [-19.888672, -19.777344, -19.666992, …, 12.777344, 12.888672, 13. ]], dtype=float32)Data variables: (1)PEA(t_dim, y_dim, x_dim)float64nan nan nan nan … nan nan nan nanunits :J / m^3standard_name :Potential Energy Anomalyarray([[[ nan, nan, nan, …, nan, nan, nan], [ nan, 262.43849344, 261.16678604, …, nan, nan, nan], [ nan, 262.22882652, 292.63335667, …, nan, nan, nan], …, [ nan, nan, nan, …, nan, nan, nan], [ nan, nan, nan, …, nan, nan, nan], [ nan, nan, nan, …, nan, nan, nan]],\n[[ nan, nan, nan, ..., nan, nan, nan], [ nan, 269.18332328, 268.17983174, ..., nan, nan, nan], [ nan, 268.76494494, 226.91876185, ..., nan, nan, nan], … [ nan, nan, nan, …, nan, nan, nan], [ nan, nan, nan, …, nan, nan, nan], [ nan, nan, nan, …, nan, nan, nan]],\n[[ nan, nan, nan, ..., nan, nan, nan], [ nan, 263.17561991, 262.13100791, ..., nan, nan, nan], [ nan, 263.60651849, 268.85516316, ..., nan, nan, nan], ..., [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan]]])\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/li\u003e\u003cli class='xr-section-item'\u003e\u003cinput id='section-48c3f025-983d-433c-b451-08d35d7bba49' class='xr-section-summary-in' type='checkbox' disabled \u003e\u003clabel for='section-48c3f025-983d-433c-b451-08d35d7bba49' class='xr-section-summary' title='Expand/collapse section'\u003eIndexes: \u003cspan\u003e(0)\u003c/span\u003e\u003c/label\u003e\u003cdiv class='xr-section-inline-details'\u003e\u003c/div\u003e\u003cdiv class='xr-section-details'\u003e\u003cul class='xr-var-list'\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/li\u003e\u003cli class='xr-section-item'\u003e\u003cinput id='section-3eeca1df-9e8b-4a62-a0aa-b7c62e59f70a' class='xr-section-summary-in' type='checkbox' disabled \u003e\u003clabel for='section-3eeca1df-9e8b-4a62-a0aa-b7c62e59f70a' class='xr-section-summary' title='Expand/collapse section'\u003eAttributes: \u003cspan\u003e(0)\u003c/span\u003e\u003c/label\u003e\u003cdiv class='xr-section-inline-details'\u003e\u003c/div\u003e\u003cdiv class='xr-section-details'\u003e\u003cdl class='xr-attrs'\u003e\u003c/dl\u003e\u003c/div\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/div\u003e Use MaskMaker to define regions and do regional analysis MaskMaker can build a stack of boolean masks in an xarray dataset for regional analysis. For the NWS we can use some built-in regions.\nmm = coast.MaskMaker() # Define Regional Masks regional_masks = [] # Define convenient aliases based on nemo data lon = nemo.dataset.longitude.values lat = nemo.dataset.latitude.values bathy = nemo.dataset.bathymetry.values # Add regional mask for whole domain regional_masks.append(np.ones(lon.shape)) # Add regional mask for English Channel regional_masks.append(mm.region_def_nws_north_sea(lon, lat, bathy)) regional_masks.append(mm.region_def_nws_outer_shelf(lon, lat, bathy)) regional_masks.append(mm.region_def_nws_norwegian_trench(lon, lat, bathy)) regional_masks.append(mm.region_def_nws_english_channel(lon, lat, bathy)) regional_masks.append(mm.region_def_south_north_sea(lon, lat, bathy)) regional_masks.append(mm.region_def_off_shelf(lon, lat, bathy)) regional_masks.append(mm.region_def_irish_sea(lon, lat, bathy)) regional_masks.append(mm.region_def_kattegat(lon, lat, bathy)) region_names = [\"whole domain\", \"north sea\", \"outer shelf\", \"norwegian trench\", \"english_channel\", \"southern north sea\", \"off shelf\", \"irish sea\", \"kattegat\",] --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) Cell In[8], line 15 12 regional_masks.append(np.ones(lon.shape)) 14 # Add regional mask for English Channel ---\u003e 15 regional_masks.append(mm.region_def_nws_north_sea(lon, lat, bathy)) 16 regional_masks.append(mm.region_def_nws_outer_shelf(lon, lat, bathy)) 17 regional_masks.append(mm.region_def_nws_norwegian_trench(lon, lat, bathy)) AttributeError: 'MaskMaker' object has no attribute 'region_def_nws_north_sea' Convert this list of masks into a dataset\nmask_list = mm.make_mask_dataset(lon, lat, regional_masks, region_names) --------------------------------------------------------------------------- NameError Traceback (most recent call last) Cell In[9], line 1 ----\u003e 1 mask_list = mm.make_mask_dataset(lon, lat, regional_masks, region_names) NameError: name 'region_names' is not defined mask_list --------------------------------------------------------------------------- NameError Traceback (most recent call last) Cell In[10], line 1 ----\u003e 1 mask_list NameError: name 'mask_list' is not defined Inspect the mask with a quick_plot() method.\nmm.quick_plot(mask_list) --------------------------------------------------------------------------- NameError Traceback (most recent call last) Cell In[11], line 1 ----\u003e 1 mm.quick_plot(mask_list) NameError: name 'mask_list' is not defined NB overlapping regions are not given special treatment, the layers are blindly superimposed on each other. E.g. as demonstrated with “Norwegian Trench” and “off shelf”, or “whole domain” and any other region.\nplt.subplot(2,2,1) mm.quick_plot(mask_list.sel(dim_mask=[0,3])) plt.subplot(2,2,2) mm.quick_plot(mask_list.sel(dim_mask=[1,2,4,5,6,7,8])) plt.tight_layout() --------------------------------------------------------------------------- NameError Traceback (most recent call last) Cell In[12], line 2 1 plt.subplot(2,2,1) ----\u003e 2 mm.quick_plot(mask_list.sel(dim_mask=[0,3])) 4 plt.subplot(2,2,2) 5 mm.quick_plot(mask_list.sel(dim_mask=[1,2,4,5,6,7,8])) NameError: name 'mask_list' is not defined # Show overlap mask_list.mask.sum(dim='dim_mask').plot( levels=(1,2,3,4)) # Save if required #plt.savefig('tmp.png') --------------------------------------------------------------------------- NameError Traceback (most recent call last) Cell In[13], line 2 1 # Show overlap ----\u003e 2 mask_list.mask.sum(dim='dim_mask').plot( levels=(1,2,3,4)) 4 # Save if required 5 #plt.savefig('tmp.png') NameError: name 'mask_list' is not defined Regional analysis Average stratification object over regions using the mask\nmask_means = (strat.dataset*mask_list.mask).mean(dim='x_dim').mean(dim='y_dim') --------------------------------------------------------------------------- NameError Traceback (most recent call last) Cell In[14], line 1 ----\u003e 1 mask_means = (strat.dataset*mask_list.mask).mean(dim='x_dim').mean(dim='y_dim') NameError: name 'mask_list' is not defined mask_means --------------------------------------------------------------------------- NameError Traceback (most recent call last) Cell In[15], line 1 ----\u003e 1 mask_means NameError: name 'mask_means' is not defined # Plot timeseries per region for count_region in range(mask_means.dims['dim_mask']): plt.plot( mask_means.PEA.isel(dim_mask=count_region), label=mask_means.region_names[count_region].values, marker=\".\", linestyle='none') plt.xlabel('time'); plt.ylabel('PEA') plt.legend() --------------------------------------------------------------------------- NameError Traceback (most recent call last) Cell In[16], line 3 1 # Plot timeseries per region ----\u003e 3 for count_region in range(mask_means.dims['dim_mask']): 5 plt.plot( 6 mask_means.PEA.isel(dim_mask=count_region), 7 label=mask_means.region_names[count_region].values, 8 marker=\".\", linestyle='none') 10 plt.xlabel('time'); plt.ylabel('PEA') NameError: name 'mask_means' is not defined ","categories":"","description":"Potential energy tutorial example.\n","excerpt":"Potential energy tutorial example.\n","ref":"/COAsT/docs/examples/notebooks/gridded/potential_energy_tutorial/","tags":"","title":"Potential energy tutorial"},{"body":"","categories":"","description":"Examples Profile scripts for the COAsT package.\n","excerpt":"Examples Profile scripts for the COAsT package.\n","ref":"/COAsT/docs/examples/notebooks/profile/","tags":"","title":"Profile"},{"body":"A demonstration of pycnocline depth and thickness diagnostics. The first and second depth moments of stratification are computed as proxies for pycnocline depth and thickness, suitable for a nearly two-layer fluid.\nNote that in the AMM7 example data the plots are not particularly spectacular as the internal tide is poorly resolved at 7km.\nRelevant imports and filepath configuration import coast import numpy as np import os import matplotlib.pyplot as plt import matplotlib.colors as colors # colormap fiddling /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pydap/lib.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.responses')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.handlers')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.tests')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages # set some paths root = \"./\" dn_files = root + \"./example_files/\" fn_nemo_grid_t_dat = dn_files + \"nemo_data_T_grid_Aug2015.nc\" fn_nemo_dom = dn_files + \"coast_example_nemo_domain.nc\" config_t = root + \"./config/example_nemo_grid_t.json\" config_w = root + \"./config/example_nemo_grid_w.json\" Loading data # Create a Gridded object and load in the data: nemo_t = coast.Gridded(fn_nemo_grid_t_dat, fn_nemo_dom, config=config_t) #nemo_t.dataset # uncomment to print data object summary # The stratification variables are computed as centred differences of the t-grid variables. # These will become w-grid variables. So, create an empty w-grid object, to store stratification. # Note how we do not pass a NEMO data file for this load. nemo_w = coast.Gridded(fn_domain=fn_nemo_dom, config=config_w) Subset the domain We are not interested in the whole doman so it is computationally efficient to subset the data for the region of interest. Here we will look at the North Sea between (51N: 62N) and (-4E:15E). We will great subset objects for both the t- and w-grids:\nind_2d = nemo_t.subset_indices(start=[51,-4], end=[62,15]) nemo_nwes_t = nemo_t.isel(y_dim=ind_2d[0], x_dim=ind_2d[1]) #nwes = northwest european shelf ind_2d = nemo_w.subset_indices(start=[51,-4], end=[62,15]) nemo_nwes_w = nemo_w.isel(y_dim=ind_2d[0], x_dim=ind_2d[1]) #nwes = northwest european shelf #nemo_nwes_t.dataset # uncomment to print data object summary Diagnostic calculations and plotting We can use a COAsT method to construct the in-situ density:\nnemo_nwes_t.construct_density( eos='EOS10' ) Then we construct stratification using a COAsT method to take the vertical derivative. Noting that the inputs are on t-pts and the outputs are on w-pt\nnemo_nwes_w = nemo_nwes_t.differentiate( 'density', dim='z_dim', out_var_str='rho_dz', out_obj=nemo_nwes_w ) # --\u003e sci_nwes_w.rho_dz This has created a variable called nemo_nwes_w.rho_dz.\nCreate internal tide diagnostics We can now use the GriddedStratification class to construct the first and second moments (over depth) of density. In the limit of an idealised two-layer fluid these converge to the depth and thickness of the interface. I.e. the pycnocline depth and thickness respectively.\nstrat = coast.GriddedStratification(nemo_nwes_t) #%% Construct pycnocline variables: depth and thickness strat.construct_pycnocline_vars( nemo_nwes_t, nemo_nwes_w ) /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/xarray/core/computation.py:821: RuntimeWarning: invalid value encountered in sqrt Plotting data Finally we plot pycnocline variables (depth and thickness) using an GriddedStratification method:\nstrat.quick_plot() (\u003cFigure size 1000x1000 with 2 Axes\u003e, \u003cAxes: title={'center': '01 Aug 2015: masked pycnocline thickness (m)'}, xlabel='longitude', ylabel='latitude'\u003e) ","categories":"","description":"Pycnocline tutorial example.\n","excerpt":"Pycnocline tutorial example.\n","ref":"/COAsT/docs/examples/notebooks/gridded/pycnocline_tutorial/","tags":"","title":"Pycnocline tutorial"},{"body":"This is a demonstration on regridding in COAsT. To do this, the COAsT package uses the already capable xesmf package, which will need to be installed independently (is not natively part of the COAsT package).\nIntroduction COAsT uses XESMF by providing a data class xesmf_convert which provides functions to prepare COAsT.Gridded objects, so they can be passed to XESMF for regridding to either a curvilinear or rectilienar grid.\nAll you need to do if provide a Gridded object and a grid type when creating a new instance of this class. It will then contain an appropriate input dataset. You may also provide a second COAsT gridded object if regridding between two objects.\nInstall XESMF See the package’s documentation website here:\nhttps://xesmf.readthedocs.io/en/latest/index.html You can install XESMF using:\nconda install -c conda-forge xesmf. The setup used by this class has been tested for xesmf v0.6.2 alongside esmpy v8.0.0. It was installed using:\nconda install -c conda-forge xesmf esmpy=8.0.0 Example useage If regridding a Gridded object to an arbitrarily defined rectilinear or curvilinear grid, you just need to do the following:\nimport xesmf as xe # Create your gridded object gridded = coast.Gridded(*args, **kwargs) # Pass the gridded object over to xesmf_convert xesmf_ready = coast.xesmf_convert(gridded, input_grid_type = 'curvilinear') # Now this object will contain a dataset called xesmf_input, which can # be passed over to xesmf. E.G: destination_grid = xesmf.util.grid_2d(-15, 15, 1, 45, 65, 1) regridder = xe.Regridder(xesmf_ready.input_grid, destination_grid, \"bilinear\") regridded_dataset = regridder(xesmf_ready.input_data) XESMF contains a couple of difference functions for quickly creating output grids, such as xesmf.util.grid_2d and xesmf.util.grid_global(). See their website for more info.\nThe process is almost the same if regridding from one COAsT.Gridded object to another (gridded0 -\u003e gridded1):\nxesmf_ready = coast.xesmf_convert(gridded0, gridded1, input_grid_type = \"curvilinear\", output_grid_type = \"curvilinear\") regridder = xe.Regridder(xesmf_ready.input_grid, xesmf_ready.output_grid, \"bilinear\") regridded_dataset = regridder(xesmf_ready.input_data) Note that you can select which variables you want to regrid, either prior to using this tool or by indexing the input_data dataset. e.g.:\nregridded_dataset = regridder(xesmf_ready.input_data['temperature']) If your input datasets were lazy loaded, then so will the regridded dataset. At this point you can either load the data or (recomended) save the regridded data to file:\nregridded_dataset.to_netcdf(\u003cfilename_to_save\u003e) Before saving back to file, call xesmf_ready.to_gridded() to convert the regridded xesmf object back to a gridded object\nCompatability Note (written 8 Sept 2022) xesmf is not included natively within COAsT as satisfying all the dependencies within COAsT gets increasingly challenging with more components in COAsT. So whilst valuable, xesmf is currently deemed not core. Here are some notes from a user on its installation with conda:\nA conda environemt with `esmpy=8.0.0` specified and `xesmf` version unspecified works suggests a downgrade of: netCDF4 1.5.6 scipy 1.5.3 lxml 4.8 A solution to avoid the downgrade maybe found in https://github.com/pangeo-data/pangeo-docker-images/issues/101 conda create … \"mpi==openmpi\" \"esmpy==mpi_openmpi*\" xesmf ","categories":"","description":"Regridding with xesmf tutorial example.\n","excerpt":"Regridding with xesmf tutorial example.\n","ref":"/COAsT/docs/examples/notebooks/gridded/regridding_with_xesmf_tutorial/","tags":"","title":"Regridding with xesmf tutorial"},{"body":"Tutorial to make a simple SEAsia 1/12 deg DIC plot.\nImport the relevant packages import coast import matplotlib.pyplot as plt /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pydap/lib.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.responses')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.handlers')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.tests')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages Define file paths for data root = \"./\" dn_files = root + \"./example_files/\" path_config = root + \"./config/\" fn_seasia_domain = dn_files + \"coast_example_domain_SEAsia.nc\" fn_seasia_var = dn_files + \"coast_example_SEAsia_BGC_1990.nc\" fn_seasia_config_bgc = path_config + \"example_nemo_bgc.json\" Create a Gridded object seasia_bgc = coast.Gridded(fn_data=fn_seasia_var, fn_domain=fn_seasia_domain, config=fn_seasia_config_bgc) Plot DIC fig = plt.figure() plt.pcolormesh( seasia_bgc.dataset.longitude, seasia_bgc.dataset.latitude, seasia_bgc.dataset.dic.isel(t_dim=0).isel(z_dim=0), cmap=\"RdYlBu_r\", vmin=1600, vmax=2080, ) plt.colorbar() plt.title(\"DIC, mmol/m^3\") plt.xlabel(\"longitude\") plt.ylabel(\"latitude\") plt.show() /tmp/ipykernel_2792/1161426776.py:2: UserWarning: The input coordinates to pcolormesh are interpreted as cell centers, but are not monotonically increasing or decreasing. This may lead to incorrectly calculated cell edges, in which case, please supply explicit cell edges to pcolormesh. ","categories":"","description":"Seasia dic example plot tutorial example.\n","excerpt":"Seasia dic example plot tutorial example.\n","ref":"/COAsT/docs/examples/notebooks/gridded/seasia_dic_example_plot_tutorial/","tags":"","title":"Seasia dic example plot tutorial"},{"body":"Overview A function within the Process_data class that will decompose time series into trend, seasonal and residual components. The function is a wrapper that adds functionality to the seasonal_decompose function contained in the statsmodels package to make it more convenient for large geospatial datasets.\nSpecifically:\nMultiple time series spread across multiple dimensions, e.g. a gridded dataset, can be processed. The user simply passes in an xarray DataArray that has a “t_dim” dimension and 1 or more additional dimensions, for example gridded spatial dimensions Masked locations, such as land points, are handled A dask wrapper is applied to the function that a) supports lazy evaluation b) allows the dataset to be easily seperated into chunks so that processing can be carried out in parallel (rather than processing every time series sequentially) The decomposed time series are returned as xarray DataArrays within a single coast.Gridded object An example Below is an example using the coast.Process_data.seasonal_decomposition function with the example data. Note that we will artifically extend the length of the example data time series for demonstrative purposes.\nBegin by importing coast, defining paths to the data, and loading the example data into a gridded object:\nimport coast import numpy as np import xarray as xr # Path to a data file root = \"./\" dn_files = root + \"./example_files/\" fn_nemo_dat = dn_files + \"coast_example_nemo_data.nc\" # Set path for domain file if required. fn_nemo_dom = dn_files + \"coast_example_nemo_domain.nc\" # Set path for model configuration file config = root + \"./config/example_nemo_grid_t.json\" # Read in data (This example uses NEMO data.) grd = coast.Gridded(fn_nemo_dat, fn_nemo_dom, config=config) /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pydap/lib.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.responses')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.handlers')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.tests')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/xarray/core/dataset.py:278: UserWarning: The specified chunks separate the stored chunks along dimension \"time_counter\" starting at index 2. This could degrade performance. Instead, consider rechunking after loading. The loaded example data only has 7 time stamps, the code below creates a new (fake) extended temperature variable with 48 monthly records. This code is not required to use the function, it is only included here to make a set of time series that are long enough to be interesting.\n# create a 4 yr monthly time coordinate array time_array = np.arange( np.datetime64(\"2010-01-01\"), np.datetime64(\"2014-01-01\"), np.timedelta64(1, \"M\"), dtype=\"datetime64[M]\" ).astype(\"datetime64[s]\") # create 4 years of monthly temperature data based on the loaded data temperature_array = ( (np.arange(0, 48) * 0.05)[:, np.newaxis, np.newaxis, np.newaxis] + np.random.normal(0, 0.1, 48)[:, np.newaxis, np.newaxis, np.newaxis] + np.tile(grd.dataset.temperature[:-1, :2, :, :], (8, 1, 1, 1)) ) # create a new temperature DataArray temperature = xr.DataArray( temperature_array, coords={ \"t_dim\": time_array, \"depth_0\": grd.dataset.depth_0[:2, :, :], \"longitude\": grd.dataset.longitude, \"latitude\": grd.dataset.latitude, }, dims=[\"t_dim\", \"z_dim\", \"y_dim\", \"x_dim\"], ) /tmp/ipykernel_2964/2929135463.py:14: UserWarning: Converting non-nanosecond precision datetime values to nanosecond precision. This behavior can eventually be relaxed in xarray, as it is an artifact from pandas which is now beginning to support non-nanosecond precision values. This warning is caused by passing non-nanosecond np.datetime64 or np.timedelta64 values to the DataArray or Variable constructor; it can be silenced by converting the values to nanosecond precision ahead of time. Check out the new data\n#temperature # uncomment to print data object summary temperature[0,0,:,:].plot() \u003cmatplotlib.collections.QuadMesh at 0x7fa804784790\u003e Check out time series at 2 different grid points\ntemperature[:,0,50,50].plot() temperature[:,0,200,200].plot() [\u003cmatplotlib.lines.Line2D at 0x7fa8001b7460\u003e] Create a coast.Process_data object, and call the seasonal_decomposition function, passing in the required arguments. The first two arguments are:\nThe input data, here the temperature data as an xarray DataArray The number of chuncks to split the data into. Here we split the data into 2 chunks so that the dask scheduler will try to run 4 processes in parallel The remaining arguments are keyword arguments for the underlying statsmodels.tsa.seasonal.seasonal_decompose function, which are documented on the statsmodels documentation pages. Here we specify:\nthree The type of model, i.e. an additive model The period of the seasonal cycle, here it is 6 months Extrapolate the trend component to cover the entire range of the time series (this is required because the trend is calculated using a convolution filter) proc_data = coast.Process_data() grd = proc_data.seasonal_decomposition(temperature, 2, model=\"additive\", period=6, extrapolate_trend=\"freq\") /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/coast/_utils/logging_util.py:21: DeprecationWarning: ProcessData is deprecated: Please use the 'ProcessData' class instead. This name will change in the next release. /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/coast/_utils/process_data.py:114: DeprecationWarning: Supplying chunks as dimension-order tuples is deprecated. It will raise an error in the future. Instead use a dict with dimension names as keys. /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/coast/_utils/process_data.py:116: DeprecationWarning: Supplying chunks as dimension-order tuples is deprecated. It will raise an error in the future. Instead use a dict with dimension names as keys. /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/coast/_utils/process_data.py:118: DeprecationWarning: Supplying chunks as dimension-order tuples is deprecated. It will raise an error in the future. Instead use a dict with dimension names as keys. The returned xarray Dataset contains the decomposed time series (trend, seasonal, residual) as dask arrays\n#grd.dataset # uncomment to print data object summary Execute the computation\ngrd.dataset.compute() \u003cxarray.Dataset\u003e Dimensions: (t_dim: 48, z_dim: 2, y_dim: 375, x_dim: 297) Coordinates:\nt_dim (t_dim) datetime64[ns] 2010-01-01 2010-02-01 … 2013-12-01 depth_0 (z_dim, y_dim, x_dim) float32 0.5 0.5 0.5 0.5 … 1.5 1.5 1.5 1.5 longitude (y_dim, x_dim) float32 -19.89 -19.78 -19.67 … 12.78 12.89 13.0 latitude (y_dim, x_dim) float32 40.07 40.07 40.07 40.07 … 65.0 65.0 65.0 Dimensions without coordinates: z_dim, y_dim, x_dim Data variables: trend (t_dim, z_dim, y_dim, x_dim) float64 nan nan nan … nan nan nan seasonal (t_dim, z_dim, y_dim, x_dim) float64 nan nan nan … nan nan nan residual (t_dim, z_dim, y_dim, x_dim) float64 nan nan nan … nan nan nanxarray.DatasetDimensions:t_dim: 48z_dim: 2y_dim: 375x_dim: 297Coordinates: (4)t_dim(t_dim)datetime64[ns]2010-01-01 … 2013-12-01array(['2010-01-01T00:00:00.000000000', '2010-02-01T00:00:00.000000000', '2010-03-01T00:00:00.000000000', '2010-04-01T00:00:00.000000000', '2010-05-01T00:00:00.000000000', '2010-06-01T00:00:00.000000000', '2010-07-01T00:00:00.000000000', '2010-08-01T00:00:00.000000000', '2010-09-01T00:00:00.000000000', '2010-10-01T00:00:00.000000000', '2010-11-01T00:00:00.000000000', '2010-12-01T00:00:00.000000000', '2011-01-01T00:00:00.000000000', '2011-02-01T00:00:00.000000000', '2011-03-01T00:00:00.000000000', '2011-04-01T00:00:00.000000000', '2011-05-01T00:00:00.000000000', '2011-06-01T00:00:00.000000000', '2011-07-01T00:00:00.000000000', '2011-08-01T00:00:00.000000000', '2011-09-01T00:00:00.000000000', '2011-10-01T00:00:00.000000000', '2011-11-01T00:00:00.000000000', '2011-12-01T00:00:00.000000000', '2012-01-01T00:00:00.000000000', '2012-02-01T00:00:00.000000000', '2012-03-01T00:00:00.000000000', '2012-04-01T00:00:00.000000000', '2012-05-01T00:00:00.000000000', '2012-06-01T00:00:00.000000000', '2012-07-01T00:00:00.000000000', '2012-08-01T00:00:00.000000000', '2012-09-01T00:00:00.000000000', '2012-10-01T00:00:00.000000000', '2012-11-01T00:00:00.000000000', '2012-12-01T00:00:00.000000000', '2013-01-01T00:00:00.000000000', '2013-02-01T00:00:00.000000000', '2013-03-01T00:00:00.000000000', '2013-04-01T00:00:00.000000000', '2013-05-01T00:00:00.000000000', '2013-06-01T00:00:00.000000000', '2013-07-01T00:00:00.000000000', '2013-08-01T00:00:00.000000000', '2013-09-01T00:00:00.000000000', '2013-10-01T00:00:00.000000000', '2013-11-01T00:00:00.000000000', '2013-12-01T00:00:00.000000000'], dtype='datetime64[ns]')depth_0(z_dim, y_dim, x_dim)float320.5 0.5 0.5 0.5 … 1.5 1.5 1.5 1.5units :mstandard_name :Depth at time zero on the t-gridarray([[[0.5 , 0.5 , 0.5 , …, 0.5 , 0.5 , 0.5 ], [0.5 , 0.4975586 , 0.4975586 , …, 0.10009766, 0.10009766, 0.5 ], [0.5 , 0.4975586 , 0.4975586 , …, 0.10009766, 0.10009766, 0.5 ], …, [0.5 , 0.10009766, 0.10009766, …, 0.10009766, 0.10009766, 0.5 ], [0.5 , 0.10009766, 0.10009766, …, 0.10009766, 0.10009766, 0.5 ], [0.5 , 0.5 , 0.5 , …, 0.5 , 0.5 , 0.5 ]],\n[[1.5 , 1.5 , 1.5 , …, 1.5 , 1.5 , 1.5 ], [1.5 , 1.5170898 , 1.5170898 , …, 0.30029297, 0.30029297, 1.5 ], [1.5 , 1.5170898 , 1.5170898 , …, 0.30029297, 0.30029297, 1.5 ], …, [1.5 , 0.30029297, 0.30029297, …, 0.30029297, 0.30029297, 1.5 ], [1.5 , 0.30029297, 0.30029297, …, 0.30029297, 0.30029297, 1.5 ], [1.5 , 1.5 , 1.5 , …, 1.5 , 1.5 , 1.5 ]]], dtype=float32)longitude(y_dim, x_dim)float32-19.89 -19.78 -19.67 … 12.89 13.0array([[-19.888672, -19.777344, -19.666992, …, 12.777344, 12.888672, 13. ], [-19.888672, -19.777344, -19.666992, …, 12.777344, 12.888672, 13. ], [-19.888672, -19.777344, -19.666992, …, 12.777344, 12.888672, 13. ], …, [-19.888672, -19.777344, -19.666992, …, 12.777344, 12.888672, 13. ], [-19.888672, -19.777344, -19.666992, …, 12.777344, 12.888672, 13. ], [-19.888672, -19.777344, -19.666992, …, 12.777344, 12.888672, 13. ]], dtype=float32)latitude(y_dim, x_dim)float3240.07 40.07 40.07 … 65.0 65.0array([[40.066406, 40.066406, 40.066406, …, 40.066406, 40.066406, 40.066406], [40.13379 , 40.13379 , 40.13379 , …, 40.13379 , 40.13379 , 40.13379 ], [40.200195, 40.200195, 40.200195, …, 40.200195, 40.200195, 40.200195], …, [64.868164, 64.868164, 64.868164, …, 64.868164, 64.868164, 64.868164], [64.93457 , 64.93457 , 64.93457 , …, 64.93457 , 64.93457 , 64.93457 ], [65.00098 , 65.00098 , 65.00098 , …, 65.00098 , 65.00098 , 65.00098 ]], dtype=float32)Data variables: (3)trend(t_dim, z_dim, y_dim, x_dim)float64nan nan nan nan … nan nan nan nanarray([[[[ nan, nan, nan, …, nan, nan, nan], [ nan, 15.14461357, 15.14135837, …, nan, nan, nan], [ nan, 15.13354587, 15.30135186, …, nan, nan, nan], …, [ nan, nan, nan, …, nan, nan, nan], [ nan, nan, nan, …, nan, nan, nan], [ nan, nan, nan, …, nan, nan, nan]],\n[[ nan, nan, nan, ..., nan, nan, nan], [ nan, 15.14363701, 15.14054456, ..., nan, nan, nan], [ nan, 15.13240654, 15.30525811, ..., nan, nan, nan], … [ nan, nan, nan, …, nan, nan, nan], [ nan, nan, nan, …, nan, nan, nan], [ nan, nan, nan, …, nan, nan, nan]],\n[[ nan, nan, nan, ..., nan, nan, nan], [ nan, 17.63406177, 17.63096932, ..., nan, nan, nan], [ nan, 17.6228313 , 17.79568286, ..., nan, nan, nan], ..., [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan]]]])\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\u003cli class='xr-var-item'\u003e\u003cdiv class='xr-var-name'\u003e\u003cspan\u003eseasonal\u003c/span\u003e\u003c/div\u003e\u003cdiv class='xr-var-dims'\u003e(t_dim, z_dim, y_dim, x_dim)\u003c/div\u003e\u003cdiv class='xr-var-dtype'\u003efloat64\u003c/div\u003e\u003cdiv class='xr-var-preview xr-preview'\u003enan nan nan nan ... nan nan nan nan\u003c/div\u003e\u003cinput id='attrs-6262a2ae-11c6-4f78-96de-2baa8292b4b4' class='xr-var-attrs-in' type='checkbox' disabled\u003e\u003clabel for='attrs-6262a2ae-11c6-4f78-96de-2baa8292b4b4' title='Show/Hide attributes'\u003e\u003csvg class='icon xr-icon-file-text2'\u003e\u003cuse xlink:href='#icon-file-text2'\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/label\u003e\u003cinput id='data-23438b93-37b1-4f4b-bdd4-e8120cf7cf5d' class='xr-var-data-in' type='checkbox'\u003e\u003clabel for='data-23438b93-37b1-4f4b-bdd4-e8120cf7cf5d' title='Show/Hide data repr'\u003e\u003csvg class='icon xr-icon-database'\u003e\u003cuse xlink:href='#icon-database'\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/label\u003e\u003cdiv class='xr-var-attrs'\u003e\u003cdl class='xr-attrs'\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class='xr-var-data'\u003e\u003cpre\u003earray([[[[ nan, nan, nan, ..., nan, nan, nan], [ nan, 5.21800999e-02, -3.14787543e-02, ..., nan, nan, nan], [ nan, 7.78962457e-02, 3.07551194e-01, ..., nan, nan, nan], ..., [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan]], [[ nan, nan, nan, ..., nan, nan, nan], [ nan, 4.14379124e-02, -4.23837022e-02, ..., nan, nan, nan], [ nan, 6.73168186e-02, 3.03644944e-01, ..., nan, nan, nan], … [ nan, nan, nan, …, nan, nan, nan], [ nan, nan, nan, …, nan, nan, nan], [ nan, nan, nan, …, nan, nan, nan]],\n[[ nan, nan, nan, ..., nan, nan, nan], [ nan, -3.47875144e-01, -3.31110821e-01, ..., nan, nan, nan], [ nan, -3.58129050e-01, -7.06761863e-01, ..., nan, nan, nan], ..., [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan]]]])\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\u003cli class='xr-var-item'\u003e\u003cdiv class='xr-var-name'\u003e\u003cspan\u003eresidual\u003c/span\u003e\u003c/div\u003e\u003cdiv class='xr-var-dims'\u003e(t_dim, z_dim, y_dim, x_dim)\u003c/div\u003e\u003cdiv class='xr-var-dtype'\u003efloat64\u003c/div\u003e\u003cdiv class='xr-var-preview xr-preview'\u003enan nan nan nan ... nan nan nan nan\u003c/div\u003e\u003cinput id='attrs-d2f66755-d9f1-4b00-a340-1fe9c5dac6f4' class='xr-var-attrs-in' type='checkbox' disabled\u003e\u003clabel for='attrs-d2f66755-d9f1-4b00-a340-1fe9c5dac6f4' title='Show/Hide attributes'\u003e\u003csvg class='icon xr-icon-file-text2'\u003e\u003cuse xlink:href='#icon-file-text2'\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/label\u003e\u003cinput id='data-b89d118b-dd34-4eb1-8c61-21fbd19156f7' class='xr-var-data-in' type='checkbox'\u003e\u003clabel for='data-b89d118b-dd34-4eb1-8c61-21fbd19156f7' title='Show/Hide data repr'\u003e\u003csvg class='icon xr-icon-database'\u003e\u003cuse xlink:href='#icon-database'\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/label\u003e\u003cdiv class='xr-var-attrs'\u003e\u003cdl class='xr-attrs'\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class='xr-var-data'\u003e\u003cpre\u003earray([[[[ nan, nan, nan, ..., nan, nan, nan], [ nan, -0.08379255, -0.08379255, ..., nan, nan, nan], [ nan, -0.08379255, -0.08379255, ..., nan, nan, nan], ..., [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan]], [[ nan, nan, nan, ..., nan, nan, nan], [ nan, -0.08379255, -0.08379255, ..., nan, nan, nan], [ nan, -0.08379255, -0.08379255, ..., nan, nan, nan], … [ nan, nan, nan, …, nan, nan, nan], [ nan, nan, nan, …, nan, nan, nan], [ nan, nan, nan, …, nan, nan, nan]],\n[[ nan, nan, nan, ..., nan, nan, nan], [ nan, -0.04929838, -0.04929838, ..., nan, nan, nan], [ nan, -0.04929838, -0.04929838, ..., nan, nan, nan], ..., [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan]]]])\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/li\u003e\u003cli class='xr-section-item'\u003e\u003cinput id='section-f5fecc2c-44d7-4531-8250-4aad378b670a' class='xr-section-summary-in' type='checkbox' \u003e\u003clabel for='section-f5fecc2c-44d7-4531-8250-4aad378b670a' class='xr-section-summary' \u003eIndexes: \u003cspan\u003e(1)\u003c/span\u003e\u003c/label\u003e\u003cdiv class='xr-section-inline-details'\u003e\u003c/div\u003e\u003cdiv class='xr-section-details'\u003e\u003cul class='xr-var-list'\u003e\u003cli class='xr-var-item'\u003e\u003cdiv class='xr-index-name'\u003e\u003cdiv\u003et_dim\u003c/div\u003e\u003c/div\u003e\u003cdiv class='xr-index-preview'\u003ePandasIndex\u003c/div\u003e\u003cdiv\u003e\u003c/div\u003e\u003cinput id='index-3b08e017-a87f-4770-87d1-d0a6a1803868' class='xr-index-data-in' type='checkbox'/\u003e\u003clabel for='index-3b08e017-a87f-4770-87d1-d0a6a1803868' title='Show/Hide index repr'\u003e\u003csvg class='icon xr-icon-database'\u003e\u003cuse xlink:href='#icon-database'\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/label\u003e\u003cdiv class='xr-index-data'\u003e\u003cpre\u003ePandasIndex(DatetimeIndex([\u0026#x27;2010-01-01\u0026#x27;, \u0026#x27;2010-02-01\u0026#x27;, \u0026#x27;2010-03-01\u0026#x27;, \u0026#x27;2010-04-01\u0026#x27;, \u0026#x27;2010-05-01\u0026#x27;, \u0026#x27;2010-06-01\u0026#x27;, \u0026#x27;2010-07-01\u0026#x27;, \u0026#x27;2010-08-01\u0026#x27;, \u0026#x27;2010-09-01\u0026#x27;, \u0026#x27;2010-10-01\u0026#x27;, \u0026#x27;2010-11-01\u0026#x27;, \u0026#x27;2010-12-01\u0026#x27;, \u0026#x27;2011-01-01\u0026#x27;, \u0026#x27;2011-02-01\u0026#x27;, \u0026#x27;2011-03-01\u0026#x27;, \u0026#x27;2011-04-01\u0026#x27;, \u0026#x27;2011-05-01\u0026#x27;, \u0026#x27;2011-06-01\u0026#x27;, \u0026#x27;2011-07-01\u0026#x27;, \u0026#x27;2011-08-01\u0026#x27;, \u0026#x27;2011-09-01\u0026#x27;, \u0026#x27;2011-10-01\u0026#x27;, \u0026#x27;2011-11-01\u0026#x27;, \u0026#x27;2011-12-01\u0026#x27;, \u0026#x27;2012-01-01\u0026#x27;, \u0026#x27;2012-02-01\u0026#x27;, \u0026#x27;2012-03-01\u0026#x27;, \u0026#x27;2012-04-01\u0026#x27;, \u0026#x27;2012-05-01\u0026#x27;, \u0026#x27;2012-06-01\u0026#x27;, \u0026#x27;2012-07-01\u0026#x27;, \u0026#x27;2012-08-01\u0026#x27;, \u0026#x27;2012-09-01\u0026#x27;, \u0026#x27;2012-10-01\u0026#x27;, \u0026#x27;2012-11-01\u0026#x27;, \u0026#x27;2012-12-01\u0026#x27;, \u0026#x27;2013-01-01\u0026#x27;, \u0026#x27;2013-02-01\u0026#x27;, \u0026#x27;2013-03-01\u0026#x27;, \u0026#x27;2013-04-01\u0026#x27;, \u0026#x27;2013-05-01\u0026#x27;, \u0026#x27;2013-06-01\u0026#x27;, \u0026#x27;2013-07-01\u0026#x27;, \u0026#x27;2013-08-01\u0026#x27;, \u0026#x27;2013-09-01\u0026#x27;, \u0026#x27;2013-10-01\u0026#x27;, \u0026#x27;2013-11-01\u0026#x27;, \u0026#x27;2013-12-01\u0026#x27;], dtype=\u0026#x27;datetime64[ns]\u0026#x27;, name=\u0026#x27;t_dim\u0026#x27;, freq=None))\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/li\u003e\u003cli class='xr-section-item'\u003e\u003cinput id='section-23e8ea85-b0a5-4ce1-8c0c-039923af6f00' class='xr-section-summary-in' type='checkbox' disabled \u003e\u003clabel for='section-23e8ea85-b0a5-4ce1-8c0c-039923af6f00' class='xr-section-summary' title='Expand/collapse section'\u003eAttributes: \u003cspan\u003e(0)\u003c/span\u003e\u003c/label\u003e\u003cdiv class='xr-section-inline-details'\u003e\u003c/div\u003e\u003cdiv class='xr-section-details'\u003e\u003cdl class='xr-attrs'\u003e\u003c/dl\u003e\u003c/div\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/div\u003e Plot the decomposed time series\ncomponent = xr.DataArray( [\"trend\",\"seasonal\",\"residual\"], dims=\"component\", name=\"component\" ) temp_decomp = xr.concat( [grd.dataset.trend, grd.dataset.seasonal,grd.dataset.residual], dim=component ) temp_decomp.name = \"temperature\" temp_decomp[:,:,0,200,200].plot(hue=\"component\") [\u003cmatplotlib.lines.Line2D at 0x7fa800117730\u003e, \u003cmatplotlib.lines.Line2D at 0x7fa80013fc70\u003e, \u003cmatplotlib.lines.Line2D at 0x7fa80013c6a0\u003e] ","categories":"","description":"Seasonal decomp example example.\n","excerpt":"Seasonal decomp example example.\n","ref":"/COAsT/docs/examples/notebooks/general/seasonal_decomp_example/","tags":"","title":"Seasonal decomp example"},{"body":"","categories":"","description":"Examples Tidegauge scripts for the COAsT package.\n","excerpt":"Examples Tidegauge scripts for the COAsT package.\n","ref":"/COAsT/docs/examples/notebooks/tidegauge/","tags":"","title":"Tidegauge"},{"body":"This tutorial gives an overview of some of validation tools available when using the Tidegauge objects in COAsT.\nThis includes:\ncreating tidegauge objects reading in tidegauge data creating tidegauge object from gridded simulation data basic plotting on maps and timeseries analysis harmonic analysis and calculation of non-tidal residual doodsonX0 tidal filtering threshold statistics error calculation: mean errors, mean absolute error (MAE), mean square error (MSE) Import necessary libraries import xarray as xr import numpy as np import coast import datetime import matplotlib.pyplot as plt /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pydap/lib.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.responses')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.handlers')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.tests')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages Define paths fn_dom = \"\u003cPATH_TO_NEMO_DOMAIN\u003e\" fn_dat = \"\u003cPATH_TO_NEMO_DATA\u003e\" fn_config = \"\u003cPATH_TO_CONFIG.json\u003e\" fn_tg = \"\u003cPATH_TO_TIDEGAUGE_NETCDF\u003e\" # This should already be processed, on the same time dimension # Change this to 0 to not use default files. if 1: #print(f\"Use default files\") dir = \"./example_files/\" fn_dom = dir + \"coast_example_nemo_domain.nc\" fn_dat = dir + \"coast_example_nemo_data.nc\" fn_config = \"./config/example_nemo_grid_t.json\" fn_tidegauge = dir + \"tide_gauges/lowestoft-p024-uk-bodc\" fn_tidegauge_bodc = dir + \"tide_gauges/LIV2010.txt\" fn_tg = dir + \"tide_gauges/coast_example_tidegauges.nc\" # These are a collection (xr.DataSet) of tidegauge observations. Created for this demonstration, they are synthetic. Reading data We can create our empty tidegauge object:\ntidegauge = coast.Tidegauge() Tidegauge object at 0x562173632980 initialised The Tidegauge class contains multiple methods for reading different typical tidegauge formats. This includes reading from the GESLA and BODC databases. To read a gesla file between two dates, we can use:\ndate0 = datetime.datetime(2007,1,10) date1 = datetime.datetime(2007,1,12) tidegauge.read_gesla(fn_tidegauge, date_start = date0, date_end = date1) A Tidegauge object is a type of Timeseries object, so it has the form:\ntidegauge.dataset \u003cxarray.Dataset\u003e Dimensions: (id_dim: 1, t_dim: 193) Coordinates: time (t_dim) datetime64[ns] 2007-01-10 … 2007-01-12 longitude (id_dim) float64 1.751 latitude (id_dim) float64 52.47 site_name (id_dim) \u003cU9 'Lowestoft' Dimensions without coordinates: id_dim, t_dim Data variables: ssh (id_dim, t_dim) float64 2.818 2.823 2.871 … 3.214 3.257 3.371 qc_flags (id_dim, t_dim) int64 1 1 1 1 1 1 1 1 1 1 … 1 1 1 1 1 1 1 1 1 1xarray.DatasetDimensions:id_dim: 1t_dim: 193Coordinates: (4)time(t_dim)datetime64[ns]2007-01-10 … 2007-01-12array(['2007-01-10T00:00:00.000000000', '2007-01-10T00:15:00.000000000', '2007-01-10T00:30:00.000000000', '2007-01-10T00:45:00.000000000', '2007-01-10T01:00:00.000000000', '2007-01-10T01:15:00.000000000', '2007-01-10T01:30:00.000000000', '2007-01-10T01:45:00.000000000', '2007-01-10T02:00:00.000000000', '2007-01-10T02:15:00.000000000', '2007-01-10T02:30:00.000000000', '2007-01-10T02:45:00.000000000', '2007-01-10T03:00:00.000000000', '2007-01-10T03:15:00.000000000', '2007-01-10T03:30:00.000000000', '2007-01-10T03:45:00.000000000', '2007-01-10T04:00:00.000000000', '2007-01-10T04:15:00.000000000', '2007-01-10T04:30:00.000000000', '2007-01-10T04:45:00.000000000', '2007-01-10T05:00:00.000000000', '2007-01-10T05:15:00.000000000', '2007-01-10T05:30:00.000000000', '2007-01-10T05:45:00.000000000', '2007-01-10T06:00:00.000000000', '2007-01-10T06:15:00.000000000', '2007-01-10T06:30:00.000000000', '2007-01-10T06:45:00.000000000', '2007-01-10T07:00:00.000000000', '2007-01-10T07:15:00.000000000', '2007-01-10T07:30:00.000000000', '2007-01-10T07:45:00.000000000', '2007-01-10T08:00:00.000000000', '2007-01-10T08:15:00.000000000', '2007-01-10T08:30:00.000000000', '2007-01-10T08:45:00.000000000', '2007-01-10T09:00:00.000000000', '2007-01-10T09:15:00.000000000', '2007-01-10T09:30:00.000000000', '2007-01-10T09:45:00.000000000', … '2007-01-11T14:30:00.000000000', '2007-01-11T14:45:00.000000000', '2007-01-11T15:00:00.000000000', '2007-01-11T15:15:00.000000000', '2007-01-11T15:30:00.000000000', '2007-01-11T15:45:00.000000000', '2007-01-11T16:00:00.000000000', '2007-01-11T16:15:00.000000000', '2007-01-11T16:30:00.000000000', '2007-01-11T16:45:00.000000000', '2007-01-11T17:00:00.000000000', '2007-01-11T17:15:00.000000000', '2007-01-11T17:30:00.000000000', '2007-01-11T17:45:00.000000000', '2007-01-11T18:00:00.000000000', '2007-01-11T18:15:00.000000000', '2007-01-11T18:30:00.000000000', '2007-01-11T18:45:00.000000000', '2007-01-11T19:00:00.000000000', '2007-01-11T19:15:00.000000000', '2007-01-11T19:30:00.000000000', '2007-01-11T19:45:00.000000000', '2007-01-11T20:00:00.000000000', '2007-01-11T20:15:00.000000000', '2007-01-11T20:30:00.000000000', '2007-01-11T20:45:00.000000000', '2007-01-11T21:00:00.000000000', '2007-01-11T21:15:00.000000000', '2007-01-11T21:30:00.000000000', '2007-01-11T21:45:00.000000000', '2007-01-11T22:00:00.000000000', '2007-01-11T22:15:00.000000000', '2007-01-11T22:30:00.000000000', '2007-01-11T22:45:00.000000000', '2007-01-11T23:00:00.000000000', '2007-01-11T23:15:00.000000000', '2007-01-11T23:30:00.000000000', '2007-01-11T23:45:00.000000000', '2007-01-12T00:00:00.000000000'], dtype='datetime64[ns]')longitude(id_dim)float641.751array([1.75083])latitude(id_dim)float6452.47array([52.473])site_name(id_dim)\u003cU9'Lowestoft'array(['Lowestoft'], dtype='\u003cU9')Data variables: (2)ssh(id_dim, t_dim)float642.818 2.823 2.871 … 3.257 3.371array([[ 2.818, 2.823, 2.871, 2.931, 2.961, 2.979, 2.953, 2.913, 2.864, 2.806, 2.723, 2.664, 2.606, 2.511, 2.43 , 2.379, 2.296, 2.201, 2.105, 2.006, 1.908, 1.801, 1.684, 1.579, 1.494, 1.402, 1.306, 1.233, 1.171, 1.102, 1.054, 1.028, 0.989, 0.97 , 0.983, 1.004, 1.026, 1.068, 1.153, 1.225, 1.296, 1.362, 1.436, 1.481, 1.536, 1.615, 1.695, 1.726, 1.802, 1.842, 1.86 , 1.872, 1.897, 1.912, 1.946, 1.994, 2.006, 2.028, 2.067, 2.081, 2.098, 2.137, 2.113, 2.068, 2.053, 1.985, 1.917, 1.869, 1.803, 1.695, 1.642, 1.545, 1.463, 1.463, 1.466, 1.462, 1.476, 1.524, 1.574, 1.633, 1.661, 1.717, 1.818, 1.918, 2.018, 2.093, 2.14 , 2.223, 2.278, 2.303, 2.372, 2.375, 2.395, 2.468, 2.481, 2.487, 2.535, 2.543, 2.578, 2.621, 2.627, 2.626, 2.585, 2.563, 2.529, 2.451, 2.335, 2.207, 2.086, 1.982, 1.855, 1.741, 1.618, 1.531, 1.429, 1.342, 1.246, 1.141, 1.031, 0.902, 0.784, 0.673, 0.571, 0.457, 0.323, 0.203, 0.13 , 0.056, -0.028, -0.077, -0.093, -0.143, -0.181, -0.211, -0.217, -0.182, -0.1 , -0.046, 0.02 , 0.121, 0.247, 0.358, 0.468, 0.65 , 0.845, 0.987, 1.059, 1.199, 1.322, 1.38 , 1.465, 1.519, 1.559, 1.691, 1.775, 1.844, 2.019, 2.113, 2.159, 2.266, 2.311, 2.406, 2.512, 2.533, 2.43 , 2.309, 2.185, 2.136, 2.086, 2.066, 2.114, 2.114, 2.051, 2.033, 2.055, 2.1 , 2.192, 2.278, 2.334, 2.421, 2.497, 2.548, 2.603, 2.679, 2.803, 2.859, 2.875, 3.001, 3.075, 3.135, 3.214, 3.257, 3.371]])qc_flags(id_dim, t_dim)int641 1 1 1 1 1 1 1 … 1 1 1 1 1 1 1 1array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])Indexes: (0)Attributes: (0)\nYou can do the same for BODC data:\ndate0 = np.datetime64(\"2020-10-11 07:59\") date1 = np.datetime64(\"2020-10-20 20:21\") tidegauge.read_bodc(fn_tidegauge_bodc, date0, date1) tidegauge.dataset \u003cxarray.Dataset\u003e Dimensions: (id_dim: 1, t_dim: 914) Coordinates: time (t_dim) datetime64[ns] 2020-10-11T08:00:00 … 2020-10-20T20:1… longitude (id_dim) float64 -3.018 latitude (id_dim) float64 53.45 site_name (id_dim) \u003cU25 'liverpool,_gladstone_dock' Dimensions without coordinates: id_dim, t_dim Data variables: ssh (id_dim, t_dim) float64 5.369 5.196 5.032 … 1.668 1.568 1.545 qc_flags (id_dim, t_dim) \u003cU1 'M' 'M' 'M' 'M' 'M' … 'M' 'M' 'M' 'M' 'M' Attributes: port: p234 site: liverpool,gladstone_dock start_date: 01oct2020-00.00.00 end_date: 31oct2020-23.45.00 contributor: national_oceanography_centre,liverpool datum_information: the_data_refer_to_admiralty_chart_datum(acd) parameter_code: aslvbg02=surface_elevation(unspecified_datum)_of_t…xarray.DatasetDimensions:id_dim: 1t_dim: 914Coordinates: (4)time(t_dim)datetime64[ns]2020-10-11T08:00:00 … 2020-10-…array(['2020-10-11T08:00:00.000000000', '2020-10-11T08:15:00.000000000', '2020-10-11T08:30:00.000000000', '2020-10-11T08:45:00.000000000', '2020-10-11T09:00:00.000000000', '2020-10-11T09:15:00.000000000', '2020-10-11T09:30:00.000000000', '2020-10-11T09:45:00.000000000', '2020-10-11T10:00:00.000000000', '2020-10-11T10:15:00.000000000', '2020-10-11T10:30:00.000000000', '2020-10-11T10:45:00.000000000', '2020-10-11T11:00:00.000000000', '2020-10-11T11:15:00.000000000', '2020-10-11T11:30:00.000000000', '2020-10-11T11:45:00.000000000', '2020-10-11T12:00:00.000000000', '2020-10-11T12:15:00.000000000', '2020-10-11T12:30:00.000000000', '2020-10-11T12:45:00.000000000', '2020-10-11T13:00:00.000000000', '2020-10-11T13:15:00.000000000', '2020-10-11T13:30:00.000000000', '2020-10-11T13:45:00.000000000', '2020-10-11T14:00:00.000000000', '2020-10-11T14:15:00.000000000', '2020-10-11T14:30:00.000000000', '2020-10-11T14:45:00.000000000', '2020-10-11T15:00:00.000000000', '2020-10-11T15:15:00.000000000', '2020-10-11T15:30:00.000000000', '2020-10-11T15:45:00.000000000', '2020-10-11T16:00:00.000000000', '2020-10-11T16:15:00.000000000', '2020-10-11T16:30:00.000000000', '2020-10-11T16:45:00.000000000', '2020-10-11T17:00:00.000000000', '2020-10-11T17:15:00.000000000', '2020-10-11T17:30:00.000000000', '2020-10-11T17:45:00.000000000', … '2020-10-20T11:00:00.000000000', '2020-10-20T11:15:00.000000000', '2020-10-20T11:30:00.000000000', '2020-10-20T11:45:00.000000000', '2020-10-20T12:00:00.000000000', '2020-10-20T12:15:00.000000000', '2020-10-20T12:30:00.000000000', '2020-10-20T12:45:00.000000000', '2020-10-20T13:00:00.000000000', '2020-10-20T13:15:00.000000000', '2020-10-20T13:30:00.000000000', '2020-10-20T13:45:00.000000000', '2020-10-20T14:00:00.000000000', '2020-10-20T14:15:00.000000000', '2020-10-20T14:30:00.000000000', '2020-10-20T14:45:00.000000000', '2020-10-20T15:00:00.000000000', '2020-10-20T15:15:00.000000000', '2020-10-20T15:30:00.000000000', '2020-10-20T15:45:00.000000000', '2020-10-20T16:00:00.000000000', '2020-10-20T16:15:00.000000000', '2020-10-20T16:30:00.000000000', '2020-10-20T16:45:00.000000000', '2020-10-20T17:00:00.000000000', '2020-10-20T17:15:00.000000000', '2020-10-20T17:30:00.000000000', '2020-10-20T17:45:00.000000000', '2020-10-20T18:00:00.000000000', '2020-10-20T18:15:00.000000000', '2020-10-20T18:30:00.000000000', '2020-10-20T18:45:00.000000000', '2020-10-20T19:00:00.000000000', '2020-10-20T19:15:00.000000000', '2020-10-20T19:30:00.000000000', '2020-10-20T19:45:00.000000000', '2020-10-20T20:00:00.000000000', '2020-10-20T20:15:00.000000000'], dtype='datetime64[ns]')longitude(id_dim)float64-3.018array([-3.018])latitude(id_dim)float6453.45array([53.44969])site_name(id_dim)\u003cU25'liverpool,_gladstone_dock'array(['liverpool,_gladstone_dock'], dtype='\u003cU25')Data variables: (2)ssh(id_dim, t_dim)float645.369 5.196 5.032 … 1.568 1.545array([[ 5.369, 5.196, 5.032, 4.886, 4.731, 4.579, 4.415, 4.284, 4.171, 4.054, 3.943, 3.853, 3.77 , 3.715, 3.663, 3.641, 3.645, 3.68 , 3.727, 3.794, 3.905, 4.03 , 4.18 , 4.344, 4.542, 4.76 , 4.977, 5.198, 5.438, 5.675, 5.87 , 6.084, 6.283, 6.451, 6.593, 6.721, 6.819, 6.909, 6.974, 6.995, 6.992, 6.97 , 6.916, 6.846, 6.744, 6.609, 6.468, 6.315, 6.164, 5.981, 5.792, 5.606, 5.407, 5.208, 5.025, 4.833, 4.644, 4.458, 4.278, 4.107, 3.965, 3.823, 3.686, 3.57 , 3.457, 3.377, 3.324, 3.278, 3.252, 3.261, 3.308, 3.393, 3.508, 3.676, 3.896, 4.127, 4.363, 4.628, 4.895, 5.185, 5.479, 5.763, 6.013, 6.248, 6.467, 6.685, 6.845, 6.99 , 7.107, 7.185, 7.233, 7.269, 7.281, 7.242, 7.188, 7.085, 6.97 , 6.845, 6.687, 6.509, 6.287, 6.101, 5.917, 5.708, 5.493, 5.289, 5.092, 4.902, 4.723, 4.569, 4.427, 4.292, 4.185, 4.083, 3.978, 3.876, 3.806, 3.772, 3.734, 3.721, 3.721, 3.79 , 3.857, 3.964, 4.127, 4.306, 4.513, 4.765, 5.011, 5.274, 5.566, 5.807, 6.066, 6.332, 6.559, 6.734, 6.915, 7.046, 7.148, 7.264, 7.245, 7.3 , 7.345, 7.258, 7.201, 7.087, 6.913, 6.831, 6.567, 6.391, 6.2 , 5.951, 5.681, 5.379, 5.155, 4.939, 4.717, 4.461, 4.235, 4.021, … 1.458, 1.21 , 0.984, 0.835, 0.708, 0.631, 0.669, 0.801, 1.062, 1.458, 2.037, 2.767, 3.555, 4.361, 5.136, 5.904, 6.613, 7.257, 7.804, 8.296, 8.731, 9.117, 9.421, 9.663, 9.852, 10.012, 10.071, 10.01 , 9.831, 9.633, 9.375, 9.056, 8.648, 8.181, 7.693, 7.209, 6.71 , 6.21 , 5.736, 5.295, 4.839, 4.399, 3.987, 3.594, 3.217, 2.876, 2.551, 2.247, 1.959, 1.691, 1.45 , 1.24 , 1.071, 0.952, 0.917, 0.978, 1.126, 1.393, 1.795, 2.295, 2.939, 3.674, 4.435, 5.187, 5.94 , 6.636, 7.285, 7.854, 8.349, 8.793, 9.169, 9.471, 9.72 , 9.91 , 10.081, 10.143, 10.113, 9.996, 9.819, 9.597, 9.277, 8.924, 8.536, 8.044, 7.57 , 7.12 , 6.669, 6.225, 5.744, 5.313, 4.883, 4.469, 4.064, 3.68 , 3.335, 3.003, 2.687, 2.391, 2.124, 1.857, 1.667, 1.49 , 1.352, 1.277, 1.28 , 1.365, 1.561, 1.859, 2.277, 2.808, 3.446, 4.11 , 4.78 , 5.489, 6.191, 6.855, 7.453, 7.995, 8.469, 8.886, 9.228, 9.516, 9.752, 9.95 , 10.09 , 10.13 , 10.08 , 9.928, 9.754, 9.521, 9.2 , 8.817, 8.406, 7.95 , 7.49 , 7.043, 6.583, 6.122, 5.682, 5.257, 4.825, 4.428, 4.054, 3.69 , 3.354, 3.05 , 2.765, 2.484, 2.228, 2.015, 1.821, 1.668, 1.568, 1.545]])qc_flags(id_dim, t_dim)\u003cU1'M' 'M' 'M' 'M' … 'M' 'M' 'M' 'M'array([['M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', … 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M']], dtype='\u003cU1')Indexes: (0)Attributes: (7)port :p234site :liverpool,gladstone_dockstart_date :01oct2020-00.00.00end_date :31oct2020-23.45.00contributor :national_oceanography_centre,liverpooldatum_information :the_data_refer_to_admiralty_chart_datum(acd)parameter_code :aslvbg02=surface_elevation(unspecified_datum)of_the_water_body_by_bubbler_tide_gauge(second_sensor)\nAn example data variable could be ssh, or ntr (non-tidal residual). This object can also be used for other instrument types, not just tide gauges. For example moorings.\nEvery id index for this object should use the same time coordinates. Therefore, timeseries need to be aligned before being placed into the object. If there is any padding needed, then NaNs should be used. NaNs should also be used for quality control/data rejection.\nFor the rest of our examples, we will use data from multiple tide gauges on the same time dimension. It will be read in from a simple netCDF file:\ntt = xr.open_dataset(fn_tg) obs = coast.Tidegauge(dataset=tt) Tidegauge object at 0x562173632980 initialised # Create the object and then inset the netcdf dataset tt = xr.open_dataset(fn_tg) obs = coast.Tidegauge(dataset=tt) obs.dataset = obs.dataset.set_coords(\"time\") Tidegauge object at 0x562173632980 initialised Quick plotting to visualise Tidegauge data Tidegauge has ready made quick plotting routines for viewing time series and tide gauge location. To look at the tide gauge location:\nfig, ax = obs.plot_on_map() /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/cartopy/io/__init__.py:241: DownloadWarning: Downloading: https://naturalearth.s3.amazonaws.com/50m_physical/ne_50m_coastline.zip There is also a slightly expanded version where you can plot multiple tidegauge objects, included as a list, and also the colour (if it is a dataarray with one value per location).\n# plot a list tidegauge datasets (here repeating obs for the point of demonstration) and colour fig, ax = obs.plot_on_map_multiple([obs,obs], color_var_str=\"latitude\") Time series can be plotted using matplotlib.pyplot methods. However xarray has its own plotting wrappers that can be used, which offers some conveniences with labelling\nstn_id=26 # pick a gauge station plt.subplot(2,1,1) obs.dataset.ssh[stn_id].plot() # rename time dimension to enable automatic x-axis labelling plt.subplot(2,1,2) obs.dataset.ssh[stn_id].swap_dims({'t_dim':'time'}).plot() # rename time dimension to enable automatic x-axis labelling plt.tight_layout() plt.show() Or you can use the Tidegauge.plot_timeseries() method, in which start and end dates can also be specified.\nobs.isel(id_dim=stn_id).plot_timeseries(date_start=np.datetime64('2007-01-01'), date_end = np.datetime64('2007-01-31') ) (\u003cFigure size 1000x1000 with 1 Axes\u003e, \u003cmatplotlib.collections.PathCollection at 0x7f67bd654940\u003e) Basic manipulation: subsetting + plotting We can do some simple spatial and temporal manipulations of this data:\n# Cut out a time window obs_cut = obs.time_slice( date0 = datetime.datetime(2007, 1, 1), date1 = datetime.datetime(2007,1,31)) # Cut out geographical boxes obs_upper = obs_cut.subset_indices_lonlat_box(lonbounds = [0, 3], latbounds = [50, 55]) obs_lower = obs_cut.subset_indices_lonlat_box(lonbounds = [-9, -3], latbounds = [55.5, 59]) #fig, ax = obs_cut.plot_on_map() fig, ax = obs_upper.plot_on_map_multiple([obs_upper, obs_lower], color_var_str=\"latitude\") Tidegauge object at 0x562173632980 initialised Tidegauge object at 0x562173632980 initialised Tidegauge object at 0x562173632980 initialised Gridded model comparison To compare our observations to the model, we will interpolate a model variable to the same time and geographical space as the tidegauge. This is done using the obs_operator() method.\nBut first load some gridded data and manipulate some variable names for convenience\nnemo = coast.Gridded(fn_dat, fn_dom, multiple=True, config=fn_config) # Rename depth_0 to be depth nemo.dataset = nemo.dataset.rename({\"depth_0\": \"depth\"}) #nemo.dataset = nemo.dataset[[\"ssh\", \"landmask\"]] /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/xarray/core/dataset.py:278: UserWarning: The specified chunks separate the stored chunks along dimension \"time_counter\" starting at index 2. This could degrade performance. Instead, consider rechunking after loading. interpolate model onto obs locations\ntidegauge_from_model = obs.obs_operator(nemo, time_interp='nearest') Doing this would create a new interpolated tidegauge called tidegauge_from_model Take a look at tidegauge_from_model.dataset to see for yourself. If a landmask variable is present in the Gridded dataset then the nearest wet points will be taken. Otherwise, just the nearest point is taken. If landmask is required but not present you will need to insert it into the dataset yourself. For nemo data, you could use the bottom_level or mbathy variables to do this. E.g:\n# Create a landmask array and put it into the nemo object. # Here, using the bottom_level == 0 variable from the domain file is enough. nemo.dataset[\"landmask\"] = nemo.dataset.bottom_level == 0 # Then do the interpolation tidegauge_from_model = obs.obs_operator(nemo, time_interp='nearest') Calculating spatial indices. Calculating time indices. Indexing model data at tide gauge locations.. Calculating interpolation distances. Interpolating in time... Tidegauge object at 0x562173632980 initialised However, the new tidegauge_from_model will the same number of time entries as the obs data, rather than the model data (so this will include lots of empty values). So, for a more useful demonstration we trim the observed gauge data so it better matches the model data.\n# Cut down data to be only in 2007 to match model data. start_date = datetime.datetime(2007, 1, 1) end_date = datetime.datetime(2007, 1, 31) obs = obs.time_slice(start_date, end_date) Tidegauge object at 0x562173632980 initialised Interpolate model data onto obs locations model_timeseries = obs.obs_operator(nemo) # Take a look model_timeseries.dataset Calculating spatial indices. Calculating time indices. Indexing model data at tide gauge locations.. Calculating interpolation distances. Interpolating in time... Tidegauge object at 0x562173632980 initialised \u003cxarray.Dataset\u003e Dimensions: (z_dim: 51, axis_nbounds: 2, t_dim: 720, id_dim: 61) Coordinates: longitude (id_dim) float32 1.444 -4.0 -5.333 … 7.666 -9.111 8.777 latitude (id_dim) float32 51.93 51.53 58.0 51.67 … 58.0 51.53 53.87 depth (z_dim, id_dim) float32 0.1001 0.2183 0.2529 … 27.32 10.11\ntime (t_dim) datetime64[ns] 2007-01-01 … 2007-01-30T23:00:00 Dimensions without coordinates: z_dim, axis_nbounds, t_dim, id_dim Data variables: deptht_bounds (z_dim, axis_nbounds) float32 0.0 6.157 … 5.924e+03 ssh (t_dim, id_dim) float32 nan nan nan … 0.3721 -0.0752 0.7412 temperature (t_dim, z_dim, id_dim) float32 nan nan nan … nan nan nan bathymetry (id_dim) float32 10.0 21.81 6.075 15.56 … 17.8 14.06 10.0 e1 (id_dim) float32 7.618e+03 7.686e+03 … 7.686e+03 7.285e+03 e2 (id_dim) float32 7.414e+03 7.414e+03 … 7.414e+03 7.414e+03 e3_0 (z_dim, id_dim) float32 0.2002 0.4365 0.5059 … 0.541 0.2002 bottom_level (id_dim) float32 50.0 50.0 12.0 32.0 … 44.0 17.0 26.0 50.0 landmask (id_dim) bool False False False False … False False False interp_dist (id_dim) float64 10.56 4.33 15.65 6.018 … 5.835 4.96 3.957 Attributes: name: AMM7_1d_20070101_20070131_25hourm_grid_T description: ocean T grid variables, 25h meaned title: ocean T grid variables, 25h meaned Conventions: CF-1.6 timeStamp: 2019-Dec-26 04:35:28 GMT uuid: 96cae459-d3a1-4f4f-b82b-9259179f95f7 history: Tue May 19 12:07:51 2020: ncks -v votemper,sossheig -d time… NCO: 4.4.7xarray.DatasetDimensions:z_dim: 51axis_nbounds: 2t_dim: 720id_dim: 61Coordinates: (4)longitude(id_dim)float321.444 -4.0 -5.333 … -9.111 8.777array([ 1.4443359 , -4. , -5.333008 , -5.111328 , -3. , -3.1113281 , -5.2226562 , -3. , -1.4443359 , 1.4443359 , -4.666992 , 1.3330078 , -6.2226562 , -2.555664 , -3.7783203 , -4.333008 , 1.1113281 , -0.55566406, -5.555664 , -5.111328 , -1.4443359 , -2. , -1.1113281 , -2.8886719 , -1.8886719 , -6.333008 , -3.1113281 , -6.2226562 , -0. , 0.11132812, -4. , 1.3330078 , -2.7783203 , -2.1113281 , -1.4443359 , -3.555664 , -1.4443359 , -2.7783203 , -3.2226562 , 1.7773438 , -5. , -5.555664 , -3.1113281 , -1.1113281 , -6.333008 , -5. , -4.7783203 , -6.666992 , -2.7783203 , -1.4443359 , -4.111328 , -4.111328 , -1.4443359 , 11.22168 , 11.777344 , 4.888672 , 11.22168 , -7.333008 , 7.6660156 , -9.111328 , 8.777344 ], dtype=float32)latitude(id_dim)float3251.93 51.53 58.0 … 51.53 53.87array([51.933594, 51.53418 , 58.000977, 51.666992, 54.000977, 51.26758 , 58.467773, 58.467773, 55.067383, 51.933594, 53.333984, 52.933594, 56.66797 , 50.600586, 53.333984, 50.333984, 51.53418 , 54.467773, 50.067383, 54.80078 , 55.067383, 57.13379 , 60.20117 , 51.53418 , 50.666992, 49.933594, 53.467773, 55.600586, 53.600586, 50.7334 , 57.600586, 51.067383, 51.53418 , 49.13379 , 55.067383, 54.666992, 55.067383, 51.53418 , 56.000977, 52.467773, 52.067383, 54.666992, 51.26758 , 50.7334 , 58.13379 , 55.734375, 54.067383, 55.26758 , 51.53418 , 55.067383, 52.734375, 51.26758 , 55.067383, 58.333984, 57.66797 , 61.93457 , 64.868164, 55.40039 , 58.000977, 51.53418 , 53.867188], dtype=float32)depth(z_dim, id_dim)float320.1001 0.2183 … 27.32 10.11units :mstandard_name :Depth at time zero on the t-gridarray([[1.00097656e-01, 2.18261719e-01, 2.52929688e-01, …, 5.02441406e-01, 2.70507812e-01, 1.00097656e-01], [3.00292969e-01, 6.54785156e-01, 7.58789062e-01, …, 1.49365234e+00, 8.11523438e-01, 3.00292969e-01], [5.00488281e-01, 1.09130859e+00, 1.26464844e+00, …, 2.46826172e+00, 1.35253906e+00, 5.00488281e-01], …, [9.70947266e+00, 2.11713867e+01, 2.45341797e+01, …, 1.70940918e+02, 2.62392578e+01, 9.70947266e+00], [9.90966797e+00, 2.16079102e+01, 2.50400391e+01, …, 1.75505371e+02, 2.67802734e+01, 9.90966797e+00], [1.01098633e+01, 2.20444336e+01, 2.55458984e+01, …, 1.79002441e+02, 2.73212891e+01, 1.01098633e+01]], dtype=float32)time(t_dim)datetime64[ns]2007-01-01 … 2007-01-30T23:00:00axis :Tstandard_name :timelong_name :Time axistime_origin :1900-01-01 00:00:00bounds :time_counter_boundsarray(['2007-01-01T00:00:00.000000000', '2007-01-01T01:00:00.000000000', '2007-01-01T02:00:00.000000000', …, '2007-01-30T21:00:00.000000000', '2007-01-30T22:00:00.000000000', '2007-01-30T23:00:00.000000000'], dtype='datetime64[ns]')Data variables: (10)deptht_bounds(z_dim, axis_nbounds)float320.0 6.157 … 5.72e+03 5.924e+03array([[ 0. , 6.1572266], [ 6.1572266, 12.678711 ], [ 12.678711 , 19.65332 ], [ 19.65332 , 27.19043 ], [ 27.19043 , 35.426758 ], [ 35.426758 , 44.527344 ], [ 44.527344 , 54.69922 ], [ 54.69922 , 66.19141 ], [ 66.19141 , 79.305664 ], [ 79.305664 , 94.41016 ], [ 94.41016 , 111.94238 ], [ 111.94238 , 132.41895 ], [ 132.41895 , 156.44531 ], [ 156.44531 , 184.71582 ], [ 184.71582 , 218.01562 ], [ 218.01562 , 257.20605 ], [ 257.20605 , 303.20508 ], [ 303.20508 , 356.95898 ], [ 356.95898 , 419.39258 ], [ 419.39258 , 491.35645 ], … [1955.7686 , 2136.3613 ], [2136.3613 , 2321.292 ], [2321.292 , 2509.8564 ], [2509.8564 , 2701.4355 ], [2701.4355 , 2895.504 ], [2895.504 , 3091.6123 ], [3091.6123 , 3289.3867 ], [3289.3867 , 3488.5156 ], [3488.5156 , 3688.7441 ], [3688.7441 , 3889.8613 ], [3889.8613 , 4091.6963 ], [4091.6963 , 4294.1094 ], [4294.1094 , 4496.9893 ], [4496.9893 , 4700.242 ], [4700.242 , 4903.797 ], [4903.797 , 5107.5938 ], [5107.5938 , 5311.584 ], [5311.584 , 5515.7295 ], [5515.7295 , 5720. ], [5720. , 5924.2705 ]], dtype=float32)ssh(t_dim, id_dim)float32nan nan nan … -0.0752 0.7412units :monline_operation :instantinterval_operation :300 sinterval_write :1 dcell_methods :time: point (interval: 300 s)array([[ nan, nan, nan, …, nan, nan, nan], [ nan, nan, nan, …, nan, nan, nan], [ nan, nan, nan, …, nan, nan, nan], …, [ 0.24902344, -0.046875 , 0.20019531, …, 0.3720703 , -0.07519531, 0.74121094], [ 0.24902344, -0.046875 , 0.20019531, …, 0.3720703 , -0.07519531, 0.74121094], [ 0.24902344, -0.046875 , 0.20019531, …, 0.3720703 , -0.07519531, 0.74121094]], dtype=float32)temperature(t_dim, z_dim, id_dim)float32nan nan nan nan … nan nan nan nanunits :degConline_operation :instantinterval_operation :300 sinterval_write :1 dcell_methods :time: point (interval: 300 s)array([[[ nan, nan, nan, …, nan, nan, nan], [ nan, nan, nan, …, nan, nan, nan], [ nan, nan, nan, …, nan, nan, nan], …, [ nan, nan, nan, …, nan, nan, nan], [ nan, nan, nan, …, nan, nan, nan], [ nan, nan, nan, …, nan, nan, nan]],\n[[ nan, nan, nan, …, nan, nan, nan], [ nan, nan, nan, …, nan, nan, nan], [ nan, nan, nan, …, nan, nan, nan], … [7.4384766, 8.22168 , nan, …, nan, nan, 5.0009766], [7.439453 , 8.22168 , nan, …, nan, nan, 5.0009766], [ nan, nan, nan, …, nan, nan, nan]],\n[[7.4335938, 8.217773 , 7.245117 , …, 5.8652344, 8.924805 , 5.0322266], [7.4345703, 8.219727 , 7.245117 , …, 5.8671875, 8.926758 , 5.0322266], [7.4345703, 8.219727 , 7.2421875, …, 5.8691406, 8.926758 , 5.0322266], …, [7.4384766, 8.22168 , nan, …, nan, nan, 5.0009766], [7.439453 , 8.22168 , nan, …, nan, nan, 5.0009766], [ nan, nan, nan, …, nan, nan, nan]]], dtype=float32)bathymetry(id_dim)float3210.0 21.81 6.075 … 14.06 10.0units :mstandard_name :bathymetrydescription :depth of last wet w-level on the horizontal t-gridarray([10. , 21.80957 , 6.0751953, 15.558594 , 10.65625 , 12.145508 , 53.125 , 47.216797 , 10.3125 , 10. , 21.898438 , 10.095703 , 11.674805 , 15.611328 , 11.230469 , 20.368164 , 10.25293 , 14.34082 , 23.28418 , 37.7666 , 10.3125 , 34.78711 , 15.394531 , 10. , 12.630859 , 21.493164 , 10. , 18.167969 , 10. , 17.354492 , 10. , 24.625 , 10. , 9.915039 , 10.3125 , 10. , 10.3125 , 10. , 10. , 10.0703125, 37.003906 , 10.40918 , 12.145508 , 10. , 30.885742 , 35.280273 , 17.089844 , 33.78711 , 10. , 10.3125 , 10. , 37.25879 , 10.3125 , 12.712891 , 8.652344 , 14.458984 , 83.96973 , 19.424805 , 17.804688 , 14.060547 , 10. ], dtype=float32)e1(id_dim)float327.618e+03 7.686e+03 … 7.285e+03array([7617.912 , 7685.6387, 6547.1533, 7663.1045, 7262.1484, 7730.582 , 6461.591 , 6461.591 , 7074.802 , 7617.912 , 7377.966 , 7446.9775, 6789.205 , 7842.205 , 7377.966 , 7886.5586, 7685.6387, 7180.49 , 7930.74 , 7121.871 , 7074.802 , 6704.8984, 6140.08 , 7685.6387, 7831.091 , 7952.7676, 7354.881 , 6980.205 , 7331.758 , 7819.965 , 6620.1465, 7764.1797, 7685.6387, 8084.0186, 7074.802 , 7145.3477, 7074.802 , 7685.6387, 6908.8594, 7527.032 , 7595.254 , 7145.3477, 7730.582 , 7819.965 , 6522.751 , 6956.461 , 7250.5117, 7039.3994, 7685.6387, 7074.802 , 7481.3477, 7730.582 , 7074.802 , 6486.082 , 6608.004 , 5812.949 , 5247.382 , 7015.75 , 6547.1533, 7685.6387, 7285.3906], dtype=float32)e2(id_dim)float327.414e+03 7.414e+03 … 7.414e+03array([7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633], dtype=float32)e3_0(z_dim, id_dim)float320.2002 0.4365 … 0.541 0.2002array([[0.20019531, 0.43652344, 0.5058594 , …, 1. , 0.5410156 , 0.20019531], [0.20019531, 0.43652344, 0.5058594 , …, 0.9824219 , 0.5410156 , 0.20019531], [0.20019531, 0.43652344, 0.5058594 , …, 0.96777344, 0.5410156 , 0.20019531], …, [0.20019531, 0.43652344, 0.5058594 , …, 5.0214844 , 0.5410156 , 0.20019531], [0.20019531, 0.43652344, 0.5058594 , …, 4.057617 , 0.5410156 , 0.20019531], [0.20019531, 0.43652344, 0.5058594 , …, 3.203125 , 0.5410156 , 0.20019531]], dtype=float32)bottom_level(id_dim)float3250.0 50.0 12.0 … 17.0 26.0 50.0array([50., 50., 12., 32., 50., 50., 50., 50., 21., 50., 29., 42., 32., 42., 40., 33., 50., 25., 37., 32., 21., 50., 20., 50., 44., 24., 50., 24., 50., 43., 50., 50., 50., 34., 21., 50., 21., 50., 50., 33., 50., 11., 50., 50., 33., 45., 29., 39., 50., 21., 50., 50., 21., 19., 25., 25., 50., 44., 17., 26., 50.], dtype=float32)landmask(id_dim)boolFalse False False … False Falsearray([False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False])interp_dist(id_dim)float6410.56 4.33 15.65 … 4.96 3.957array([10.55753639, 4.3298596 , 15.65224834, 6.01847521, 6.21944535, 6.03485965, 10.09762811, 5.83889348, 6.6716266 , 7.06250586, 3.81132755, 2.10403027, 10.89520817, 7.65307627, 3.12483581, 11.15992752, 27.30753169, 4.52090493, 4.06485834, 4.67908778, 6.6716266 , 4.96772739, 5.4813996 , 7.053344 , 5.35290231, 2.05865294, 6.49778121, 3.61860367, 12.72904061, 6.59773619, 0.24707792, 5.27676859, 5.09090741, 5.52241217, 6.6716266 , 1.95744877, 6.6716266 , 5.13046117, 2.83309362, 1.88771332, 6.06839329, 7.32299892, 6.03485965, 7.69045765, 8.78690904, 6.13400869, 2.11138548, 6.79115084, 5.13046117, 6.6716266 , 4.76955022, 6.29453137, 6.6716266 , 1.82312538, 2.14643925, 11.94656233, 1.34373275, 3.71293547, 5.83537153, 4.96020927, 3.9566329 ])Indexes: (1)timePandasIndexPandasIndex(DatetimeIndex(['2007-01-01 00:00:00', '2007-01-01 01:00:00', '2007-01-01 02:00:00', '2007-01-01 03:00:00', '2007-01-01 04:00:00', '2007-01-01 05:00:00', '2007-01-01 06:00:00', '2007-01-01 07:00:00', '2007-01-01 08:00:00', '2007-01-01 09:00:00', … '2007-01-30 14:00:00', '2007-01-30 15:00:00', '2007-01-30 16:00:00', '2007-01-30 17:00:00', '2007-01-30 18:00:00', '2007-01-30 19:00:00', '2007-01-30 20:00:00', '2007-01-30 21:00:00', '2007-01-30 22:00:00', '2007-01-30 23:00:00'], dtype='datetime64[ns]', name='time', length=720, freq=None))Attributes: (8)name :AMM7_1d_20070101_20070131_25hourm_grid_Tdescription :ocean T grid variables, 25h meanedtitle :ocean T grid variables, 25h meanedConventions :CF-1.6timeStamp :2019-Dec-26 04:35:28 GMTuuid :96cae459-d3a1-4f4f-b82b-9259179f95f7history :Tue May 19 12:07:51 2020: ncks -v votemper,sossheig -d time_counter,0,30,5 AMM7_1d_20070101_20070131_25hourm_grid_T.nc example_data.ncNCO :4.4.7\nstn_id=26 # pick a gauge station plt.subplot(2,1,1) model_timeseries.dataset.ssh.isel(id_dim=stn_id).plot() # rename time dimension to enable automatic x-axis labelling plt.subplot(2,1,2) obs.dataset.ssh.isel(id_dim=stn_id).swap_dims({'t_dim':'time'}).plot() # rename time dimension to enable automatic x-axis labelling plt.tight_layout() plt.show() We can see that the structure for the new dataset model_timeseries, generated from the gridded model simulation, is that of a tidegauge object. NB in the example simulation data the ssh variable is output as 5-day means. So it is not particulatly useful for high frequency validation but serves as a demonstration of the workflow.\nTidegauge analysis methods For a good comparison, we would like to make sure that both the observed and modelled Tidegauge objects contain the same missing values. TidegaugeAnalysis contains a routine for ensuring this. First create our analysis object:\ntganalysis = coast.TidegaugeAnalysis() # This routine searches for missing values in each dataset and applies them # equally to each corresponding dataset obs_new, model_new = tganalysis.match_missing_values(obs.dataset.ssh, model_timeseries.dataset.ssh) Tidegauge object at 0x562173632980 initialised Tidegauge object at 0x562173632980 initialised Although we input data arrays to the above routine, it returns two new Tidegauge objects. Now you have equivalent and comparable sets of time series that can be easily compared.\nHarmonic Analysis \u0026 Non tidal-Residuals The Tidegauge object contains some routines which make harmonic analysis and the calculation/comparison of non-tidal residuals easier. Harmonic analysis is done using the utide package. Please see here for more info.\nFirst we can use the TidegaugeAnalysis class to do a harmonic analysis. Suppose we have two Tidegauge objects called obs and model_timeseries generated from tidegauge observations and model simulations respectively.\nThen subtract means from all the time series\n# Subtract means from all time series obs_new = tganalysis.demean_timeseries(obs_new.dataset) model_new = tganalysis.demean_timeseries(model_new.dataset) # Now you have equivalent and comparable sets of time series that can be # easily compared. Tidegauge object at 0x562173632980 initialised Tidegauge object at 0x562173632980 initialised plt.figure() plt.plot( model_new.dataset.time, model_new.dataset.ssh.isel(id_dim=stn_id), label='model - demeaned', color='orange' ) plt.plot( obs_new.dataset.time, obs_new.dataset.ssh.isel(id_dim=stn_id) , label='obs - demeaned', color='blue' ) plt.title(f'model and observed timeseries : {obs_new.dataset.site_name.isel(id_dim=stn_id).coords}') plt.legend() plt.xticks(rotation=45) plt.show() Then we can apply the harmonic analysis (though the example data is too short for this example to be that meaningful and the model data is only saved as 5-day means! Nevertheless we proceed):\nCalculate non tidal residuals First, do a harmonic analysis. This routine uses utide\nha_mod = tganalysis.harmonic_analysis_utide(model_new.dataset.ssh, min_datapoints=1) ha_obs = tganalysis.harmonic_analysis_utide(obs_new.dataset.ssh, min_datapoints=1) solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. The harmonic_analysis_utide routine returns a list of utide structure object, one for each id_dim in the Tidegauge object. It can be passed any of the arguments that go to utide. It also has an additional argument min_datapoints which determines a minimum number of data points for the harmonics analysis. If a tidegauge id_dim has less than this number, it will not return an analysis.\nNow, create new TidegaugeMultiple objects containing reconstructed tides:\ntide_mod = tganalysis.reconstruct_tide_utide(model_new.dataset.time, ha_mod) tide_obs = tganalysis.reconstruct_tide_utide(obs_new.dataset.time, ha_obs) prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. Tidegauge object at 0x562173632980 initialised prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. Tidegauge object at 0x562173632980 initialised Get new TidegaugeMultiple objects containing non tidal residuals:\nntr_mod = tganalysis.calculate_non_tidal_residuals(model_new.dataset.ssh, tide_mod.dataset.reconstructed, apply_filter=False) ntr_obs = tganalysis.calculate_non_tidal_residuals(obs_new.dataset.ssh, tide_obs.dataset.reconstructed, apply_filter=True, window_length=10, polyorder=2) # Take a look ntr_obs.dataset Tidegauge object at 0x562173632980 initialised Tidegauge object at 0x562173632980 initialised \u003cxarray.Dataset\u003e Dimensions: (t_dim: 720, id_dim: 61) Coordinates: time (t_dim) datetime64[ns] 2007-01-01 … 2007-01-30T23:00:00 longitude (id_dim) float64 1.292 -3.975 -5.158 -5.051 … 7.567 350.8 8.717 latitude (id_dim) float64 51.95 51.57 57.9 51.71 … 58.0 51.53 53.87 site_name (id_dim) \u003cU25 'Harwich' 'Mumbles' 'Ullapool' … 'N/A' 'N/A' Dimensions without coordinates: t_dim, id_dim Data variables: ntr (id_dim, t_dim) float64 0.4182 0.4182 0.4182 … -0.02699 0.2945xarray.DatasetDimensions:t_dim: 720id_dim: 61Coordinates: (4)time(t_dim)datetime64[ns]2007-01-01 … 2007-01-30T23:00:00array(['2007-01-01T00:00:00.000000000', '2007-01-01T01:00:00.000000000', '2007-01-01T02:00:00.000000000', '2007-01-01T03:00:00.000000000', '2007-01-01T04:00:00.000000000', '2007-01-01T05:00:00.000000000', '2007-01-01T06:00:00.000000000', '2007-01-01T07:00:00.000000000', '2007-01-01T08:00:00.000000000', '2007-01-01T09:00:00.000000000', '2007-01-01T10:00:00.000000000', '2007-01-01T11:00:00.000000000', '2007-01-01T12:00:00.000000000', '2007-01-01T13:00:00.000000000', '2007-01-01T14:00:00.000000000', '2007-01-01T15:00:00.000000000', '2007-01-01T16:00:00.000000000', '2007-01-01T17:00:00.000000000', '2007-01-01T18:00:00.000000000', '2007-01-01T19:00:00.000000000', '2007-01-01T20:00:00.000000000', '2007-01-01T21:00:00.000000000', '2007-01-01T22:00:00.000000000', '2007-01-01T23:00:00.000000000', '2007-01-02T00:00:00.000000000', '2007-01-02T01:00:00.000000000', '2007-01-02T02:00:00.000000000', '2007-01-02T03:00:00.000000000', '2007-01-02T04:00:00.000000000', '2007-01-02T05:00:00.000000000', '2007-01-02T06:00:00.000000000', '2007-01-02T07:00:00.000000000', '2007-01-02T08:00:00.000000000', '2007-01-02T09:00:00.000000000', '2007-01-02T10:00:00.000000000', '2007-01-02T11:00:00.000000000', '2007-01-02T12:00:00.000000000', '2007-01-02T13:00:00.000000000', '2007-01-02T14:00:00.000000000', '2007-01-02T15:00:00.000000000', … '2007-01-29T10:00:00.000000000', '2007-01-29T11:00:00.000000000', '2007-01-29T12:00:00.000000000', '2007-01-29T13:00:00.000000000', '2007-01-29T14:00:00.000000000', '2007-01-29T15:00:00.000000000', '2007-01-29T16:00:00.000000000', '2007-01-29T17:00:00.000000000', '2007-01-29T18:00:00.000000000', '2007-01-29T19:00:00.000000000', '2007-01-29T20:00:00.000000000', '2007-01-29T21:00:00.000000000', '2007-01-29T22:00:00.000000000', '2007-01-29T23:00:00.000000000', '2007-01-30T00:00:00.000000000', '2007-01-30T01:00:00.000000000', '2007-01-30T02:00:00.000000000', '2007-01-30T03:00:00.000000000', '2007-01-30T04:00:00.000000000', '2007-01-30T05:00:00.000000000', '2007-01-30T06:00:00.000000000', '2007-01-30T07:00:00.000000000', '2007-01-30T08:00:00.000000000', '2007-01-30T09:00:00.000000000', '2007-01-30T10:00:00.000000000', '2007-01-30T11:00:00.000000000', '2007-01-30T12:00:00.000000000', '2007-01-30T13:00:00.000000000', '2007-01-30T14:00:00.000000000', '2007-01-30T15:00:00.000000000', '2007-01-30T16:00:00.000000000', '2007-01-30T17:00:00.000000000', '2007-01-30T18:00:00.000000000', '2007-01-30T19:00:00.000000000', '2007-01-30T20:00:00.000000000', '2007-01-30T21:00:00.000000000', '2007-01-30T22:00:00.000000000', '2007-01-30T23:00:00.000000000'], dtype='datetime64[ns]')longitude(id_dim)float641.292 -3.975 -5.158 … 350.8 8.717array([ 1.29210000e+00, -3.97544000e+00, -5.15789000e+00, -5.05148000e+00, -2.92042000e+00, -3.13433000e+00, -5.05036000e+00, -3.08631000e+00, -1.43978000e+00, 1.34839000e+00, -4.62044000e+00, 1.30164000e+00, -6.06422000e+00, -2.44794000e+00, -3.82522000e+00, -4.18525000e+00, 7.43440000e-01, -6.14170000e-01, -5.54283000e+00, -5.12003000e+00, -1.43978000e+00, -2.08013000e+00, -1.14031000e+00, -2.98744000e+00, -1.87486000e+00, -6.31642000e+00, -3.01800000e+00, -6.19006000e+00, -1.86030000e-01, 5.70300000e-02, -4.00220000e+00, 1.32267000e+00, -2.71497000e+00, -2.11667000e+00, -1.43978000e+00, -3.56764000e+00, -1.43978000e+00, -2.72848000e+00, -3.18169000e+00, 1.75083000e+00, -4.98333000e+00, -5.66947000e+00, -3.13433000e+00, -1.11175000e+00, -6.38889000e+00, -4.90583000e+00, -4.76806000e+00, -6.65683000e+00, -2.72848000e+00, -1.43978000e+00, -4.04517000e+00, -4.11094000e+00, -1.43978000e+00, 1.12150002e+01, 1.18000002e+01, 5.11700010e+00, 1.12500000e+01, 3.52666992e+02, 7.56699991e+00, 3.50816986e+02, 8.71700001e+00])latitude(id_dim)float6451.95 51.57 57.9 … 51.53 53.87array([51.94798 , 51.57 , 57.89525 , 51.7064 , 54.03167 , 51.21525 , 58.45661 , 58.44097 , 55.00744 , 51.95675 , 53.31394 , 52.93436 , 56.62311 , 50.6085 , 53.33167 , 50.36839 , 51.44564 , 54.49008 , 50.103 , 54.84256 , 55.00744 , 57.14406 , 60.15403 , 51.55 , 50.71433 , 49.91847 , 53.44969 , 55.62742 , 53.63103 , 50.78178 , 57.5987 , 51.11439 , 51.51089 , 49.18333 , 55.00744 , 54.65081 , 55.00744 , 51.50002 , 55.98983 , 52.473 , 52.01378 , 54.66475 , 51.21525 , 50.80256 , 58.20711 , 55.74964 , 54.08539 , 55.20678 , 51.50002 , 55.00744 , 52.71906 , 51.21097 , 55.00744 , 58.34999847, 57.68299866, 61.93299866, 64.86699677, 55.36700058, 58. , 51.53300095, 53.86700058])site_name(id_dim)\u003cU25'Harwich' 'Mumbles' … 'N/A' 'N/A'array(['Harwich', 'Mumbles', 'Ullapool', 'Milford Haven', 'Heysham', 'Hinkley Point', 'Kinlochbervie', 'Wick', 'North Shields', 'Felixstowe', 'Holyhead', 'Cromer', 'Tobermory', 'Weymouth', 'Llandudno', 'Devonport', 'Sheerness', 'Whitby', 'Newlyn', 'Portpatrick', 'North Shields', 'Aberdeen', 'Lerwick', 'Newport', 'Bournemouth', \"St. Mary's\", 'Liverpool, Gladstone Dock', 'Port Ellen (Islay)', 'Immingham', 'Newhaven', 'Moray Firth', 'Dover', 'Avonmouth', 'St. Helier (Jersey)', 'North Shields', 'Workington', 'North Shields', 'Portbury', 'Leith', 'Lowestoft', 'Fishguard', 'Bangor', 'Hinkley Point', 'Portsmouth', 'Stornoway', 'Millport', 'Port Erin', 'Portrush', 'Portbury', 'North Shields', 'Barmouth', 'Ilfracombe', 'North Shields', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A'], dtype='\u003cU25')Data variables: (1)ntr(id_dim, t_dim)float640.4182 0.4182 … -0.02699 0.2945array([[ 0.41824902, 0.41824902, 0.41824902, …, -0.34442645, -0.14399667, 0.11602685], [ 0.21228445, 0.21228445, 0.21228445, …, 0.11863819, 0.11863819, 0.11863819], [ 0.1459287 , 0.1459287 , 0.1459287 , …, 0.1846614 , 0.2464215 , 0.31395693], …, [-0.17406373, -0.17406373, -0.17406373, …, -0.1497518 , -0.18778463, -0.23589221], [-0.09974818, -0.09974818, -0.09974818, …, -0.00720227, -0.07338597, -0.16374877], [ 0.75580794, 0.75580794, 0.75580794, …, -0.2887721 , -0.0269909 , 0.29453278]])Indexes: (0)Attributes: (0)\nThe dataset structure is preserved and has created a new variable called ntr - non-tidal residual.\nBy default, this routines will apply scipy.signal.savgol_filter to the non-tidal residuals to remove some noise. This can be switched off using apply_filter = False.\nThe Doodson X0 filter is an alternative way of estimating non-tidal residuals. This will return a new Tidegauge() object containing filtered ssh data:\ndx0 = tganalysis.doodson_x0_filter(obs_new.dataset, \"ssh\") # take a look dx0.dataset Tidegauge object at 0x562173632980 initialised \u003cxarray.Dataset\u003e Dimensions: (t_dim: 720, id_dim: 61) Coordinates: longitude (id_dim) float64 1.292 -3.975 -5.158 -5.051 … 7.567 350.8 8.717 latitude (id_dim) float64 51.95 51.57 57.9 51.71 … 58.0 51.53 53.87 site_name (id_dim) \u003cU25 'Harwich' 'Mumbles' 'Ullapool' … 'N/A' 'N/A' Dimensions without coordinates: t_dim, id_dim Data variables: time (t_dim) datetime64[ns] 2007-01-01 … 2007-01-30T23:00:00 ssh (id_dim, t_dim) float64 nan nan nan nan nan … nan nan nan nanxarray.DatasetDimensions:t_dim: 720id_dim: 61Coordinates: (3)longitude(id_dim)float641.292 -3.975 -5.158 … 350.8 8.717array([ 1.29210000e+00, -3.97544000e+00, -5.15789000e+00, -5.05148000e+00, -2.92042000e+00, -3.13433000e+00, -5.05036000e+00, -3.08631000e+00, -1.43978000e+00, 1.34839000e+00, -4.62044000e+00, 1.30164000e+00, -6.06422000e+00, -2.44794000e+00, -3.82522000e+00, -4.18525000e+00, 7.43440000e-01, -6.14170000e-01, -5.54283000e+00, -5.12003000e+00, -1.43978000e+00, -2.08013000e+00, -1.14031000e+00, -2.98744000e+00, -1.87486000e+00, -6.31642000e+00, -3.01800000e+00, -6.19006000e+00, -1.86030000e-01, 5.70300000e-02, -4.00220000e+00, 1.32267000e+00, -2.71497000e+00, -2.11667000e+00, -1.43978000e+00, -3.56764000e+00, -1.43978000e+00, -2.72848000e+00, -3.18169000e+00, 1.75083000e+00, -4.98333000e+00, -5.66947000e+00, -3.13433000e+00, -1.11175000e+00, -6.38889000e+00, -4.90583000e+00, -4.76806000e+00, -6.65683000e+00, -2.72848000e+00, -1.43978000e+00, -4.04517000e+00, -4.11094000e+00, -1.43978000e+00, 1.12150002e+01, 1.18000002e+01, 5.11700010e+00, 1.12500000e+01, 3.52666992e+02, 7.56699991e+00, 3.50816986e+02, 8.71700001e+00])latitude(id_dim)float6451.95 51.57 57.9 … 51.53 53.87array([51.94798 , 51.57 , 57.89525 , 51.7064 , 54.03167 , 51.21525 , 58.45661 , 58.44097 , 55.00744 , 51.95675 , 53.31394 , 52.93436 , 56.62311 , 50.6085 , 53.33167 , 50.36839 , 51.44564 , 54.49008 , 50.103 , 54.84256 , 55.00744 , 57.14406 , 60.15403 , 51.55 , 50.71433 , 49.91847 , 53.44969 , 55.62742 , 53.63103 , 50.78178 , 57.5987 , 51.11439 , 51.51089 , 49.18333 , 55.00744 , 54.65081 , 55.00744 , 51.50002 , 55.98983 , 52.473 , 52.01378 , 54.66475 , 51.21525 , 50.80256 , 58.20711 , 55.74964 , 54.08539 , 55.20678 , 51.50002 , 55.00744 , 52.71906 , 51.21097 , 55.00744 , 58.34999847, 57.68299866, 61.93299866, 64.86699677, 55.36700058, 58. , 51.53300095, 53.86700058])site_name(id_dim)\u003cU25'Harwich' 'Mumbles' … 'N/A' 'N/A'array(['Harwich', 'Mumbles', 'Ullapool', 'Milford Haven', 'Heysham', 'Hinkley Point', 'Kinlochbervie', 'Wick', 'North Shields', 'Felixstowe', 'Holyhead', 'Cromer', 'Tobermory', 'Weymouth', 'Llandudno', 'Devonport', 'Sheerness', 'Whitby', 'Newlyn', 'Portpatrick', 'North Shields', 'Aberdeen', 'Lerwick', 'Newport', 'Bournemouth', \"St. Mary's\", 'Liverpool, Gladstone Dock', 'Port Ellen (Islay)', 'Immingham', 'Newhaven', 'Moray Firth', 'Dover', 'Avonmouth', 'St. Helier (Jersey)', 'North Shields', 'Workington', 'North Shields', 'Portbury', 'Leith', 'Lowestoft', 'Fishguard', 'Bangor', 'Hinkley Point', 'Portsmouth', 'Stornoway', 'Millport', 'Port Erin', 'Portrush', 'Portbury', 'North Shields', 'Barmouth', 'Ilfracombe', 'North Shields', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A'], dtype='\u003cU25')Data variables: (2)time(t_dim)datetime64[ns]2007-01-01 … 2007-01-30T23:00:00array(['2007-01-01T00:00:00.000000000', '2007-01-01T01:00:00.000000000', '2007-01-01T02:00:00.000000000', '2007-01-01T03:00:00.000000000', '2007-01-01T04:00:00.000000000', '2007-01-01T05:00:00.000000000', '2007-01-01T06:00:00.000000000', '2007-01-01T07:00:00.000000000', '2007-01-01T08:00:00.000000000', '2007-01-01T09:00:00.000000000', '2007-01-01T10:00:00.000000000', '2007-01-01T11:00:00.000000000', '2007-01-01T12:00:00.000000000', '2007-01-01T13:00:00.000000000', '2007-01-01T14:00:00.000000000', '2007-01-01T15:00:00.000000000', '2007-01-01T16:00:00.000000000', '2007-01-01T17:00:00.000000000', '2007-01-01T18:00:00.000000000', '2007-01-01T19:00:00.000000000', '2007-01-01T20:00:00.000000000', '2007-01-01T21:00:00.000000000', '2007-01-01T22:00:00.000000000', '2007-01-01T23:00:00.000000000', '2007-01-02T00:00:00.000000000', '2007-01-02T01:00:00.000000000', '2007-01-02T02:00:00.000000000', '2007-01-02T03:00:00.000000000', '2007-01-02T04:00:00.000000000', '2007-01-02T05:00:00.000000000', '2007-01-02T06:00:00.000000000', '2007-01-02T07:00:00.000000000', '2007-01-02T08:00:00.000000000', '2007-01-02T09:00:00.000000000', '2007-01-02T10:00:00.000000000', '2007-01-02T11:00:00.000000000', '2007-01-02T12:00:00.000000000', '2007-01-02T13:00:00.000000000', '2007-01-02T14:00:00.000000000', '2007-01-02T15:00:00.000000000', … '2007-01-29T10:00:00.000000000', '2007-01-29T11:00:00.000000000', '2007-01-29T12:00:00.000000000', '2007-01-29T13:00:00.000000000', '2007-01-29T14:00:00.000000000', '2007-01-29T15:00:00.000000000', '2007-01-29T16:00:00.000000000', '2007-01-29T17:00:00.000000000', '2007-01-29T18:00:00.000000000', '2007-01-29T19:00:00.000000000', '2007-01-29T20:00:00.000000000', '2007-01-29T21:00:00.000000000', '2007-01-29T22:00:00.000000000', '2007-01-29T23:00:00.000000000', '2007-01-30T00:00:00.000000000', '2007-01-30T01:00:00.000000000', '2007-01-30T02:00:00.000000000', '2007-01-30T03:00:00.000000000', '2007-01-30T04:00:00.000000000', '2007-01-30T05:00:00.000000000', '2007-01-30T06:00:00.000000000', '2007-01-30T07:00:00.000000000', '2007-01-30T08:00:00.000000000', '2007-01-30T09:00:00.000000000', '2007-01-30T10:00:00.000000000', '2007-01-30T11:00:00.000000000', '2007-01-30T12:00:00.000000000', '2007-01-30T13:00:00.000000000', '2007-01-30T14:00:00.000000000', '2007-01-30T15:00:00.000000000', '2007-01-30T16:00:00.000000000', '2007-01-30T17:00:00.000000000', '2007-01-30T18:00:00.000000000', '2007-01-30T19:00:00.000000000', '2007-01-30T20:00:00.000000000', '2007-01-30T21:00:00.000000000', '2007-01-30T22:00:00.000000000', '2007-01-30T23:00:00.000000000'], dtype='datetime64[ns]')ssh(id_dim, t_dim)float64nan nan nan nan … nan nan nan nanarray([[nan, nan, nan, …, nan, nan, nan], [nan, nan, nan, …, nan, nan, nan], [nan, nan, nan, …, nan, nan, nan], …, [nan, nan, nan, …, nan, nan, nan], [nan, nan, nan, …, nan, nan, nan], [nan, nan, nan, …, nan, nan, nan]])Indexes: (0)Attributes: (0)\nWe can compare these analyses e.g. the observed timeseries against the harmonic reconstruction. Noting we can in principle extend the harmoninc reconstruction beyond the observation time window.\nplt.figure() plt.plot( tide_obs.dataset.time, tide_obs.dataset.reconstructed.isel(id_dim=stn_id), label='reconstructed from harmonics', color='orange' ) plt.plot( obs_new.dataset.time, obs_new.dataset.ssh.isel(id_dim=stn_id) , label='observed', color='blue' ) plt.legend() plt.xticks(rotation=45) plt.show() We can also look closer at the difference between the observed timeseries and the harmonic reconstruction, that is the non-tidal residual. And we can compare the observed and modelled non-harmonic residual and contrast the different methods of removing the tides. Here we contrast using a harmonic analysis (and creating a “non-tidal residual”) with the DoodsonX0 filter. Note that the timeseries is far too short for a sensible analysis and that this is really a demonstration of concept.\nplt.figure() plt.subplot(2,1,1) plt.plot( dx0.dataset.time, dx0.isel(id_dim=stn_id).dataset.ssh, label='obs: doodsonX0 filtered', color='orange' ) plt.plot( ntr_obs.dataset.time, ntr_obs.isel(id_dim=stn_id).dataset.ntr, label='obs: non-tidal residual', color='blue' ) plt.title('analysis comparison: non-tidal residual vs doodsonX0') plt.legend() plt.xticks(rotation=45) plt.subplot(2,1,2) plt.plot( ntr_mod.dataset.time, ntr_mod.isel(id_dim=stn_id).dataset.ntr, label='model: non-tidal residual', color='g' ) plt.plot( ntr_obs.dataset.time, ntr_obs.isel(id_dim=stn_id).dataset.ntr, label='obs: non-tidal residual', color='blue' ) plt.title('model vs observation: non-tidal residual') plt.legend() plt.xticks(rotation=45) plt.tight_layout() Threshold Statistics for non-tidal residuals This is a simple extreme value analysis of whatever data you use. It will count the number of peaks and the total time spent over each threshold provided. It will also count the numbers of daily and monthly maxima over each threshold. To this, a Tidegauge object and an array of thresholds (in metres) should be passed. The method return peak_count_*, time_over_threshold_*, dailymax_count_*, monthlymax_count_*:\nthresh_mod = tganalysis.threshold_statistics(ntr_mod.dataset, thresholds=np.arange(-2, 2, 0.1)) thresh_obs = tganalysis.threshold_statistics(ntr_obs.dataset, thresholds=np.arange(-2, 2, 0.1)) # Have a look thresh_obs \u003cxarray.Dataset\u003e Dimensions: (t_dim: 720, id_dim: 61, threshold: 40) Coordinates: longitude (id_dim) float64 1.292 -3.975 … 350.8 8.717 latitude (id_dim) float64 51.95 51.57 57.9 … 51.53 53.87 site_name (id_dim) \u003cU25 'Harwich' 'Mumbles' … 'N/A' 'N/A'\nthreshold (threshold) float64 -2.0 -1.9 -1.8 … 1.7 1.8 1.9 Dimensions without coordinates: t_dim, id_dim Data variables: time (t_dim) datetime64[ns] 2007-01-01 … 2007-01-30… peak_count_ntr (id_dim, threshold) float64 40.0 40.0 … 2.0 1.0 time_over_threshold_ntr (id_dim, threshold) float64 720.0 720.0 … 5.0 4.0 dailymax_count_ntr (id_dim, threshold) float64 30.0 30.0 … 2.0 1.0 monthlymax_count_ntr (id_dim, threshold) float64 1.0 1.0 1.0 … 1.0 1.0xarray.DatasetDimensions:t_dim: 720id_dim: 61threshold: 40Coordinates: (4)longitude(id_dim)float641.292 -3.975 -5.158 … 350.8 8.717array([ 1.29210000e+00, -3.97544000e+00, -5.15789000e+00, -5.05148000e+00, -2.92042000e+00, -3.13433000e+00, -5.05036000e+00, -3.08631000e+00, -1.43978000e+00, 1.34839000e+00, -4.62044000e+00, 1.30164000e+00, -6.06422000e+00, -2.44794000e+00, -3.82522000e+00, -4.18525000e+00, 7.43440000e-01, -6.14170000e-01, -5.54283000e+00, -5.12003000e+00, -1.43978000e+00, -2.08013000e+00, -1.14031000e+00, -2.98744000e+00, -1.87486000e+00, -6.31642000e+00, -3.01800000e+00, -6.19006000e+00, -1.86030000e-01, 5.70300000e-02, -4.00220000e+00, 1.32267000e+00, -2.71497000e+00, -2.11667000e+00, -1.43978000e+00, -3.56764000e+00, -1.43978000e+00, -2.72848000e+00, -3.18169000e+00, 1.75083000e+00, -4.98333000e+00, -5.66947000e+00, -3.13433000e+00, -1.11175000e+00, -6.38889000e+00, -4.90583000e+00, -4.76806000e+00, -6.65683000e+00, -2.72848000e+00, -1.43978000e+00, -4.04517000e+00, -4.11094000e+00, -1.43978000e+00, 1.12150002e+01, 1.18000002e+01, 5.11700010e+00, 1.12500000e+01, 3.52666992e+02, 7.56699991e+00, 3.50816986e+02, 8.71700001e+00])latitude(id_dim)float6451.95 51.57 57.9 … 51.53 53.87array([51.94798 , 51.57 , 57.89525 , 51.7064 , 54.03167 , 51.21525 , 58.45661 , 58.44097 , 55.00744 , 51.95675 , 53.31394 , 52.93436 , 56.62311 , 50.6085 , 53.33167 , 50.36839 , 51.44564 , 54.49008 , 50.103 , 54.84256 , 55.00744 , 57.14406 , 60.15403 , 51.55 , 50.71433 , 49.91847 , 53.44969 , 55.62742 , 53.63103 , 50.78178 , 57.5987 , 51.11439 , 51.51089 , 49.18333 , 55.00744 , 54.65081 , 55.00744 , 51.50002 , 55.98983 , 52.473 , 52.01378 , 54.66475 , 51.21525 , 50.80256 , 58.20711 , 55.74964 , 54.08539 , 55.20678 , 51.50002 , 55.00744 , 52.71906 , 51.21097 , 55.00744 , 58.34999847, 57.68299866, 61.93299866, 64.86699677, 55.36700058, 58. , 51.53300095, 53.86700058])site_name(id_dim)\u003cU25'Harwich' 'Mumbles' … 'N/A' 'N/A'array(['Harwich', 'Mumbles', 'Ullapool', 'Milford Haven', 'Heysham', 'Hinkley Point', 'Kinlochbervie', 'Wick', 'North Shields', 'Felixstowe', 'Holyhead', 'Cromer', 'Tobermory', 'Weymouth', 'Llandudno', 'Devonport', 'Sheerness', 'Whitby', 'Newlyn', 'Portpatrick', 'North Shields', 'Aberdeen', 'Lerwick', 'Newport', 'Bournemouth', \"St. Mary's\", 'Liverpool, Gladstone Dock', 'Port Ellen (Islay)', 'Immingham', 'Newhaven', 'Moray Firth', 'Dover', 'Avonmouth', 'St. Helier (Jersey)', 'North Shields', 'Workington', 'North Shields', 'Portbury', 'Leith', 'Lowestoft', 'Fishguard', 'Bangor', 'Hinkley Point', 'Portsmouth', 'Stornoway', 'Millport', 'Port Erin', 'Portrush', 'Portbury', 'North Shields', 'Barmouth', 'Ilfracombe', 'North Shields', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A'], dtype='\u003cU25')threshold(threshold)float64-2.0 -1.9 -1.8 -1.7 … 1.7 1.8 1.9array([-2.000000e+00, -1.900000e+00, -1.800000e+00, -1.700000e+00, -1.600000e+00, -1.500000e+00, -1.400000e+00, -1.300000e+00, -1.200000e+00, -1.100000e+00, -1.000000e+00, -9.000000e-01, -8.000000e-01, -7.000000e-01, -6.000000e-01, -5.000000e-01, -4.000000e-01, -3.000000e-01, -2.000000e-01, -1.000000e-01, 1.776357e-15, 1.000000e-01, 2.000000e-01, 3.000000e-01, 4.000000e-01, 5.000000e-01, 6.000000e-01, 7.000000e-01, 8.000000e-01, 9.000000e-01, 1.000000e+00, 1.100000e+00, 1.200000e+00, 1.300000e+00, 1.400000e+00, 1.500000e+00, 1.600000e+00, 1.700000e+00, 1.800000e+00, 1.900000e+00])Data variables: (5)time(t_dim)datetime64[ns]2007-01-01 … 2007-01-30T23:00:00array(['2007-01-01T00:00:00.000000000', '2007-01-01T01:00:00.000000000', '2007-01-01T02:00:00.000000000', '2007-01-01T03:00:00.000000000', '2007-01-01T04:00:00.000000000', '2007-01-01T05:00:00.000000000', '2007-01-01T06:00:00.000000000', '2007-01-01T07:00:00.000000000', '2007-01-01T08:00:00.000000000', '2007-01-01T09:00:00.000000000', '2007-01-01T10:00:00.000000000', '2007-01-01T11:00:00.000000000', '2007-01-01T12:00:00.000000000', '2007-01-01T13:00:00.000000000', '2007-01-01T14:00:00.000000000', '2007-01-01T15:00:00.000000000', '2007-01-01T16:00:00.000000000', '2007-01-01T17:00:00.000000000', '2007-01-01T18:00:00.000000000', '2007-01-01T19:00:00.000000000', '2007-01-01T20:00:00.000000000', '2007-01-01T21:00:00.000000000', '2007-01-01T22:00:00.000000000', '2007-01-01T23:00:00.000000000', '2007-01-02T00:00:00.000000000', '2007-01-02T01:00:00.000000000', '2007-01-02T02:00:00.000000000', '2007-01-02T03:00:00.000000000', '2007-01-02T04:00:00.000000000', '2007-01-02T05:00:00.000000000', '2007-01-02T06:00:00.000000000', '2007-01-02T07:00:00.000000000', '2007-01-02T08:00:00.000000000', '2007-01-02T09:00:00.000000000', '2007-01-02T10:00:00.000000000', '2007-01-02T11:00:00.000000000', '2007-01-02T12:00:00.000000000', '2007-01-02T13:00:00.000000000', '2007-01-02T14:00:00.000000000', '2007-01-02T15:00:00.000000000', … '2007-01-29T10:00:00.000000000', '2007-01-29T11:00:00.000000000', '2007-01-29T12:00:00.000000000', '2007-01-29T13:00:00.000000000', '2007-01-29T14:00:00.000000000', '2007-01-29T15:00:00.000000000', '2007-01-29T16:00:00.000000000', '2007-01-29T17:00:00.000000000', '2007-01-29T18:00:00.000000000', '2007-01-29T19:00:00.000000000', '2007-01-29T20:00:00.000000000', '2007-01-29T21:00:00.000000000', '2007-01-29T22:00:00.000000000', '2007-01-29T23:00:00.000000000', '2007-01-30T00:00:00.000000000', '2007-01-30T01:00:00.000000000', '2007-01-30T02:00:00.000000000', '2007-01-30T03:00:00.000000000', '2007-01-30T04:00:00.000000000', '2007-01-30T05:00:00.000000000', '2007-01-30T06:00:00.000000000', '2007-01-30T07:00:00.000000000', '2007-01-30T08:00:00.000000000', '2007-01-30T09:00:00.000000000', '2007-01-30T10:00:00.000000000', '2007-01-30T11:00:00.000000000', '2007-01-30T12:00:00.000000000', '2007-01-30T13:00:00.000000000', '2007-01-30T14:00:00.000000000', '2007-01-30T15:00:00.000000000', '2007-01-30T16:00:00.000000000', '2007-01-30T17:00:00.000000000', '2007-01-30T18:00:00.000000000', '2007-01-30T19:00:00.000000000', '2007-01-30T20:00:00.000000000', '2007-01-30T21:00:00.000000000', '2007-01-30T22:00:00.000000000', '2007-01-30T23:00:00.000000000'], dtype='datetime64[ns]')peak_count_ntr(id_dim, threshold)float6440.0 40.0 40.0 40.0 … 2.0 2.0 1.0array([[40., 40., 40., …, 0., 0., 0.], [22., 22., 22., …, 0., 0., 0.], [35., 35., 35., …, 0., 0., 0.], …, [39., 39., 39., …, 0., 0., 0.], [42., 42., 42., …, 0., 0., 0.], [38., 38., 38., …, 2., 2., 1.]])time_over_threshold_ntr(id_dim, threshold)float64720.0 720.0 720.0 … 8.0 5.0 4.0array([[720., 720., 720., …, 0., 0., 0.], [720., 720., 720., …, 0., 0., 0.], [720., 720., 720., …, 0., 0., 0.], …, [720., 720., 720., …, 0., 0., 0.], [720., 720., 720., …, 0., 0., 0.], [720., 720., 720., …, 8., 5., 4.]])dailymax_count_ntr(id_dim, threshold)float6430.0 30.0 30.0 30.0 … 3.0 2.0 1.0array([[30., 30., 30., …, 0., 0., 0.], [30., 30., 30., …, 0., 0., 0.], [30., 30., 30., …, 0., 0., 0.], …, [30., 30., 30., …, 0., 0., 0.], [30., 30., 30., …, 0., 0., 0.], [30., 30., 30., …, 3., 2., 1.]])monthlymax_count_ntr(id_dim, threshold)float641.0 1.0 1.0 1.0 … 1.0 1.0 1.0 1.0array([[1., 1., 1., …, 0., 0., 0.], [1., 1., 1., …, 0., 0., 0.], [1., 1., 1., …, 0., 0., 0.], …, [1., 1., 1., …, 0., 0., 0.], [1., 1., 1., …, 0., 0., 0.], [1., 1., 1., …, 1., 1., 1.]])Indexes: (1)thresholdPandasIndexPandasIndex(Index([ -2.0, -1.9, -1.7999999999999998, -1.6999999999999997, -1.5999999999999996, -1.4999999999999996, -1.3999999999999995, -1.2999999999999994, -1.1999999999999993, -1.0999999999999992, -0.9999999999999991, -0.899999999999999, -0.7999999999999989, -0.6999999999999988, -0.5999999999999988, -0.49999999999999867, -0.3999999999999986, -0.2999999999999985, -0.1999999999999984, -0.09999999999999831, 1.7763568394002505e-15, 0.10000000000000187, 0.20000000000000195, 0.30000000000000204, 0.40000000000000213, 0.5000000000000022, 0.6000000000000023, 0.7000000000000024, 0.8000000000000025, 0.9000000000000026, 1.0000000000000027, 1.1000000000000028, 1.2000000000000028, 1.300000000000003, 1.400000000000003, 1.500000000000003, 1.6000000000000032, 1.7000000000000033, 1.8000000000000034, 1.9000000000000035], dtype='float64', name='threshold'))Attributes: (0) plt.plot( thresh_obs.threshold, thresh_obs.peak_count_ntr.mean(dim='id_dim') ) #plt.ylim([0,15]) plt.title(f\"Observed peaks over threshold\") plt.xlabel('Analysis threshold (m)') plt.ylabel('event count') Text(0, 0.5, 'event count') Note that the non-tidal residual is a noisy timeseries (computed as a difference between two timeseries) so peaks do not necessarily correspond to peaks in total water level. For this reason time_over_threshold_* can be useful. Below we see that about 28% (y-axis) of the observations of non-tidal residual exceed 20cm (x-axis). Again, threshold statistics are more useful with a longer record.\nnormalised_event_count = 100 * thresh_obs.time_over_threshold_ntr.isel(id_dim=stn_id) / thresh_obs.time_over_threshold_ntr.isel(id_dim=stn_id).max() plt.plot( thresh_obs.threshold, normalised_event_count ) plt.ylim([20,40]) plt.xlim([0.0,0.4]) plt.title(f\"time over threshold\") plt.xlabel('Analysis threshold (m)') plt.ylabel('normalised event count (%)') plt.plot(thresh_obs.threshold[22], normalised_event_count[22], 'r+', markersize=20) [\u003cmatplotlib.lines.Line2D at 0x7f67b5df74c0\u003e] Other TidegaugeAnalysis methods Calculate errors The difference() routine will calculate differences, absolute_differences and squared differenced for all variables. Corresponding new variables are created with names diff_*, abs_diff_* and square_diff_*\nntr_diff = tganalysis.difference(ntr_obs.dataset, ntr_mod.dataset) ssh_diff = tganalysis.difference(obs_new.dataset, model_new.dataset) # Take a look ntr_diff.dataset Tidegauge object at 0x562173632980 initialised Tidegauge object at 0x562173632980 initialised \u003cxarray.Dataset\u003e Dimensions: (t_dim: 720, id_dim: 61) Coordinates:\ntime (t_dim) datetime64[ns] 2007-01-01 … 2007-01-30T23:00:00 site_name (id_dim) \u003cU25 'Harwich' 'Mumbles' … 'N/A' 'N/A' longitude (id_dim) float64 1.292 -3.975 -5.158 … 7.567 350.8 8.717 latitude (id_dim) float64 51.95 51.57 57.9 … 58.0 51.53 53.87 Dimensions without coordinates: t_dim, id_dim Data variables: diff_ntr (id_dim, t_dim) float64 nan nan nan … 0.05885 0.3435 abs_diff_ntr (id_dim, t_dim) float64 nan nan nan … 0.05885 0.3435 square_diff_ntr (id_dim, t_dim) float64 nan nan nan … 0.003464 0.118xarray.DatasetDimensions:t_dim: 720id_dim: 61Coordinates: (4)time(t_dim)datetime64[ns]2007-01-01 … 2007-01-30T23:00:00array(['2007-01-01T00:00:00.000000000', '2007-01-01T01:00:00.000000000', '2007-01-01T02:00:00.000000000', …, '2007-01-30T21:00:00.000000000', '2007-01-30T22:00:00.000000000', '2007-01-30T23:00:00.000000000'], dtype='datetime64[ns]')site_name(id_dim)\u003cU25'Harwich' 'Mumbles' … 'N/A' 'N/A'array(['Harwich', 'Mumbles', 'Ullapool', 'Milford Haven', 'Heysham', 'Hinkley Point', 'Kinlochbervie', 'Wick', 'North Shields', 'Felixstowe', 'Holyhead', 'Cromer', 'Tobermory', 'Weymouth', 'Llandudno', 'Devonport', 'Sheerness', 'Whitby', 'Newlyn', 'Portpatrick', 'North Shields', 'Aberdeen', 'Lerwick', 'Newport', 'Bournemouth', \"St. Mary's\", 'Liverpool, Gladstone Dock', 'Port Ellen (Islay)', 'Immingham', 'Newhaven', 'Moray Firth', 'Dover', 'Avonmouth', 'St. Helier (Jersey)', 'North Shields', 'Workington', 'North Shields', 'Portbury', 'Leith', 'Lowestoft', 'Fishguard', 'Bangor', 'Hinkley Point', 'Portsmouth', 'Stornoway', 'Millport', 'Port Erin', 'Portrush', 'Portbury', 'North Shields', 'Barmouth', 'Ilfracombe', 'North Shields', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A'], dtype='\u003cU25')longitude(id_dim)float641.292 -3.975 -5.158 … 350.8 8.717array([ 1.29210000e+00, -3.97544000e+00, -5.15789000e+00, -5.05148000e+00, -2.92042000e+00, -3.13433000e+00, -5.05036000e+00, -3.08631000e+00, -1.43978000e+00, 1.34839000e+00, -4.62044000e+00, 1.30164000e+00, -6.06422000e+00, -2.44794000e+00, -3.82522000e+00, -4.18525000e+00, 7.43440000e-01, -6.14170000e-01, -5.54283000e+00, -5.12003000e+00, -1.43978000e+00, -2.08013000e+00, -1.14031000e+00, -2.98744000e+00, -1.87486000e+00, -6.31642000e+00, -3.01800000e+00, -6.19006000e+00, -1.86030000e-01, 5.70300000e-02, -4.00220000e+00, 1.32267000e+00, -2.71497000e+00, -2.11667000e+00, -1.43978000e+00, -3.56764000e+00, -1.43978000e+00, -2.72848000e+00, -3.18169000e+00, 1.75083000e+00, -4.98333000e+00, -5.66947000e+00, -3.13433000e+00, -1.11175000e+00, -6.38889000e+00, -4.90583000e+00, -4.76806000e+00, -6.65683000e+00, -2.72848000e+00, -1.43978000e+00, -4.04517000e+00, -4.11094000e+00, -1.43978000e+00, 1.12150002e+01, 1.18000002e+01, 5.11700010e+00, 1.12500000e+01, 3.52666992e+02, 7.56699991e+00, 3.50816986e+02, 8.71700001e+00])latitude(id_dim)float6451.95 51.57 57.9 … 51.53 53.87array([51.94798 , 51.57 , 57.89525 , 51.7064 , 54.03167 , 51.21525 , 58.45661 , 58.44097 , 55.00744 , 51.95675 , 53.31394 , 52.93436 , 56.62311 , 50.6085 , 53.33167 , 50.36839 , 51.44564 , 54.49008 , 50.103 , 54.84256 , 55.00744 , 57.14406 , 60.15403 , 51.55 , 50.71433 , 49.91847 , 53.44969 , 55.62742 , 53.63103 , 50.78178 , 57.5987 , 51.11439 , 51.51089 , 49.18333 , 55.00744 , 54.65081 , 55.00744 , 51.50002 , 55.98983 , 52.473 , 52.01378 , 54.66475 , 51.21525 , 50.80256 , 58.20711 , 55.74964 , 54.08539 , 55.20678 , 51.50002 , 55.00744 , 52.71906 , 51.21097 , 55.00744 , 58.34999847, 57.68299866, 61.93299866, 64.86699677, 55.36700058, 58. , 51.53300095, 53.86700058])Data variables: (3)diff_ntr(id_dim, t_dim)float64nan nan nan … 0.05885 0.3435array([[ nan, nan, nan, …, -0.32699312, -0.12564265, 0.13424107], [ nan, nan, nan, …, nan, nan, nan], [ nan, nan, nan, …, 0.27731735, 0.3287257 , 0.39544093], …, [ nan, nan, nan, …, -0.13998686, -0.18434602, -0.23884966], [ nan, nan, nan, …, 0.09161719, 0.03131287, -0.05355329], [ nan, nan, nan, …, -0.16968976, 0.05885439, 0.34348952]])abs_diff_ntr(id_dim, t_dim)float64nan nan nan … 0.05885 0.3435array([[ nan, nan, nan, …, 0.32699312, 0.12564265, 0.13424107], [ nan, nan, nan, …, nan, nan, nan], [ nan, nan, nan, …, 0.27731735, 0.3287257 , 0.39544093], …, [ nan, nan, nan, …, 0.13998686, 0.18434602, 0.23884966], [ nan, nan, nan, …, 0.09161719, 0.03131287, 0.05355329], [ nan, nan, nan, …, 0.16968976, 0.05885439, 0.34348952]])square_diff_ntr(id_dim, t_dim)float64nan nan nan … 0.003464 0.118array([[ nan, nan, nan, …, 0.1069245 , 0.01578608, 0.01802066], [ nan, nan, nan, …, nan, nan, nan], [ nan, nan, nan, …, 0.07690491, 0.10806059, 0.15637353], …, [ nan, nan, nan, …, 0.01959632, 0.03398346, 0.05704916], [ nan, nan, nan, …, 0.00839371, 0.0009805 , 0.00286796], [ nan, nan, nan, …, 0.02879461, 0.00346384, 0.11798505]])Indexes: (1)timePandasIndexPandasIndex(DatetimeIndex(['2007-01-01 00:00:00', '2007-01-01 01:00:00', '2007-01-01 02:00:00', '2007-01-01 03:00:00', '2007-01-01 04:00:00', '2007-01-01 05:00:00', '2007-01-01 06:00:00', '2007-01-01 07:00:00', '2007-01-01 08:00:00', '2007-01-01 09:00:00', … '2007-01-30 14:00:00', '2007-01-30 15:00:00', '2007-01-30 16:00:00', '2007-01-30 17:00:00', '2007-01-30 18:00:00', '2007-01-30 19:00:00', '2007-01-30 20:00:00', '2007-01-30 21:00:00', '2007-01-30 22:00:00', '2007-01-30 23:00:00'], dtype='datetime64[ns]', name='time', length=720, freq=None))Attributes: (0) We can then easily get mean errors, MAE and MSE\nmean_stats = ntr_diff.dataset.mean(dim=\"t_dim\", skipna=True) ","categories":"","description":"Tidegauge validation tutorial example.\n","excerpt":"Tidegauge validation tutorial example.\n","ref":"/COAsT/docs/examples/notebooks/tidegauge/tidegauge_validation_tutorial/","tags":"","title":"Tidegauge validation tutorial"},{"body":"Tutorial for processing tabulated tide gauge data. Tidal highs and lows can be scraped from a website such as:\nhttps://www.ntslf.org/tides/tidepred?port=Liverpool\nand format them into a csv file:\nLIVERPOOL (GLADSTONE DOCK) TZ: UT(GMT)/BST Units: METRES Datum: Chart Datum\n01/10/2020 06:29 1.65\n01/10/2020 11:54 9.01\n01/10/2020 18:36 1.87\nThe data can be used in the following demonstration.\nimport coast import numpy as np /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pydap/lib.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.responses')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.handlers')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.tests')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages Load and plot High and Low Water data.\nprint(\"load and plot HLW data\") filnam = \"./example_files/Gladstone_2020-10_HLW.txt\" load and plot HLW data Set the start and end dates.\ndate_start = np.datetime64(\"2020-10-12 23:59\") date_end = np.datetime64(\"2020-10-14 00:01\") Initiate a TideGauge object, if a filename is passed it assumes it is a GESLA type object.\ntg = coast.Tidegauge() Tidegauge object at 0x56422f0cb980 initialised Specify the data read as a High Low Water dataset.\ntg.read_hlw(filnam, date_start, date_end) Show dataset. If timezone is specified then it is presented as requested, otherwise uses UTC.\nprint(\"Try the TideGauge.show() method:\") tg.show(timezone=\"Europe/London\") Try the TideGauge.show() method: Do a basic plot of these points.\ntg.dataset.plot.scatter(x=\"time\", y=\"ssh\") \u003cmatplotlib.collections.PathCollection at 0x7f4ca6607bb0\u003e There is a method to locate HLW events around an approximate date and time. First state the time of interest.\ntime_guess = np.datetime64(\"2020-10-13 12:48\") Then recover all the HLW events in a +/- window, of specified size (iteger hrs). The default winsize = 2 (hrs).\nHLW = tg.get_tide_table_times(np.datetime64(\"2020-10-13 12:48\"), method=\"window\", winsize=24) Alternatively recover the closest HLW event to the input timestamp.\nHLW = tg.get_tide_table_times(np.datetime64(\"2020-10-13 12:48\"), method=\"nearest_1\") Or the nearest two events to the input timestamp.\nHLW = tg.get_tide_table_times(np.datetime64(\"2020-10-13 12:48\"), method=\"nearest_2\") Extract the Low Tide value.\nprint(\"Try the TideGauge.get_tidetabletimes() methods:\") print(\"LT:\", HLW[np.argmin(HLW.data)].values, \"m at\", HLW[np.argmin(HLW.data)].time.values) Try the TideGauge.get_tidetabletimes() methods: LT: 2.83 m at 2020-10-13T14:36:00.000000000 Extract the High Tide value.\nprint(\"HT:\", HLW[np.argmax(HLW.data)].values, \"m at\", HLW[np.argmax(HLW.data)].time.values) HT: 8.01 m at 2020-10-13T07:59:00.000000000 Or use the the nearest High Tide method to get High Tide.\nHT = tg.get_tide_table_times(np.datetime64(\"2020-10-13 12:48\"), method=\"nearest_HW\") print(\"HT:\", HT.values, \"m at\", HT.time.values) HT: 8.01 m at 2020-10-13T07:59:00.000000000 The get_tidetabletimes() method can take extra paremeters such as a window size, an integer number of hours to seek either side of the guess.\nHLW = tg.get_tide_table_times(np.datetime64(\"2020-10-13 12:48\"), winsize=2, method=\"nearest_1\") HLW = tg.get_tide_table_times(np.datetime64(\"2020-10-13 12:48\"), winsize=1, method=\"nearest_1\") ","categories":"","description":"Tidetable tutorial example.\n","excerpt":"Tidetable tutorial example.\n","ref":"/COAsT/docs/examples/notebooks/tidegauge/tidetable_tutorial/","tags":"","title":"Tidetable tutorial"},{"body":"This is a demonstration script for using the Transect class in the COAsT package. This object has strict data formatting requirements, which are outlined in tranect.py.\nTransect subsetting (a vertical slice of data between two coordinates): Creating them and performing some custom diagnostics with them.\nIn this tutorial we take a look at subsetting the model data along a transect (a custom straight line) and creating some bespoke diagnostics along it. We look at:\n1. Creating a TRANSECT object, defined between two points. 2. Plotting data along a transect. 3. Calculating flow normal to the transect Import relevant packages import coast import matplotlib.pyplot as plt /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pydap/lib.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.responses')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.handlers')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.tests')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages Define filepaths for data and configuration root = \"./\" # And by defining some file paths dn_files = root + \"./example_files/\" fn_nemo_dat_t = dn_files + \"nemo_data_T_grid.nc\" fn_nemo_dat_u = dn_files + \"nemo_data_U_grid.nc\" fn_nemo_dat_v = dn_files + \"nemo_data_V_grid.nc\" fn_nemo_dom = dn_files + \"coast_example_nemo_domain.nc\" # Configuration files describing the data files fn_config_t_grid = root + \"./config/example_nemo_grid_t.json\" fn_config_f_grid = root + \"./config/example_nemo_grid_f.json\" fn_config_u_grid = root + \"./config/example_nemo_grid_u.json\" fn_config_v_grid = root + \"./config/example_nemo_grid_v.json\" Load data variables that are on the NEMO t-grid nemo_t = coast.Gridded(fn_data=fn_nemo_dat_t, fn_domain=fn_nemo_dom, config=fn_config_t_grid) Now create a transect using the coast.TransectT object. The transect is between the points (54 N 15 W) and (56 N, 12 W). This needs to be passed the corresponding NEMO object and transect end points. The model points closest to these coordinates will be selected as the transect end points.\ntran_t = coast.TransectT(nemo_t, (54, -15), (56, -12)) # Inspect the data #tran_t.data # uncomment to print data object summary Plot the data # It is simple to plot a scalar such as temperature along the transect: temp_mean = tran_t.data.temperature.mean(dim=\"t_dim\") plt.figure() temp_mean.plot.pcolormesh(y=\"depth_0\", yincrease=False) plt.show() Create a nemo f-grid object With NEMO’s staggered grid, the first step is to define the transect on the f-grid so that the velocity components are between f-points. We do not need any model data on the f-grid, just the grid information, so create a nemo f-grid object\nnemo_f = coast.Gridded(fn_domain=fn_nemo_dom, config=fn_config_f_grid) Transect on the f-grid tran_f = coast.TransectF(nemo_f, (54, -15), (56, -12)) # Inspect the data #tran_f.data # uncomment to print data object summary Load model data on the u- and v- grids nemo_u = coast.Gridded(fn_data=fn_nemo_dat_u, fn_domain=fn_nemo_dom, config=fn_config_u_grid) nemo_v = coast.Gridded(fn_data=fn_nemo_dat_v, fn_domain=fn_nemo_dom, config=fn_config_v_grid) Calculate the flow across the transect tran_f.calc_flow_across_transect(nemo_u, nemo_v) # The flow across the transect is stored in a new dataset where the variables are all defined at the points between f-points. #tran_f.data_cross_tran_flow # uncomment to print data object summary Plot the time averaged velocity across the transect # To do this we can plot the ‘normal_velocities’ variable. cross_velocity_mean = tran_f.data_cross_tran_flow.normal_velocities.mean(dim=\"t_dim\") plt.figure() cross_velocity_mean.rolling(r_dim=2).mean().plot.pcolormesh(yincrease=False, y=\"depth_0\", cbar_kwargs={\"label\": \"m/s\"}) plt.show() Plot volume transport across the transect # To do this we can plot the ‘normal_transports’ variable. plt.figure() cross_transport_mean = tran_f.data_cross_tran_flow.normal_transports.mean(dim=\"t_dim\") cross_transport_mean.rolling(r_dim=2).mean().plot() plt.ylabel(\"Sv\") plt.show() ","categories":"","description":"Transect tutorial example.\n","excerpt":"Transect tutorial example.\n","ref":"/COAsT/docs/examples/notebooks/gridded/transect_tutorial/","tags":"","title":"Transect tutorial"},{"body":"An example of using COAsT to analysis observational profile data alongside gridded NEMO data.\nLoad modules import coast import glob # For getting file paths import gsw import matplotlib.pyplot as plt import datetime import numpy as np import xarray as xr import coast._utils.general_utils as general_utils import scipy as sp # ====================== UNIV PARAMS =========================== path_examples = \"./example_files/\" path_config = \"./config/\" /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pydap/lib.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.responses')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.handlers')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.tests')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`. Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages load and preprocess profile and model data fn_wod_var = path_examples + \"WOD_example_ragged_standard_level.nc\" fn_wod_config = path_config + \"example_wod_profiles.json\" wod_profile_1d = coast.Profile(config=fn_wod_config) wod_profile_1d.read_wod(fn_wod_var) ./config/example_wod_profiles.json Reshape into 2D. Choose which observed variables you want.\nvar_user_want = [\"salinity\", \"temperature\", \"nitrate\", \"oxygen\", \"dic\", \"phosphate\", \"alkalinity\"] wod_profile = coast.Profile.reshape_2d(wod_profile_1d, var_user_want) Depth OK reshape successful salinity observed variable exist OK reshape successful temperature observed variable exist OK reshape successful nitrate variable not in observations oxygen observed variable exist OK reshape successful dic observed variable exist OK reshape successful phosphate observed variable exist OK reshape successful alkalinity variable not in observations Keep subset.\nwod_profile_sub = wod_profile.subset_indices_lonlat_box(lonbounds=[90, 120], latbounds=[-5, 5]) #wod_profile_sub.dataset # uncomment to print data object summary SEAsia read BGC. Note in this simple test nemo data are only for 3 months from 1990 so the comparisons are not going to be correct but just as a demo.\nfn_seasia_domain = path_examples + \"coast_example_domain_SEAsia.nc\" fn_seasia_config_bgc = path_config + \"example_nemo_bgc.json\" fn_seasia_var = path_examples + \"coast_example_SEAsia_BGC_1990.nc\" seasia_bgc = coast.Gridded( fn_data=fn_seasia_var, fn_domain=fn_seasia_domain, config=fn_seasia_config_bgc, multiple=True ) Domain file does not have mask so this is just a trick.\nseasia_bgc.dataset[\"landmask\"] = seasia_bgc.dataset.bottom_level == 0 seasia_bgc.dataset = seasia_bgc.dataset.rename({\"depth_0\": \"depth\"}) model_profiles = wod_profile_sub.obs_operator(seasia_bgc) #model_profiles.dataset # uncomment to print data object summary /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/coast/data/profile.py:456: UserWarning: Converting non-nanosecond precision timedelta values to nanosecond precision. This behavior can eventually be relaxed in xarray, as it is an artifact from pandas which is now beginning to support non-nanosecond precision values. This warning is caused by passing non-nanosecond np.datetime64 or np.timedelta64 values to the DataArray or Variable constructor; it can be silenced by converting the values to nanosecond precision ahead of time. /usr/share/miniconda/envs/coast/lib/python3.10/site-packages/coast/data/profile.py:456: UserWarning: Converting non-nanosecond precision timedelta values to nanosecond precision. This behavior can eventually be relaxed in xarray, as it is an artifact from pandas which is now beginning to support non-nanosecond precision values. This warning is caused by passing non-nanosecond np.datetime64 or np.timedelta64 values to the DataArray or Variable constructor; it can be silenced by converting the values to nanosecond precision ahead of time. Remove any points that are far from model.\ntoo_far = 5 keep_indices = model_profiles.dataset.interp_dist \u003c= too_far model_profiles = model_profiles.isel(id_dim=keep_indices) wod_profile = wod_profile_sub.isel(id_dim=keep_indices) #wod_profile.dataset # uncomment to print data object summary Plot profiles Transform observed DIC from mmol/l to mmol C/ m^3 that the model has.\nfig = plt.figure() plt.plot(1000 * wod_profile.dataset.dic[8, :], wod_profile.dataset.depth[8, :], linestyle=\"\", marker=\"o\") plt.plot(model_profiles.dataset.dic[8, :], model_profiles.dataset.depth[:, 8], linestyle=\"\", marker=\"o\") plt.ylim([2500, 0]) plt.title(\"DIC vs depth\") plt.show() fig = plt.figure() plt.plot(wod_profile.dataset.oxygen[8, :], wod_profile.dataset.depth[8, :], linestyle=\"\", marker=\"o\") plt.plot(model_profiles.dataset.oxygen[8, :], model_profiles.dataset.depth[:, 8], linestyle=\"\", marker=\"o\") plt.ylim([2500, 0]) plt.title(\"Oxygen vs depth\") plt.show() Perform profile analysis to evaluate differences Interpolate seasia to profile depths, using ProfileAnalysis class.\nreference_depths = wod_profile.dataset.depth[20, :].values model_profiles.dataset = model_profiles.dataset[[\"dic\"]] / 1000 pa = coast.ProfileAnalysis() model_interpolated = pa.interpolate_vertical(model_profiles, wod_profile) Calculate differences.\ndifferences = pa.difference(model_interpolated, wod_profile) #differences.dataset.load() # uncomment to print data object summary ","categories":"","description":"Wod bgc ragged example example.\n","excerpt":"Wod bgc ragged example example.\n","ref":"/COAsT/docs/examples/notebooks/profile/wod_bgc_ragged_example/","tags":"","title":"Wod bgc ragged example"},{"body":"Remote access to Copernicus Marine Environment Monitoring Service CMEMS datasets is enabled via OPeNDAP and Pydap.\nOPeNDAP allows COAsT to stream data from Copernicus without downloading specific subsets or the dataset as a whole.\nIn order to access CMEMS data, you must first create an account.\nAfter you have created your account, or if you already have one, a product ID can be selected from the product catalogue.\nExample import coast # Replace with your own credentials username = \"my_username\" password = \"my_password\" # Authenticate with Copernicus and select a database. database = coast.Copernicus(username, password, \"nrt\") # Instantiate a product with its ID. forecast = database.get_product(\"cmems_mod_glo_phy-thetao_anfc_0.083deg_PT6H-i\") # Create a COAsT object with the relevant config file. nemo_t = coast.Gridded(fn_data=forecast, config=\"./config/example_cmems_grid_t.json\") Look inside the COAsT gridded object:\n\u003e\u003e\u003e nemo_t.dataset \u003cxarray.Dataset\u003e Dimensions: (x_dim: 4320, y_dim: 2041, z_dim: 50, t_dim: 4409) Coordinates: * longitude (x_dim) float32 -180.0 -179.9 -179.8 ... 179.8 179.8 179.9 * latitude (y_dim) float32 -80.0 -79.92 -79.83 -79.75 ... 89.83 89.92 90.0 * z_dim (z_dim) float32 0.494 1.541 2.646 ... 5.275e+03 5.728e+03 * time (t_dim) datetime64[ns] 2020-11-01 ... 2023-11-08 Dimensions without coordinates: x_dim, y_dim, t_dim Data variables: temperature (t_dim, z_dim, y_dim, x_dim) float32 dask.array\u003cchunksize=(1, 50, 2041, 4320), meta=np.ndarray\u003e Attributes: (12/16) title: Instantaneous fields for product GLOBAL_AN... references: http://marine.copernicus.eu credit: E.U. Copernicus Marine Service Information... licence: http://marine.copernicus.eu/services-portf... contact: servicedesk.cmems@mercator-ocean.eu producer: CMEMS - Global Monitoring and Forecasting ... ... ... source: MERCATOR GLO12 product_user_manual: http://marine.copernicus.eu/documents/PUM/... quality_information_document: http://marine.copernicus.eu/documents/QUID... compute_hosts: fidjim01-sihpc,fidjim02-sihpc,fidjim03-sih... n_workers: 4 sshcluster_timeout: 2700 View temperature data directly:\n\u003e\u003e\u003e nemo_t.dataset.temperature.isel(t_dim=1,z_dim=1) \u003cxarray.DataArray 'temperature' (y_dim: 2041, x_dim: 4320)\u003e dask.array\u003cgetitem, shape=(2041, 4320), dtype=float32, chunksize=(2041, 4320), chunktype=numpy.ndarray\u003e Coordinates: * longitude (x_dim) float32 -180.0 -179.9 -179.8 -179.8 ... 179.8 179.8 179.9 * latitude (y_dim) float32 -80.0 -79.92 -79.83 -79.75 ... 89.83 89.92 90.0 z_dim float32 1.541 time datetime64[ns] 2020-11-01T06:00:00 Dimensions without coordinates: y_dim, x_dim Attributes: long_name: Temperature standard_name: sea_water_potential_temperature units: degrees_C unit_long: Degrees Celsius valid_min: -10 valid_max: 40 cell_methods: area: mean _ChunkSizes: [1, 6, 256, 540] Or plot a snapshot of surface temperature (this loaded “lazily” from Copernicus so may take time to render):\nimport matplotlib.pyplot as plt plt.pcolormesh(nemo_t.dataset.temperature.isel(t_dim=1,z_dim=1)) plt.show() ","categories":"","description":"Examples of access to Copernicus datasets via OPeNDAP.\n","excerpt":"Examples of access to Copernicus datasets via OPeNDAP.\n","ref":"/COAsT/docs/examples/remote-datasets/copernicus/","tags":"","title":"Copernicus"},{"body":"","categories":"","description":"Example scripts for access to remote datasets.\n","excerpt":"Example scripts for access to remote datasets.\n","ref":"/COAsT/docs/examples/remote-datasets/","tags":"","title":"Remote Datasets"},{"body":"To date the workflow has been to unit test anything and everything that goes into the develop branch and then periodically push to master less frequently and issue a new github release.\nWith the push to master Git Actions build the conda and pip packages and the package receives a zenodo update (https://zenodo.org/account/settings/github/repository/British-Oceanographic-Data-Centre/COAsT) and DOI.\n1. Push to master Any push to master initiates the Git Actions to build and release the package. It is advisable then to prepare the release in develop and only ever pull into master from develop. (Pulling from master to develop could bring unexpected Git Actions to develop). In order for the package builds to work the version of the package must be unique. The version of the package is set in file setup.py. E.g. shown as 2.0.1 below:\n# setup.py ... PACKAGE = SimpleNamespace(**{ \"name\": \"COAsT\", \"version\": \"2.0.1\", \"description\": \"This is the Coast Ocean Assessment Tool\", \"url\": \"https://www.bodc.ac.uk\", \"download_url\": \"https://github.com/British-Oceanographic-Data-Centre/COAsT/\", .... Package version also appears in CITATION.cff file, which therefore also needs updating. E.g.:\n... title: British-Oceanographic-Data-Centre/COAsT: v2.0.1 version: v2.0.1 date-released: 2022-04-07 Version numbering follows the semantic versioning convention. Briefly, given a version number MAJOR.MINOR.PATCH, increment the:\nMAJOR version when you make incompatible API changes, MINOR version when you add functionality in a backwards compatible manner, and PATCH version when you make backwards compatible bug fixes. Additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format. 2. Issue new release Then issue a new release, with the new version label, and annotate the major changes.\n","categories":"","description":"Procedure for pushing to master / publishing.\n","excerpt":"Procedure for pushing to master / publishing.\n","ref":"/COAsT/docs/general-information/push_to_master/","tags":"","title":"Push to master"},{"body":"AMM15 - 1.5km resolution Atlantic Margin Model \"\"\" AMM15_example_plot.py Make simple AMM15 SST plot. \"\"\" #%% import coast import numpy as np import xarray as xr import matplotlib.pyplot as plt import matplotlib.colors as colors # colormap fiddling ################################################# #%% Loading data ################################################# config = 'AMM15' dir_nam = \"/projectsa/NEMO/gmaya/2013p2/\" fil_nam = \"20130415_25hourm_grid_T.nc\" dom_nam = \"/projectsa/NEMO/gmaya/AMM15_GRID/amm15.mesh_mask.cs3x.nc\" config = \"/work/jelt/GitHub/COAsT/config/example_nemo_grid_t.json\" sci_t = coast.Gridded(dir_nam + fil_nam, dom_nam, config=config) # , chunks=chunks) chunks = { \"x_dim\": 10, \"y_dim\": 10, \"t_dim\": 10, } # Chunks are prescribed in the config json file, but can be adjusted while the data is lazy loaded. sci_t.dataset.chunk(chunks) # create an empty w-grid object, to store stratification sci_w = coast.Gridded(fn_domain=dom_nam, config=config.replace(\"t_nemo\", \"w_nemo\")) sci_w.dataset.chunk({\"x_dim\": 10, \"y_dim\": 10}) # Can reset after loading config json print('* Loaded ',config, ' data') ################################################# #%% subset of data and domain ## ################################################# # Pick out a North Sea subdomain print('* Extract North Sea subdomain') ind_sci = sci_t.subset_indices([51,-4], [62,15]) sci_nwes_t = sci_t.isel(y_dim=ind_sci[0], x_dim=ind_sci[1]) #nwes = northwest europe shelf ind_sci = sci_w.subset_indices([51,-4], [62,15]) sci_nwes_w = sci_w.isel(y_dim=ind_sci[0], x_dim=ind_sci[1]) #nwes = northwest europe shelf #%% Apply masks to temperature and salinity if config == 'AMM15': sci_nwes_t.dataset['temperature_m'] = sci_nwes_t.dataset.temperature.where( sci_nwes_t.dataset.mask.expand_dims(dim=sci_nwes_t.dataset['t_dim'].sizes) \u003e 0) sci_nwes_t.dataset['salinity_m'] = sci_nwes_t.dataset.salinity.where( sci_nwes_t.dataset.mask.expand_dims(dim=sci_nwes_t.dataset['t_dim'].sizes) \u003e 0) else: # Apply fake masks to temperature and salinity sci_nwes_t.dataset['temperature_m'] = sci_nwes_t.dataset.temperature sci_nwes_t.dataset['salinity_m'] = sci_nwes_t.dataset.salinity #%% Plots fig = plt.figure() plt.pcolormesh( sci_t.dataset.longitude, sci_t.dataset.latitude, sci_t.dataset.temperature.isel(z_dim=0).squeeze()) #plt.xlabel('longitude') #plt.ylabel('latitude') #plt.colorbar() plt.axis('off') plt.show() fig.savefig('AMM15_SST_nocolorbar.png', dpi=120) India subcontinent maritime domain. WCSSP India configuration #%% import coast import numpy as np import xarray as xr import dask import matplotlib.pyplot as plt import matplotlib.colors as colors # colormap fiddling ################################################# #%% Loading data ################################################# dir_nam = \"/projectsa/COAsT/NEMO_example_data/MO_INDIA/\" fil_nam = \"ind_1d_cat_20180101_20180105_25hourm_grid_T.nc\" dom_nam = \"domain_cfg_wcssp.nc\" config_t = \"/work/jelt/GitHub/COAsT/config/example_nemo_grid_t.json\" sci_t = coast.Gridded(dir_nam + fil_nam, dir_nam + dom_nam, config=config_t) #%% Plot fig = plt.figure() plt.pcolormesh( sci_t.dataset.longitude, sci_t.dataset.latitude, sci_t.dataset.temperature.isel(t_dim=0).isel(z_dim=0)) plt.xlabel('longitude') plt.ylabel('latitude') plt.title('WCSSP India SST') plt.colorbar() plt.show() fig.savefig('WCSSP_India_SST.png', dpi=120) South East Asia, 1/12 deg configuration (ACCORD: SEAsia_R12) #%% import coast import numpy as np import xarray as xr import dask import matplotlib.pyplot as plt import matplotlib.colors as colors # colormap fiddling ################################################# #%% Loading data ################################################# dir_nam = \"/projectsa/COAsT/NEMO_example_data/SEAsia_R12/\" fil_nam = \"SEAsia_R12_5d_20120101_20121231_gridT.nc\" dom_nam = \"domain_cfg_ORCA12_adj.nc\" config_t = \"/work/jelt/GitHub/COAsT/config/example_nemo_grid_t.json\" sci_t = coast.Gridded(dir_nam + fil_nam, dir_nam + dom_nam, config=config_t) #%% Plot fig = plt.figure() plt.pcolormesh( sci_t.dataset.longitude, sci_t.dataset.latitude, sci_t.dataset.soce.isel(t_dim=0).isel(z_dim=0)) plt.xlabel('longitude') plt.ylabel('latitude') plt.title('SE Asia, surface salinity (psu)') plt.colorbar() plt.show() fig.savefig('SEAsia_R12_SSS.png', dpi=120) ","categories":"","description":"Example scripts and gallery for other NEMO configurations. Scripts from [example_scripts](https://github.com/British-Oceanographic-Data-Centre/COAsT/tree/master/example_scripts)\n","excerpt":"Example scripts and gallery for other NEMO configurations. Scripts …","ref":"/COAsT/docs/examples/configs_gallery/","tags":"","title":"Example scripts and Gallery"},{"body":"__________________________________________________________________________________________ ______ ___ _ _________ .' ___ | .' `. / \\ | _ _ | / .' \\_|/ .-. \\ / _ \\ .--.|_/ | | \\_| | | | | | | / ___ \\ ( (`\\] | | \\ `.___.'\\\\ `-' /_/ / \\ \\_ `'.'. _| |_ `.____ .' `.___.'|____| |____|[\\__) )|_____| Coastal Ocean Assessment Toolbox __________________________________________________________________________________________ Coastal Ocean Assessment Toolbox (COAST) is a Python package specifically designed to assist in the assessment, management and analysis of high resolution regional ocean models outputs.\nHere you can find information on obtaining, installing and using COAsT as well as guidelines for contributing to the project.\nThis documentation site is still under construction but you can still find guidelines for contributing to the package and this website. See below for description of each section.\n","categories":"","description":"","excerpt":"______________________________________________________________________ …","ref":"/COAsT/docs/","tags":"","title":"COAsT Documentation"},{"body":"Objects Altimetry()\nAltimetry.read_cmems()\nAltimetry.load()\nAltimetry.load_single()\nAltimetry.load_multiple()\nAltimetry.subset_indices_lonlat_box()\nAltimetry.quick_plot()\nAltimetry.obs_operator()\nAltimetry.crps()\nAltimetry.difference()\nAltimetry.absolute_error()\nAltimetry.mean_absolute_error()\nAltimetry.root_mean_square_error()\nAltimetry.time_mean()\nAltimetry.time_std()\nAltimetry.time_correlation()\nAltimetry.time_covariance()\nAltimetry.basic_stats()\nAltimetry class\nAltimetry() class Altimetry(Track): An object for reading, storing and manipulating altimetry data. Currently the object contains functionality for reading altimetry netCDF data from the CMEMS database. This is the default for initialisation. Data should be stored in an xarray.Dataset, in the form: * Date Format Overview * 1. A single dimension (time). 2. Three coordinates: time, latitude, longitude. All lie on the 'time' dimension. 3. Observed variable DataArrays on the time dimension. There are currently no naming conventions for the variables however examples from the CMEMS database include sla_filtered, sla_unfiltered and mdt (mean dynamic topography). * Methods Overview * *Initialisation and File Reading* -\u003e __init__(): Initialises an ALTIMETRY object. -\u003e read_cmems(): Reads data from a CMEMS netCDF file. *Plotting* -\u003e quick_plot(): Makes a quick along-track plot of specified data. *Model Comparison* -\u003e obs_operator(): For interpolating model data to this object. -\u003e cprs(): Calculates the CRPS between a model and obs variable. -\u003e difference(): Differences two specified variables -\u003e absolute_error(): Absolute difference, two variables -\u003e mean_absolute_error(): MAE between two variables -\u003e root_mean_square_error(): RMSE between two variables -\u003e time_mean(): Mean of a variable in time -\u003e time_std(): St. Dev of a variable in time -\u003e time_correlation(): Correlation between two variables -\u003e time_covariance(): Covariance between two variables -\u003e basic_stats(): Calculates multiple of the above metrics. Altimetry.read_cmems() def Altimetry.read_cmems(self, file_path, multiple): Read file.\nArgs:\n                file_path (str): path to data file\n                multiple (boolean): True if reading multiple files otherwise False\nAltimetry.load() def Altimetry.load(self, file_or_dir, chunks=None, multiple=False): Loads a file into a object's dataset variable using xarray\nArgs:\n                file_or_dir (str) : file name or directory to multiple files.\n                chunks (dict)                : Chunks to use in Dask [default None]\n                multiple (bool) : If true, load in multiple files from directory.\n                                                                                                If false load a single file [default False]\nAltimetry.load_single() def Altimetry.load_single(self, file, chunks=None): Loads a single file into object's dataset variable.\nArgs:\n                file (str) : file name or directory to multiple files.\n                chunks (dict) : Chunks to use in Dask [default None]\nAltimetry.load_multiple() def Altimetry.load_multiple(self, directory_to_files, chunks=None): Loads multiple files from directory into dataset variable.\nArgs:\n                directory_to_files (str) : directory path to multiple files.\n                chunks (dict) : Chunks to use in Dask [default None]\nAltimetry.subset_indices_lonlat_box() def Altimetry.subset_indices_lonlat_box(self, lonbounds, latbounds): Generates array indices for data which lies in a given lon/lat box.\nKeyword arguments:\nlon                -- Longitudes, 1D or 2D.\nlat                -- Latitudes, 1D or 2D\nlonbounds -- Array of form [min_longitude=-180, max_longitude=180]\nlatbounds -- Array of form [min_latitude, max_latitude]\nreturn: Indices corresponding to datapoints inside specified box\nAltimetry.quick_plot() def Altimetry.quick_plot(self, color_var_str=None): Quick plot\nAltimetry.obs_operator() def Altimetry.obs_operator(self, model, mod_var_name, time_interp=nearest, model_mask=None): For interpolating a model dataarray onto altimetry locations and times.\nFor ALTIMETRY, the interpolation is done independently in two steps:\n                1. Horizontal space\n                2. Time\nModel data is taken at the surface if necessary (0 index).\nExample usage:\n--------------\naltimetry.obs_operator(nemo_obj, 'sossheig')\nParameters\n----------\nmodel : model object (e.g. NEMO)\nmod_var: variable name string to use from model object\ntime_interp: time interpolation method (optional, default: 'nearest')\n                This can take any string scipy.interpolate would take. e.g.\n                'nearest', 'linear' or 'cubic'\nmodel_mask : Mask to apply to model data in geographical interpolation\n                of model. For example, use to ignore land points.\n                If None, no mask is applied. If 'bathy', model variable\n                (bathymetry==0) is used. Custom 2D mask arrays can be\n                supplied.\nReturns\n-------\nAdds a DataArray to self.dataset, containing interpolated values.\nAltimetry.crps() def Altimetry.crps(self, model_object, model_var_name, obs_var_name, nh_radius=20, time_interp=linear, create_new_object=True): Comparison of observed variable to modelled using the Continuous\nRanked Probability Score. This is done using this ALTIMETRY object.\nThis method specifically performs a single-observation neighbourhood-\nforecast method.\nParameters\n----------\nmodel_object (model) : Model object (NEMO) containing model data\nmodel_var_name (str) : Name of model variable to compare.\nobs_var_name (str) : Name of observed variable to compare.\nnh_radius (float)                : Neighbourhood radius (km)\ntime_interp (str)                : Type of time interpolation to use (s)\ncreate_new_obj (bool): If True, save output to new ALTIMETRY obj.\n                                                                                Otherwise, save to this obj.\nReturns\n-------\nxarray.Dataset containing times, sealevel and quality control flags\nExample Usage\n-------\n# Compare modelled 'sossheig' with 'sla_filtered' using CRPS\ncrps = altimetry.crps(nemo, 'sossheig', 'sla_filtered')\nAltimetry.difference() def Altimetry.difference(self, var_str0, var_str1, date0=None, date1=None): Difference two variables defined by var_str0 and var_str1 between\ntwo dates date0 and date1. Returns xr.DataArray\nAltimetry.absolute_error() def Altimetry.absolute_error(self, var_str0, var_str1, date0=None, date1=None): Absolute difference two variables defined by var_str0 and var_str1\nbetween two dates date0 and date1. Return xr.DataArray\nAltimetry.mean_absolute_error() def Altimetry.mean_absolute_error(self, var_str0, var_str1, date0=None, date1=None): Mean absolute difference two variables defined by var_str0 and\nvar_str1 between two dates date0 and date1. Return xr.DataArray\nAltimetry.root_mean_square_error() def Altimetry.root_mean_square_error(self, var_str0, var_str1, date0=None, date1=None): Root mean square difference two variables defined by var_str0 and\nvar_str1 between two dates date0 and date1. Return xr.DataArray\nAltimetry.time_mean() def Altimetry.time_mean(self, var_str, date0=None, date1=None): Time mean of variable var_str between dates date0, date1\nAltimetry.time_std() def Altimetry.time_std(self, var_str, date0=None, date1=None): Time st. dev of variable var_str between dates date0 and date1\nAltimetry.time_correlation() def Altimetry.time_correlation(self, var_str0, var_str1, date0=None, date1=None, method=pearson): Time correlation between two variables defined by var_str0,\nvar_str1 between dates date0 and date1. Uses Pandas corr().\nAltimetry.time_covariance() def Altimetry.time_covariance(self, var_str0, var_str1, date0=None, date1=None): Time covariance between two variables defined by var_str0,\nvar_str1 between dates date0 and date1. Uses Pandas corr().\nAltimetry.basic_stats() def Altimetry.basic_stats(self, var_str0, var_str1, date0=None, date1=None, create_new_object=True): Calculates a selection of statistics for two variables defined by\nvar_str0 and var_str1, between dates date0 and date1. This will return\ntheir difference, absolute difference, mean absolute error, root mean\nsquare error, correlation and covariance. If create_new_object is True\nthen this method returns a new ALTIMETRY object containing statistics,\notherwise variables are saved to the dateset inside this object.\n","categories":"","description":"Docstrings for the Altimetry class\n","excerpt":"Docstrings for the Altimetry class\n","ref":"/COAsT/docs/reference/altimetry/","tags":"","title":"Altimetry"},{"body":"","categories":"","description":"This page provides an auto-generated summary of COAsT’s API. For more details and examples, refer to the relevant chapters in the main part of the documentation.\n","excerpt":"This page provides an auto-generated summary of COAsT’s API. For more …","ref":"/COAsT/docs/reference/","tags":"","title":"API Reference"},{"body":"Objects Argos()\nArgos.read_data()\nArgos class\nArgos() class Argos(Indexed): Class for reading Argos CSV formatted data files into an xarray object Argos.read_data() def Argos.read_data(self, file_path): Read the data file\nExpected format and variable names are\n                DATIM,LAT,LON,SST,SST_F,PSST,PSST_F,PPRES,PPRES_F,BEN\nxarray dataset to have dimension as time and coordinates as time, latitude and longitude\nArgs:\n                file_path (str) : path of data file\n","categories":"","description":"Docstrings for the Argos class\n","excerpt":"Docstrings for the Argos class\n","ref":"/COAsT/docs/reference/argos/","tags":"","title":"Argos"},{"body":"Objects Climatology()\nClimatology.make_climatology()\nClimatology._get_date_ranges()\nClimatology.multiyear_averages()\nClimatology class\nClimatology() class Climatology(Coast): A Python class containing methods for lazily creating climatologies of NEMO data (or any xarray datasets) and writing to file. Also for resampling methods. Climatology.make_climatology() @staticmethod def Climatology.make_climatology(ds, output_frequency, monthly_weights=False, time_var_name=time, time_dim_name=t_dim, fn_out=None): Calculates a climatology for all variables in a supplied dataset.\nThe resulting xarray dataset will NOT be loaded to RAM. Instead,\nit is a set of dask operations. To load to RAM use, e.g. .compute().\nHowever, if the original data was large, this may take a long time and\na lot of memory. Make sure you have the available RAM or chunking\nand parallel processes are specified correctly.\nOtherwise, it is recommended that you access the climatology data\nin an indexed way. I.E. compute only at specific parts of the data\nare once.\nThe resulting cliamtology dataset can be written to disk using\n.to_netcdf(). Again, this may take a while for larger datasets.\nds :: xarray dataset object from a Coast object.\noutput_frequency :: any xarray groupby string. i.e:\n                'month'\n                'season'\ntime_var_name :: the string name of the time variable in dataset\ntime_dim_name :: the string name of the time dimension variable in dataset\nfn_out :: string defining full output netcdf file path and name.\nClimatology._get_date_ranges() @staticmethod def Climatology._get_date_ranges(years, month_periods): Calculates a list of datetime date ranges for a given list of years and a specified start/end month.\nArgs:\n                years (list): A list of years to calculate date ranges for.\n                month_periods (list): A list containing tuples of start and end month integers.\n                (i.e. [(3,5),(12, 2)] is Mar -\u003e May, Dec -\u003e Feb). Must be in chronological order.\n                Returns:\n                                date_ranges (list): A list of tuples, each containing a start and end datetime.date object.\nClimatology.multiyear_averages() @classmethod def Climatology.multiyear_averages(cls, ds, month_periods, time_var=time, time_dim=t_dim): Calculate multiyear means for all Data variables in a dataset between a given start and end month.\nArgs:\n                ds (xr.Dataset): xarray dataset containing data.\n                month_periods (list): A list containing tuples of start and end month integers.\n                (i.e. [(3,5),(12, 2)] is Mar -\u003e May, Dec -\u003e Feb). Must be in chronological order.\n                The seasons module can be used for convenience (e.g. seasons.WINTER, seasons.ALL etc. )\n                time_var (str): String representing the time variable name within the dataset.\n                time_dim (str): String representing the time dimension name within the dataset.\nreturns:\n                ds_mean (xr.Dataset): A new dataset containing mean averages for each data variable across all years and\n                date periods. Indexed by the multi-index 'year_period' (i.e. (2000, 'Dec-Feb')).\n","categories":"","description":"Docstrings for the Climatology class\n","excerpt":"Docstrings for the Climatology class\n","ref":"/COAsT/docs/reference/climatology/","tags":"","title":"Climatology"},{"body":"Objects setup_dask_client()\nCoast()\nCoast.load()\nCoast.load_single()\nCoast.load_multiple()\nCoast.load_dataset()\nCoast.set_dimension_mapping()\nCoast.set_variable_mapping()\nCoast.set_grid_ref_attribute()\nCoast.set_dimension_names()\nCoast.set_variable_names()\nCoast.set_variable_grid_ref_attribute()\nCoast.copy()\nCoast.isel()\nCoast.sel()\nCoast.rename()\nCoast.subset()\nCoast.subset_as_copy()\nCoast.distance_between_two_points()\nCoast.subset_indices_by_distance()\nCoast.subset_indices_lonlat_box()\nCoast.calculate_haversine_distance()\nCoast.get_subset_as_xarray()\nCoast.get_2d_subset_as_xarray()\nCoast.plot_simple_2d()\nCoast.plot_cartopy()\nCoast.plot_movie()\nThe coast class is the main access point into this package.\nsetup_dask_client() def setup_dask_client(workers=2, threads=2, memory_limit_per_worker=2GB): None\nCoast() class Coast(): None Coast.load() def Coast.load(self, file_or_dir, chunks=None, multiple=False): Loads a file into a COAsT object's dataset variable using xarray.\nArgs:\n                file_or_dir (str): file name, OPeNDAP accessor, or directory to multiple files.\n                chunks (dict): Chunks to use in Dask [default None].\n                multiple (bool): If true, load in multiple files from directory. If false load a single file [default False].\nCoast.load_single() def Coast.load_single(self, file, chunks=None): Loads a single file into COAsT object's dataset variable.\nArgs:\n                file (str): Input file.\n                chunks (Dict): Chunks to use in Dask [default None].\nCoast.load_multiple() def Coast.load_multiple(self, directory_to_files, chunks=None): Loads multiple files from directory into dataset variable.\nArgs:\n                directory_to_files (str):\n                chunks (Dict): Chunks to use in Dask [default None].\nCoast.load_dataset() def Coast.load_dataset(self, dataset): Loads a dataset.\nArgs:\n                dataset (xr.Dataset): Dataset to load.\nCoast.set_dimension_mapping() def Coast.set_dimension_mapping(self): Set mapping of dimensions.\nCoast.set_variable_mapping() def Coast.set_variable_mapping(self): Set mapping of variable.\nCoast.set_grid_ref_attribute() def Coast.set_grid_ref_attribute(self): Set grid reference attribute.\nCoast.set_dimension_names() def Coast.set_dimension_names(self, dim_mapping): Relabel dimensions in COAsT object xarray.dataset to ensure consistent naming throughout the COAsT package.\nArgs:\n                dim_mapping (Dict): keys are dimension names to change and values new dimension names.\nCoast.set_variable_names() def Coast.set_variable_names(self, var_mapping): Relabel variables in COAsT object xarray.dataset to ensure consistent naming throughout the COAsT package.\nArgs:\n                var_mapping (Dict): keys are variable names to change and values are new variable names\nCoast.set_variable_grid_ref_attribute() def Coast.set_variable_grid_ref_attribute(self, grid_ref_attr_mapping): Set attributes for variables to access within package and set grid attributes to identify which grid variable is associated with.\nArgs:\n                grid_ref_attr_mapping (Dict): Dict containing mappings.\nCoast.copy() def Coast.copy(self): Method to copy self.\nCoast.isel() def Coast.isel(self, indexers=None, drop=False, **kwargs): Indexes COAsT object along specified dimensions using xarray isel.\nInput is of same form as xarray.isel. Basic use, hand in either:\n1. dictionary with keys = dimensions, values = indices\n2. **kwargs of form dimension = indices.\nArgs:\n                indexers (Dict): A dict with keys matching dimensions and values given by integers, slice objects or arrays.\n                drop (bool): If drop=True, drop coordinates variables indexed by integers instead of making them scalar.\n                **kwargs (Any): The keyword arguments form of indexers. One of indexers or indexers_kwargs must be provided.\nCoast.sel() def Coast.sel(self, indexers=None, drop=False, **kwargs): Indexes COAsT object along specified dimensions using xarray sel.\nInput is of same form as xarray.sel. Basic use, hand in either:\n                1. Dictionary with keys = dimensions, values = indices\n                2. **kwargs of form dimension = indices\nArgs:\n                indexers (Dict): A dict with keys matching dimensions and values given by scalars, slices or arrays of tick labels.\n                drop (bool): If drop=True, drop coordinates variables in indexers instead of making them scalar.\n                **kwargs (Any): The keyword arguments form of indexers. One of indexers or indexers_kwargs must be provided.\nCoast.rename() def Coast.rename(self, rename_dict, **kwargs): Rename dataset.\nArgs:\n                rename_dict (Dict): Dictionary whose keys are current variable or dimension names and whose values are the desired names.\n                **kwargs (Any): Keyword form of name_dict. One of name_dict or names must be provided.\nCoast.subset() def Coast.subset(self, **kwargs): Subsets all variables within the dataset inside self (a COAsT object).\nInput is a set of keyword argument pairs of the form: dimension_name = indices.\nThe entire object is then subsetted along this dimension at indices\nArgs:\n                **kwargs (Any): The keyword arguments form of indexers. One of indexers or indexers_kwargs must be provided.\nCoast.subset_as_copy() def Coast.subset_as_copy(self, **kwargs): Similar to COAsT.subset() however applies the subsetting to a copy of the original COAsT object.\nThis subsetted copy is then returned.Useful for preserving the original object whilst creating smaller subsetted object copies.\nArgs:\n                **kwargs (Any): The keyword arguments form of indexers. One of indexers or indexers_kwargs must be provided.\nCoast.distance_between_two_points() def Coast.distance_between_two_points(self): None\nCoast.subset_indices_by_distance() def Coast.subset_indices_by_distance(self, centre_lon, centre_lat, radius): This method returns a `tuple` of indices within the `radius` of the lon/lat point given by the user.\nDistance is calculated as haversine - see `self.calculate_haversine_distance`.\nArgs:\n                centre_lon (float): The longitude of the users central point.\n                centre_lat (float): The latitude of the users central point.\n                radius (float): The haversine distance (in km) from the central point.\nReturn:\n                Tuple[xr.DataArray, xr.DataArray]: All indices in a `tuple` with the haversine distance of the central point.\nCoast.subset_indices_lonlat_box() def Coast.subset_indices_lonlat_box(self, lonbounds, latbounds): Generates array indices for data which lies in a given lon/lat box.\nArgs:\n                lonbounds: Longitude boundaries. List of form [min_longitude=-180, max_longitude=180].\n                latbounds: Latitude boundaries. List of form [min_latitude, max_latitude].\nReturns:\n                np.ndarray: Indices corresponding to datapoints inside specified box.\nCoast.calculate_haversine_distance() @staticmethod def Coast.calculate_haversine_distance(lon1, lat1, lon2, lat2): Estimation of geographical distance using the Haversine function.\nInput can be single values or 1D arrays of locations. This does NOT create a distance matrix but outputs another 1D array.\nThis works for either location vectors of equal length OR a single location and an arbitrary length location vector.\nArgs:\n                lon1 (Any): Angles in degrees.\n                lat1 (Any): Angles in degrees.\n                lon2 (Any): Angles in degrees.\n                lat2 (Any): Angles in degrees.\nReturns:\n                float: Haversine distance between points.\nCoast.get_subset_as_xarray() def Coast.get_subset_as_xarray(self, var, points_x, points_y, line_length=None, time_counter=0): This method gets a subset of the data across the x/y indices given for the chosen variable.\nSetting time_counter to None will treat `var` as only having 3 dimensions depth, y, x\nthere is a check on `var` to see the size of the time_counter, if 1 then time_counter is fixed to index 0.\nArgs:\n                var (str): The name of the variable to get data from.\n                points_x (slice): A list/array of indices for the x dimension.\n                points_y (slice): A list/array of indices for the y dimension.\n                line_length (int): The length of your subset (assuming simple line transect). TODO This is unused.\n                time_counter (int): Which time slice to get data from, if None and the variable only has one a time\n                                channel of length 1 then time_counter is fixed too an index of 0.\nReturns:\n                xr.DataArray: Data across all depths for the chosen variable along the given indices.\nCoast.get_2d_subset_as_xarray() def Coast.get_2d_subset_as_xarray(self, var, points_x, points_y, line_length=None, time_counter=0): Get 2d subset as an xarray.\nArgs:\n                var (str): Member of dataset.\n                points_x (slice): Keys matching dimensions.\n                points_y (slice): Keys matching dimensions.\n                line_length (int): Unused.\n                time_counter (int): Time counter.\nReturn:\n                xr.Dataset: Subset.\nCoast.plot_simple_2d() def Coast.plot_simple_2d(self, x, y, data, cmap, plot_info): This is a simple method that will plot data in a 2d. It is a wrapper for matplotlib's 'pcolormesh' method.\n`cmap` and `plot_info` are required to run this method, `cmap` is passed directly to `pcolormesh`.\n`plot_info` contains all the required information for setting the figure;\n                - ylim\n                - xlim\n                - clim\n                - title\n                - fig_size\n                - ylabel\nArgs:\n                x (xr.Variable): The variable contain the x axis information.\n                y (xr.Variable): The variable contain the y axis information.\n                data (xr.DataArray): the DataArray a user wishes to plot.\n                cmap (matplotlib.cm): Matplotlib color map.\n                plot_info (Dict): Dict containing all the required information for setting the figure.\nCoast.plot_cartopy() def Coast.plot_cartopy(self, var, plot_var, params, time_counter=0): Plot cartopy.\nCoast.plot_movie() def Coast.plot_movie(self): Plot movie.\n","categories":"","description":"Docstrings for the Coast class\n","excerpt":"Docstrings for the Coast class\n","ref":"/COAsT/docs/reference/coast/","tags":"","title":"Coast"},{"body":"Objects ConfigParser()\nConfigParser._parse_gridded()\nConfigParser._parse_indexed()\nConfigParser._get_code_processing_object()\nConfigParser._get_datafile_object()\nConfig parser.\nConfigParser() class ConfigParser(): Class for parsing gridded and indexed configuration files. ConfigParser._parse_gridded() @staticmethod def ConfigParser._parse_gridded(json_content): Static method to parse Gridded config files.\nArgs:\n                json_content (dict): Config file json.\nConfigParser._parse_indexed() @staticmethod def ConfigParser._parse_indexed(json_content): Static method to parse Indexed config files.\nArgs:\n                json_content (dict): Config file json.\nConfigParser._get_code_processing_object() @staticmethod def ConfigParser._get_code_processing_object(json_content): Static method to convert static_variables configs into objects.\nArgs:\n                json_content (dict): Config file json.\nConfigParser._get_datafile_object() @staticmethod def ConfigParser._get_datafile_object(json_content, data_file_type): Static method to convert dataset and domain configs into objects.\nArgs:\n                json_content (dict): Config file json.\n                data_file_type (str): key of datafile type (dataset or domain).\n","categories":"","description":"Docstrings for the Config_parser class\n","excerpt":"Docstrings for the Config_parser class\n","ref":"/COAsT/docs/reference/config_parser/","tags":"","title":"Config_parser"},{"body":"Objects ConfigTypes()\nConfigKeys()\nDataFile()\nCodeProcessing()\nDataset()\nDomain()\nConfig()\nGriddedConfig()\nIndexedConfig()\nClasses defining config structure.\nConfigTypes() class ConfigTypes(Enum): Enum class containing the valid types for config files. ConfigKeys() class ConfigKeys(): Class of constants representing valid keys within configuriation json. DataFile() class DataFile(): General parent dataclass for holding common config attributes of datafiles. Args: variable_map (dict): dict containing mapping for variable names. dimension_map (dict): dict containing mapping for dimension names. keep_all_vars (boolean): True if xarray is to retain all data file variables otherwise False i.e keep only those in the json config file variable mappings. CodeProcessing() class CodeProcessing(): Dataclass holding config attributes for static variables that might not need changing between model runs Args: not_grid_variables (list): A list of variables not belonging to the grid. delete_variables (list): A list of variables to drop from the dataset. Dataset() class Dataset(DataFile): Dataclass holding config attributes for Dataset datafiles. Extends DataFile. Args: coord_var (list): list of dataset coordinate variables to apply once dataset is loaded Domain() class Domain(DataFile): Dataclass holding config attributes for Domain datafiles. Extends DataFile. Config() class Config(): General dataclass for holding common config file attributes. Args: dataset (Dataset): Dataset object representing 'dataset' config. processing_flags (list): List of processing flags. chunks (dict): dict for dask chunking config. (i.e. {\"dim1\":100, \"dim2\":100, \"dim3\":100}). type (ConfigTypes): Type of config. Must be a valid ConfigType. GriddedConfig() class GriddedConfig(Config): Dataclass for holding gridded-config specific attributes. Extends Config. Args: type (ConfigTypes): Type of config. Set to ConfigTypes.GRIDDED. grid_ref (dict): dict containing key:value of grid_ref:[list of grid variables]. domain (Domain): Domain object representing 'domain' config. IndexedConfig() class IndexedConfig(Config): Dataclass for holding indexed-config specific attributes. Extends Config. Args: type (ConfigTypes): Type of config. Set to ConfigTypes.INDEXED. ","categories":"","description":"Docstrings for the Config_structure class\n","excerpt":"Docstrings for the Config_structure class\n","ref":"/COAsT/docs/reference/config_structure/","tags":"","title":"Config_structure"},{"body":"Objects Contour()\nContour.get_contours()\nContour.plot_contour()\nContour.get_contour_segment()\nContour.process_contour()\nContour.gen_z_levels()\nContourF()\nContourF.calc_cross_contour_flow()\nContourF._update_cross_flow_vars()\nContourF._update_cross_flow_latlon()\nContourF._pressure_gradient_fpoint2()\nContourF.calc_geostrophic_flow()\nContourT()\nContourT.construct_pressure()\nContourT.calc_along_contour_flow()\nContourT.calc_along_contour_flow_2d()\nContourT._update_flow_vars()\nContourT._update_along_flow_latlon()\nContour classes\nContour() class Contour(): None Contour.get_contours() @staticmethod def Contour.get_contours(gridded, contour_depth): A method to obtain the continuous isbobath contours within a supplied\ngridded domain as a set of y indices and x indices for the model grid.\nParameters\n----------\ngridded : Coast\n                The gridded object containing the dataset with the 'bathymetry' variable\ncontour_depth : int\n                Depth of desired contours\nReturns\n-------\nList of 2d ndarrays\n                Each item of the list contains a different continuous isobath contour as a\n                2d ndarray of indicies, i.e. for each list item:\n                contour[:,0] contains the y indices for the contour on the model grid\n                contour[:,1] contains the x indices for the contour on the model grid\nContour.plot_contour() @staticmethod def Contour.plot_contour(gridded, contour): Quick plot method to plot a contour over a pcolormesh of the\nmodel bathymetry\nParameters\n----------\ngridded : Coast\n                The gridded object containing the dataset with the 'bathymetry' variable\ncontour : 2d array\n                contour[:,0] contains the y indices for the contour on the model grid\n                contour[:,1] contains the x indices for the contour on the model grid\n                i.e. contour = np.vstack((y_indices,x_indices)).T\nReturns\n-------\nNone\nContour.get_contour_segment() @staticmethod def Contour.get_contour_segment(gridded, contour, start_coords, end_coords): Method that will take a contour from the list of contours generated by\ncoast.Contour.get_contours() and trim it to start at supplied (lat,lon)\ncoordinates and end at supplied (lat, lon) coordinates.\nParameters\n----------\ngridded : Coast\n                The gridded object containing the dataset with the 'bathymetry' variable\ncontour : numpy.ndarray\n                contour[:,0] contains the y indices for the contour on the model grid\n                contour[:,1] contains the x indices for the contour on the model grid\nstart_coords : numpy.ndarray\n                1d array containing [latitude,longitude] of the start point of the contour\nend_coords : numpy.ndarray\n                1d array containing [latitude,longitude] of the end point of the contour\nReturns\n-------\ny_ind : numpy.ndarray\n                y indices of the contour on the model grid\nx_ind : numpy.ndarray\n                x indices of the contour on the model grid\ncontour : numpy.ndarray\n                For the convenience of plotting using coast.Contour.plot_contour()\n                contour[:,0] = y_ind\n                contour[:,1] = x_ind\nContour.process_contour() def Contour.process_contour(self, dataset, y_ind, x_ind): Redefine contour so that each point on the contour defined by\ny_ind and x_ind is seperated from its neighbours by a single index change\nin y or x, but not both.\nexample: convert y_ind = [10,11], x_ind = [1,2] to y_ind = [10,10], x_ind = [1,2]\nor y_ind = [10,11], x_ind = [1,1]\nParameters\n----------\ndataset : xarray.Dataset\n                xarray Dataset from supplied gridded object\ny_ind : numpy.ndarray\n                1d array of y indices defining the contour on the model grid\nx_ind : numpy.ndarray\n                1d array of x indices defining the contour on the model grid\nReturns\n-------\ny_ind : numpy.ndarray\n                processed y indices of the contour on the model grid\nx_ind : numpy.ndarray\n                processed x indices of the contour on the model grid\nContour.gen_z_levels() @staticmethod def Contour.gen_z_levels(max_depth): Generates a pre-defined 1d vertical depth coordinates,\ni.e. horizontal z-level vertical coordinates up to a supplied\nmaximum depth, 'max_depth'\nContourF() class ContourF(Contour): Class defining a Contour type on the f-grid, which is a 3d dataset of points between a point A and a point B defining an isobath contour. The dataset has a time, depth and contour dimension. The contour dimension defines the points along the contour. The supplied model f-grid Data is subsetted in its entirety along these dimensions within Contour_f.data_contour of type xarray.Dataset Parameters ---------- gridded_f : Coast f-grid gridded object containing the model dataset. y_ind : numpy.ndarray 1d array of y indices defining the contour on the model grid x_ind : numpy.ndarray 1d array of x indices defining the contour on the model grid depth : int Depth of contour isobath ContourF.calc_cross_contour_flow() def ContourF.calc_cross_contour_flow(self, gridded_u, gridded_v): Method that will calculate the flow across the contour and store this data\nwithin Contour_f.data_cross_flow, which is an xarray.Dataset. Specifically\nContour_f.normal_velocities are the velocities across the contour\n(time, depth, position along contour) in m/s\nContour_f.depth_integrated_normal_transport are the depth integrated\nvolume transports across the contour (time, position along contour) in Sv\nIf the time dependent cell thicknesses (e3) on the u and v grids are\npresent in the gridded_u and gridded_v datasets they will be used, if they\nare not then the initial cell thicknesses (e3_0) will be used.\nParameters\n----------\ngridded_u : Coast\n                The gridded object containing the model data on the u-grid.\ngridded_v : Coast\n                The gridded object containing the model data on the v-grid.\nReturns\n-------\nNone.\nContourF._update_cross_flow_vars() def ContourF._update_cross_flow_vars(self, var, u_var, v_var, dr_n, dr_s, dr_e, dr_w, pos): This method will pull variable data at specific points along the contour\nfrom the u and v grid datasets and put them into the self.data_cross_flow dataset\nContourF._update_cross_flow_latlon() def ContourF._update_cross_flow_latlon(self, ds_u, ds_v, dr_n, dr_s, dr_e, dr_w): This method will pull the latitude and longitude data at specific points along the\ncontour from the u and v grid datasets and put them into the self.data_cross_flow dataset\nContourF._pressure_gradient_fpoint2() @staticmethod def ContourF._pressure_gradient_fpoint2(ds_t, ds_t_j1, ds_t_i1, ds_t_j1i1, r_ind, velocity_component): Calculates the hydrostatic and surface pressure gradients at a set of f-points\nalong the contour, i.e. at a set of specific values of r_dim (but for all time and depth).\nThe caller must supply four datasets that contain the variables which define\nthe hydrostatic and surface pressure at all vertical z_levels and all time\non the t-points around the contour i.e. for a set of f-points on the contour\ndefined each defined at (j+1/2, i+1/2), we want t-points at (j,i), (j+1,i), (j,i+1), (j+1,i+1),\ncorresponding to ds_t, ds_t_j1, ds_t_i1, ds_t_j1i1, respectively.\nds_t, ds_t_j1, ds_t_i1, ds_t_j1i1 will have dimensions in time and depth.\nThe velocity_component defines whether u or v is normal to the contour\nfor the segments of the contour. A segment of contour is\ndefined as being r_dim to r_dim+1 where r_dim is the along contour dimension.\nReturns\n-------\nhpg_f : DataArray with dimensions in time and depth and along contour\n                hydrostatic pressure gradient at a set of f-points along the contour\n                for all time and depth\nspg_f : DataArray with dimensions in time and depth and along contour\n                surface pressure gradient at a set of f-points along the contour\nContourF.calc_geostrophic_flow() def ContourF.calc_geostrophic_flow(self, gridded_t, ref_density=None, config_u=config/example_nemo_grid_u.json, config_v=config/example_nemo_grid_v.json): This method will calculate the geostrophic velocity and volume transport\n(due to the geostrophic current) across the contour.\nFour variables are added to the Contour.data_cross_flow dataset:\n1. normal_velocity_hpg                (t_dim, depth_z_levels, r_dim)\nThis is the velocity due to the hydrostatic pressure gradient\n2. normal_velocity_spg                (t_dim, r_dim)\nThis is the velocity due to the surface pressure gradient\n3. transport_across_AB_hpg (t_dim, r_dim)\nThis is the volume transport due to the hydrostatic pressure gradient\n4. transport_across_AB_spg (t_dim, r_dim\nThis is the volume transport due to the surface pressure gradient\nThis implementation works by regridding vertically onto horizontal z_levels in order\nto perform the horizontal gradients. Currently s_level depths are\nassumed fixed at their initial depths, i.e. at time zero.\nRequirements: The gridded t-grid dataset, gridded_t, must contain the sea surface height,\nPractical Salinity and the Potential Temperature variables. The depth_0\nfield must also be supplied. The GSW package is used to calculate\nThe Absolute Pressure, Absolute Salinity and Conservate Temperature.\nParameters\n----------\ngridded_t : Coast\n                This is the gridded model data on the t-grid for the entire domain.\nref_density : TYPE, optional\n                reference density value. If not supplied a mean in time, depth and\n                along the contour will be used as the mean reference value.\nconfig_u : file\n                configuration file for u-grid object\nconfig_v : file\n                configuration file for v-grid object\nReturns\n-------\nNone.\nContourT() class ContourT(Contour): Class defining a Contour type on the t-grid, which is a 3d dataset of points between a point A and a point B defining an isobath contour. The dataset has a time, depth and contour dimension. The contour dimension defines the points along the contour. The supplied model t-grid Data is subsetted in its entirety along these dimensions and calculations can be performed on this dataset. Parameters ---------- gridded_t : Coast t-grid gridded object containing the model dataset. y_ind : numpy.ndarray 1d array of y indices defining the contour on the model grid x_ind : numpy.ndarray 1d array of x indices defining the contour on the model grid depth : int Depth of contour isobath ContourT.construct_pressure() def ContourT.construct_pressure(self, ref_density=None, z_levels=None, extrapolate=False):                 This method is for calculating the hydrostatic and surface pressure fields\n                on horizontal levels in the vertical (z-levels). The motivation\n                is to enable the calculation of horizontal gradients; however,\n                the variables can quite easily be interpolated onto the original\n                vertical grid.\n                Requirements: The object's t-grid dataset must contain the sea surface height,\n                Practical Salinity and the Potential Temperature variables.\n                The GSW package is used to calculate the Absolute Pressure,\n                Absolute Salinity and Conservate Temperature.\n                Three new variables (density, hydrostatic pressure, surface pressure)\n                are created and added to the Contour_t.data_contour dataset:\n                                density_zlevels                (t_dim, depth_z_levels, r_dim)\n                                pressure_h_zlevels                (t_dim, depth_z_levels, r_dim)\n                                pressure_s                                                (t_dim, r_dim)\n                Note that density is constructed using the EOS10\n                equation of state.\nParameters\n----------\nref_density: float\n                reference density value, if None, then the Contour mean across time,\n                depth and along contour will be used.\nz_levels : (optional) numpy array\n                1d array that defines the depths to interpolate the density and pressure\n                on to.\nextrapolate : boolean, default False\n                If true the variables are extrapolated to the deepest z_level, if false,\n                values below the bathymetry are set to NaN\nReturns\n-------\nNone.\nContourT.calc_along_contour_flow() def ContourT.calc_along_contour_flow(self, gridded_u, gridded_v): Function that will calculate the flow along the contour and store this data\nwithin Contour_t.data_along_flow, which is an xarray.Dataset. Specifically\nContour_t.data_along_flow.velocities are the velocities along the contour with dimensions\n(t_dim, z_dim, r_dim), where r_dim is the dimension along the contour.\nContour_t.data_along_flow.transport are the velocities along the contour multiplied by the\nthickness of the cell (velocity * e3) with dimensions\n(t_dim, z_dim, r_dim).\nIf the time dependent cell thicknesses (e3) on the u and v grids are\npresent in the gridded_u and gridded_v datasets they will be used, if they\nare not then the initial cell thicknesses (e3_0) will be used.\nParameters\n----------\ngridded_u : Coast\n                The nemo object containing the model data on the u-grid.\ngridded_v : Coast\n                The nemo object containing the model data on the v-grid.\nReturns\n-------\nNone.\nContourT.calc_along_contour_flow_2d() def ContourT.calc_along_contour_flow_2d(self, gridded_u, gridded_v): Function that will calculate the 2d flow (no vertical dimension\nalong the contour and store this data within Contour_t.data_along_flow,\nwhich is an xarray.Dataset. Contour_t.data_along_flow.velocities are\nthe velocities along the contour with dimensions (t_dim, r_dim),\nwhere r_dim is the dimension along the contour. e3 and\ne3_0 are interpreted to be the water column thicknesses and\nare included in the dataset as Contour_t.data_along_flow.e3 and\nContour_t.data_along_flow.e3_0\nParameters\n----------\ngridded_u : Coast\n                The nemo object containing the model data on the u-grid.\ngridded_v : Coast\n                The nemo object containing the model data on the v-grid.\nReturns\n-------\nNone.\nContourT._update_flow_vars() def ContourT._update_flow_vars(self, var, u_var, v_var, dr_n, dr_s, dr_e, dr_w, pos): This method will pull variable data at specific points along the contour\nfrom the u and v grid datasets and put them into the self.data_along_flow dataset\nContourT._update_along_flow_latlon() def ContourT._update_along_flow_latlon(self, ds_u, ds_v, dr_n, dr_s, dr_e, dr_w): This method will pull latitude and longitude data at specific points along the\ncontour from the u and v grid datasets and put them into the self.data_along_flow dataset\n","categories":"","description":"Docstrings for the Contour class\n","excerpt":"Docstrings for the Contour class\n","ref":"/COAsT/docs/reference/contour/","tags":"","title":"Contour"},{"body":"Objects CopernicusBase()\nCopernicusBase.get_url()\nProduct()\nProduct.from_copernicus()\nCopernicus()\nCopernicus.get_product()\nFunctionality for accessing Copernicus datasets via OPeNDAP.\nCopernicusBase() class CopernicusBase(): Information required for accessing Copernicus datasets via OPeNDAP. CopernicusBase.get_url() def CopernicusBase.get_url(self, product_id): Get the URL required to access a Copernicus OPeNDAP dataset.\nArgs:\n                product_id: The product ID belonging to the chosen dataset.\nReturns:\n                The constructed URL.\nProduct() class Product(OpendapInfo): Information required to access and stream data from a Copernicus product. Product.from_copernicus() @classmethod def Product.from_copernicus(cls, product_id, copernicus): Instantiate a Product using Copernicus information and a specific product ID.\nArgs:\n                product_id: The product ID of the chosen Copernicus OPeNDAP dataset.\n                copernicus: A previously instantiated Copernicus info object.\nReturns:\n                An instantiated Product accessor.\nCopernicus() class Copernicus(CopernicusBase): An object for accessing Copernicus products via OPeNDAP. Copernicus.get_product() def Copernicus.get_product(self, product_id): Instantiate a Product related to a specific product ID.\nArgs:\n                product_id: The product ID of the chosen Copernicus OPeNDAP dataset.\nReturns:\n                The instantiated Product accessor.\n","categories":"","description":"Docstrings for the Copernicus class\n","excerpt":"Docstrings for the Copernicus class\n","ref":"/COAsT/docs/reference/copernicus/","tags":"","title":"Copernicus"},{"body":"Objects crps_empirical()\ncrps_empirical.calc()\ncrps_empirical_loop()\ncrps_empirical_loop.calc()\ncrps_sonf_fixed()\ncrps_sonf_moving()\nPython definitions used to aid in the calculation of Continuous Ranked Probability Score. Methods Overview -\u003e crps_sonf_fixed(): Single obs neighbourhood forecast CRPS for fixed obs -\u003e crps_song_moving(): Same as above for moving obs\ncrps_empirical() def crps_empirical(sample, obs): Calculates CRPS for a single observations against a sample of values.\nThis sample of values may be an ensemble of model forecasts or a model\nneighbourhood. This is a comparison of a Heaviside function defined by\nthe observation value and an Empirical Distribution Function (EDF)\ndefined by the sample of values. This sample is sorted to create the\nEDF.The calculation method is that outlined by Hersbach et al. (2000).\nEach member of a supplied sample is weighted equally.\nArgs:\n                sample (array): Array of points (ensemble or neighbourhood)\n                obs (float): A single 'observation' value which to compare against\n                                                                sample CDF.\nReturns:\n                np.ndarray: A single CRPS value.\ncrps_empirical.calc() def crps_empirical.calc(alpha, beta, p): None\ncrps_empirical_loop() def crps_empirical_loop(sample, obs): Like crps_empirical, however a loop is used instead of numpy\nboolean indexing. For large samples, will be slower but consume less\nmemory.\nArgs:\n                sample (array): Array of points (ensemble or neighbourhood)\n                obs (float): A single 'observation' value which to compare against\n                                                                sample CDF.\nReturns:\n                float: A single CRPS integral value.\ncrps_empirical_loop.calc() def crps_empirical_loop.calc(alpha, beta, p): None\ncrps_sonf_fixed() def crps_sonf_fixed(mod_array, obs_lon, obs_lat, obs_var, obs_time, nh_radius, time_interp): Single-observation neighbourhood forecast CRPS for a time series at a fixed observation location.\nHandles the calculation of single-observation neighbourhood forecast CRPS for a time series at a fixed observation location.\nDiffers from crps_sonf_moving in that it only need calculate a model neighbourhood once.\nArgs:\n                mod_array (xr.DataArray): DataArray from a Model Dataset.\n                obs_lon (float): Longitude of fixed observation point.\n                obs_lat (float): Latitude of fixed observation point.\n                obs_var (np.ndarray): of floatArray of variable values, e.g time series.\n                obs_time (np.ndarray): of datetimeArray of times, corresponding to obs_var.\n                nh_radius (float): Neighbourhood radius in km.\n                time_interp (str): Type of time interpolation to use.\nReturns:\n                Tuple[np.ndarray, np.ndarray, np.ndarray]: Array of CRPS values, array containing the number of model points used for\n                                each CRPS value and an array of bools indicating where a model neighbourhood\n                                contained land.\ncrps_sonf_moving() def crps_sonf_moving(mod_array, obs_lon, obs_lat, obs_var, obs_time, nh_radius, time_interp): Handles the calculation of single-observation neighbourhood forecast CRPS for a moving observation instrument.\nDiffers from crps_sonf_fixed in that latitude and longitude are arrays of locations. Mod_array must contain\ndimensions x_dim, y_dim and t_dim and coordinates longitude, latitude, time.\nArgs:\n                mod_array (xr.DataArray): DataArray from a Model Dataset.\n                obs_lon (np.ndarray): Longitudes of fixed observation point.\n                obs_lat (np.ndarray): Latitudes of fixed observation point.\n                obs_var (np.ndarray): of floatArray of variable values, e.g time series.\n                obs_time: (np.ndarray): of datetimeArray of times, corresponding to obs_var.\n                nh_radius (float): Neighbourhood radius in km.\n                time_interp (str): Type of time interpolation to use.\nReturns:\n                Tuple[np.ndarray, np.ndarray, np.ndarray]: Array of CRPS values,\n                                Array containing the number of model points used for each CRPS value,\n                                Array of bools indicating where a model neighbourhood contained land.\n","categories":"","description":"Docstrings for the Crps_util class\n","excerpt":"Docstrings for the Crps_util class\n","ref":"/COAsT/docs/reference/crps_util/","tags":"","title":"Crps_util"},{"body":"Objects DocsyTools()\nDocsyTools.write_class_to_markdown()\nDocsyTools._method_to_str()\nDocsyTools._get_list_of_methods()\nA class to help with writting markdown.\nDocsyTools() class DocsyTools(): DocsyTools Class DocsyTools.write_class_to_markdown() @classmethod def DocsyTools.write_class_to_markdown(cls, class_to_write, fn_out, method_to_omit=unknown, omit_private_methods=True, omit_parent_methods=True): None\nDocsyTools._method_to_str() @classmethod def DocsyTools._method_to_str(cls, method_name): None\nDocsyTools._get_list_of_methods() @classmethod def DocsyTools._get_list_of_methods(cls, class_to_search, methods_to_omit=unknown, omit_private_methods=True, omit_parent_methods=True): Method get a list of methods inside a provided COAsT class, with some other options.\nArgs:\n                class_to_search (Type): Class imported from COAsT (e.g. from coast import Profile)\n                methods_to_omit (List): List of method strings to omit from the output. The default is [].\n                omit_private_methods (bool): If true, omit methods beginning with \"_\". The default is True.\n                omit_parent_methods (bool): If true, omit methods in any parent/ancestor class. The default is True.\nReturns:\n                List[str]: List of strings denoting method names.\n","categories":"","description":"Docstrings for the Docsy_tools class\n","excerpt":"Docstrings for the Docsy_tools class\n","ref":"/COAsT/docs/reference/docsy_tools/","tags":"","title":"Docsy_tools"},{"body":"Objects compute_eofs()\ncompute_hilbert_eofs()\n_compute()\nThis is file deals with empirical orthogonal functions.\ncompute_eofs() def compute_eofs(variable, full_matrices=False, time_dim_name=t_dim): Compute some numbers is a helper method.\nComputes the Empirical Orthogonal Functions (EOFs) of a variable (time series)\nthat has 3 dimensions where one is time, i.e. (x,y,time)\nReturns the set of EOF modes, the associated temporal projections and the\nvariance explained by each mode as DataArrays within an xarray Dataset.\nAll-NaN time series, such as those at land points, are handled and ignored;\nhowever, isolated NaNs within a time series, i.e. missing data point, must\nbe filled before calling the function.\nThe variable will be de-meaned in time before the EOFs are computed, normalisation\nshould be carried out before calling the function if desired. The returned EOFs and\ntemporal projections are not scaled or normalised.\nParameters\n----------\nvariable : (xarray.DataArray), 3-dimensional variable of size (I,J,T),\ncontaining I*J time series\nfull_matrices : (boolean, default False) if false computes only first K EOFs\nwhere K=min(I*J,T), where T is total number of time points. Setting to True\ncould demand significant memory.\ntime_dim_name : (string, default 't_dim') the name of the time dimension.\nReturns\n-------\ndataset : xarray Dataset, containing the EOFs, temporal projections and\nvariance explained as xarray DataArrays. The relevent coordinates\nfrom the original data variable are also included\ncompute_hilbert_eofs() def compute_hilbert_eofs(variable, full_matrices=False, time_dim_name=t_dim): Compute with hilbert is a helper method.\nComputes the complex Hilbert Empirical Orthogonal Functions (HEOFs) of a\nvariable (time series) that has 3 dimensions where one is time, i.e. (x,y,time).\nSee https://doi.org/10.1002/joc.1499\nReturns the set of HEOF amplitude and phase modes, the associated temporal\nprojection amplitudes and phases and the variance explained by each mode\nas DataArrays within an xarray Dataset.\nAll-NaN time series, such as those at land points, are handled and ignored;\nhowever, isolated NaNs within a time series, i.e. missing data point, must\nbe filled before calling the function.\nThe variable will be de-meaned in time before the EOFs are computed, normalisation\nshould be carried out before calling the function if desired. The returned EOFs and\ntemporal projections are not scaled or normalised.\nParameters\n----------\nvariable : (xarray.DataArray), 3-dimensional variable of size (I,J,T),\ncontaining I*J time series\nfull_matrices : (boolean, default False) if false computes only first K EOFs\nwhere K=min(I*J,T), where T is total number of time points.\ntime_dim_name : (string, default 't_dim') the name of the time dimension.\nReturns\n-------\ndataset : xarray Dataset, containing the EOF amplitudes and phases,\ntemporal projection amplitude and phases and the variance explained\nas xarray DataArrays. The relevent coordinates\nfrom the original data variable are also in the dataset.\n_compute() def _compute(signal, full_matrices, active_ind, number_points): Private compute method is a helper method.\nCompute eofs, projections and variance explained using a Singular Value Decomposition\nParameters\n----------\nsignal : (array) the signal\nfull_matrices : (boolean) whether to return a full or abbreviated SVD\nactive_ind : (array) indices of points with non-null signal\nnumber_points : (int) number of points in original data set\nReturns\n-------\nEOFs : (array) the EOFs in 2d form\nprojections : (array) the projection of the EOFs\nvariance_explained : (array) variance explained by each mode\nmode_count : (int) number of modes computed\n","categories":"","description":"Docstrings for the Eof class\n","excerpt":"Docstrings for the Eof class\n","ref":"/COAsT/docs/reference/eof/","tags":"","title":"Eof"},{"body":"Objects experiments()\nnemo_filename_maker()\nSet of functions to control basic experiment file handling\nexperiments() def experiments(experiments=experiments.json): Reads a json formatted files, default name is experiments.json\nfor lists of:\nexperiment names (exp_names)\ndirectory names (dir names)\ndomain file names (domains)\nfile names (file_names)\nParameters\n----------\nexperiments : TYPE, optional\n                DESCRIPTION. The default is 'experiments.json'.\nReturns\n-------\nexp_names,dirs,domains,file_names\nnemo_filename_maker() def nemo_filename_maker(directory, year_start, year_stop, grid=T): Creates a list of NEMO file names from a set of standard templates.\nArgs:\n                directory: path to the files'\n                year_start: start year\n                year_stop: stop year\n                grid: NEMO grid type defaults to T\nReturns: a list of possible nemo file names\n","categories":"","description":"Docstrings for the Experiments_file_handling class\n","excerpt":"Docstrings for the Experiments_file_handling class\n","ref":"/COAsT/docs/reference/experiments_file_handling/","tags":"","title":"Experiments_file_handling"},{"body":"Objects determine_season()\nsubset_indices_by_distance_balltree()\nsubset_indices_by_distance()\ncompare_angles()\ncartesian_to_polar()\npolar_to_cartesian()\nsubset_indices_lonlat_box()\ncalculate_haversine_distance()\nremove_indices_by_mask()\nreinstate_indices_by_mask()\nnearest_indices_2d()\ndata_array_time_slice()\nday_of_week()\nnan_helper()\nA general utility file.\ndetermine_season() def determine_season(t): Determine season (or array of seasons) from a time (Datetime or xarray)\nobject. Put in an array of times, get out an array of seasons.\nsubset_indices_by_distance_balltree() def subset_indices_by_distance_balltree(longitude, latitude, centre_lon, centre_lat, radius, mask=None): Returns the indices of points that lie within a specified radius (km) of\ncentral latitude and longitudes. This makes use of BallTree.query_radius.\nParameters\n----------\nlongitude : (numpy.ndarray) longitudes in degrees\nlatitude                : (numpy.ndarray) latitudes in degrees\ncentre_lon : Central longitude. Can be single value or array of values\ncentre_lat : Central latitude. Can be single value or array of values\nradius                : (float) Radius in km within which to find indices\nmask                                : (numpy.ndarray) of same dimension as longitude and latitude.\n                                                If specified, will mask out points from the routine.\nReturns\n-------\n                Returns an array of indices corresponding to points within radius.\n                If more than one central location is specified, this will be a list\n                of index arrays. Each element of which corresponds to one centre.\nIf longitude is 1D:\n                Returns one array of indices per central location\nIf longitude is 2D:\n                Returns arrays of x and y indices per central location.\n                ind_y corresponds to row indices of the original input arrays.\nsubset_indices_by_distance() def subset_indices_by_distance(longitude, latitude, centre_lon, centre_lat, radius): This method returns a `tuple` of indices within the `radius` of the\nlon/lat point given by the user.\nScikit-learn BallTree is used to obtain indices.\n:param longitude: The longitude of the users central point\n:param latitude: The latitude of the users central point\n:param radius: The haversine distance (in km) from the central point\n:return: All indices in a `tuple` with the haversine distance of the\n                                central point\ncompare_angles() def compare_angles(a1, a2, degrees=True): # Compares the difference between two angles. e.g. it is 2 degrees between\n# 359 and 1 degree. If degrees = False then will treat angles as radians.\ncartesian_to_polar() def cartesian_to_polar(x, y, degrees=True): # Conversion of cartesian to polar coordinate system\n# Output theta is in radians\npolar_to_cartesian() def polar_to_cartesian(r, theta, degrees=True): # Conversion of polar to cartesian coordinate system\n# Input theta must be in radians\nsubset_indices_lonlat_box() def subset_indices_lonlat_box(array_lon, array_lat, lon_min, lon_max, lat_min, lat_max): None\ncalculate_haversine_distance() def calculate_haversine_distance(lon1, lat1, lon2, lat2): # Estimation of geographical distance using the Haversine function.\n# Input can be single values or 1D arrays of locations. This\n# does NOT create a distance matrix but outputs another 1D array.\n# This works for either location vectors of equal length OR a single loc\n# and an arbitrary length location vector.\n#\n# lon1, lat1 :: Location(s) 1.\n# lon2, lat2 :: Location(s) 2.\nremove_indices_by_mask() def remove_indices_by_mask(array, mask): Removes indices from a 2-dimensional array, A, based on true elements of\nmask. A and mask variable should have the same shape.\nreinstate_indices_by_mask() def reinstate_indices_by_mask(array_removed, mask, fill_value=unknown): Rebuilds a 2D array from a 1D array created using remove_indices_by_mask().\nFalse elements of mask will be populated using array_removed. MAsked\nindices will be replaced with fill_value\nnearest_indices_2d() def nearest_indices_2d(mod_lon, mod_lat, new_lon, new_lat, mask=None): Obtains the 2 dimensional indices of the nearest model points to specified\nlists of longitudes and latitudes. Makes use of sklearn.neighbours\nand its BallTree haversine method. Ensure there are no NaNs in\ninput longitude/latitude arrays (or mask them using \"mask\"\")\nExample Usage\n----------\n# Get indices of model points closest to altimetry points\nind_x, ind_y = nemo.nearest_indices(altimetry.dataset.longitude,\n                                                                                                                                                altimetry.dataset.latitude)\n# Nearest neighbour interpolation of model dataset to these points\ninterpolated = nemo.dataset.isel(x_dim = ind_x, y_dim = ind_y)\nParameters\n----------\nmod_lon (2D array): Model longitude (degrees) array (2-dimensional)\nmod_lat (2D array): Model latitude (degrees) array (2-dimensions)\nnew_lon (1D array): Array of longitudes (degrees) to compare with model\nnew_lat (1D array): Array of latitudes (degrees) to compare with model\nmask (2D array): Mask array. Where True (or 1), elements of array will\n                                                                not be included. For example, use to mask out land in\n                                                                case it ends up as the nearest point.\nReturns\n-------\nArray of x indices, Array of y indices\ndata_array_time_slice() def data_array_time_slice(data_array, date0, date1): Takes an xr.DataArray object and returns a new object with times\nsliced between dates date0 and date1. date0 and date1 may be a string or\ndatetime type object.\nday_of_week() def day_of_week(date=None): Return the day of the week (3 letter str)\nnan_helper() def nan_helper(y): Helper to handle indices and logical indices of NaNs.\nInput:\n                - y, 1d numpy array, or xr.DataArray, with possible NaNs\nOutput:\n                - nans, logical indices of NaNs\n                - index, a function, with signature indices= index(logical_indices),\n                to convert logical indices of NaNs to 'equivalent' indices\nExample:\n                \u003e\u003e\u003e # linear interpolation of NaNs\n                \u003e\u003e\u003e nans, x= nan_helper(y)\n                \u003e\u003e\u003e y[nans]= np.interp(x(nans), x(~nans), y[~nans])\n","categories":"","description":"Docstrings for the General_utils class\n","excerpt":"Docstrings for the General_utils class\n","ref":"/COAsT/docs/reference/general_utils/","tags":"","title":"General_utils"},{"body":"Objects Glider()\nGlider.load_single()\nGlider class\nGlider() class Glider(Indexed): Glider class for reading in glider data (netcdf format) into an xarray object. Glider.load_single() def Glider.load_single(self, file_path, chunks=None): Loads a single file into object's dataset variable.\nArgs:\n                file_path (str): path to data file\n                chunks (dict): chunks\n","categories":"","description":"Docstrings for the Glider class\n","excerpt":"Docstrings for the Glider class\n","ref":"/COAsT/docs/reference/glider/","tags":"","title":"Glider"},{"body":"Objects Gridded()\nGridded._setup_grid_obj()\nGridded.make_lonLat_2d()\nGridded.set_grid_vars()\nGridded.load_domain()\nGridded.merge_domain_into_dataset()\nGridded.set_grid_ref_attr()\nGridded.get_contour_complex()\nGridded.set_timezero_depths()\nGridded.calc_bathymetry()\nGridded.subset_indices()\nGridded.find_j_i()\nGridded.find_j_i_list()\nGridded.find_j_i_domain()\nGridded.transect_indices()\nGridded.interpolate_in_space()\nGridded.interpolate_in_time()\nGridded.construct_density()\nGridded.trim_domain_size()\nGridded.copy_domain_vars_to_dataset()\nGridded.differentiate()\nGridded.apply_doodson_x0_filter()\nGridded.get_e3_from_ssh()\nGridded.harmonics_combine()\nGridded.harmonics_convert()\nGridded.time_slice()\nGridded.calculate_vertical_mask()\nGridded class\nGridded() class Gridded(Coast): Words to describe the NEMO class kwargs -- define addition keyworded arguemts for domain file. E.g. ln_sco=1 if using s-scoord in an old domain file that does not carry this flag. Gridded._setup_grid_obj() def Gridded._setup_grid_obj(self, chunks, multiple, **kwargs): This is a helper method to reduce the size of def __init__\nArgs:\n                chunks: This is a setting for xarray as to whether dask (parrell processing) should be on and how it works\n                multiple: falg to tell if we are loading one or more files\n                **kwargs: pass direct to loaded xarray dataset\nGridded.make_lonLat_2d() def Gridded.make_lonLat_2d(self): Expand 1D latitude and longitude variables to 2D.\nGridded.set_grid_vars() def Gridded.set_grid_vars(self): Define the variables to map from the domain file to the NEMO obj\nGridded.load_domain() def Gridded.load_domain(self, fn_domain, chunks): Loads domain file and renames dimensions with dim_mapping_domain\nGridded.merge_domain_into_dataset() def Gridded.merge_domain_into_dataset(self, dataset_domain): Merge domain dataset variables into self.dataset, using grid_ref\nGridded.set_grid_ref_attr() def Gridded.set_grid_ref_attr(self): None\nGridded.get_contour_complex() def Gridded.get_contour_complex(self, var, points_x, points_y, points_z, tolerance=0.2): None\nGridded.set_timezero_depths() def Gridded.set_timezero_depths(self, dataset_domain, calculate_bathymetry=False): Calculates the depths at time zero (from the domain_cfg input file)\nfor the appropriate grid.\nThe depths are assigned to domain_dataset.depth_0\nArgs:\n                dataset_domain: a complex data object.\n                calculate_bathymetry: Flag that will either calculate bathymetry (true) or load it from dataset_domain file\n                (false).\nGridded.calc_bathymetry() def Gridded.calc_bathymetry(self, dataset_domain): NEMO approach to defining bathymetry by summing scale factors at various\ngrid locations.\nWorks with z-coordinates on u- and v- faces where bathymetry is defined\nat the top of the cliff, not at the bottom\nArgs:\n                dataset_domain: a complex data object.\nGridded.subset_indices() def Gridded.subset_indices(self): based on transect_indices, this method looks to return all indices between the given points.\nThis results in a 'box' (Quadrilateral) of indices.\nconsequently the returned lists may have different lengths.\n:param start: A lat/lon pair\n:param end: A lat/lon pair\n:return: list of y indices, list of x indices,\nGridded.find_j_i() def Gridded.find_j_i(self): A routine to find the nearest y x coordinates for a given latitude and longitude\nUsage: [y,x] = find_j_i(lat=49, lon=-12)\n:param lat: latitude\n:param lon: longitude\n:return: the y and x coordinates for the NEMO object's grid_ref, i.e. t,u,v,f,w.\nGridded.find_j_i_list() def Gridded.find_j_i_list(self): A routine to find the nearest y x coordinates for a list of latitude and longitude values\nUsage: [y,x] = find_j_i(lat=[49,50,51], lon=[-12,-11,10])\n:param lat: latitude\n:param lon: longitude\n:optional n_nn=1 number of nearest neighbours\n:return: the j, i coordinates for the NEMO object's grid_ref, i.e. t,u,v,f,w. and a distance measure\nGridded.find_j_i_domain() def Gridded.find_j_i_domain(self): A routine to find the nearest y x coordinates for a given latitude and longitude\nUsage: [y,x] = find_j_i_domain(lat=49, lon=-12, dataset_domain=dataset_domain)\n:param lat: latitude\n:param lon: longitude\n:param dataset_domain: dataset domain\n:return: the y and x coordinates for the grid_ref variable within the domain file\nGridded.transect_indices() def Gridded.transect_indices(self, start, end): This method returns the indices of a simple straight line transect between two\nlat lon points defined on the NEMO object's grid_ref, i.e. t,u,v,f,w.\n:type start: tuple A lat/lon pair\n:type end: tuple A lat/lon pair\n:return: array of y indices, array of x indices, number of indices in transect\nGridded.interpolate_in_space() @staticmethod def Gridded.interpolate_in_space(model_array, new_lon, new_lat, mask=None): Interpolates a provided xarray.DataArray in space to new longitudes\nand latitudes using a nearest neighbour method (BallTree).\nExample Usage\n----------\n# Get an interpolated DataArray for temperature onto two locations\ninterpolated = nemo.interpolate_in_space(nemo.dataset.votemper,\n                                                                                                                                                                [0,1], [45,46])\nParameters\n----------\nmodel_array (xr.DataArray): Model variable DataArray to interpolate\nnew_lons (1Darray): Array of longitudes (degrees) to compare with model\nnew_lats (1Darray): Array of latitudes (degrees) to compare with model\nmask (2D array): Mask array. Where True (or 1), elements of array will\n                                                not be included. For example, use to mask out land in\n                                                case it ends up as the nearest point.\nReturns\n-------\nInterpolated DataArray\nGridded.interpolate_in_time() @staticmethod def Gridded.interpolate_in_time(model_array, new_times, interp_method=nearest, extrapolate=True): Interpolates a provided xarray.DataArray in time to new python\ndatetimes using a specified scipy.interpolate method.\nExample Useage\n----------\n# Get an interpolated DataArray for temperature onto altimetry times\nnew_times = altimetry.dataset.time\ninterpolated = nemo.interpolate_in_space(nemo.dataset.votemper,\n                                                                                                                                                                new_times)\nParameters\n----------\nmodel_array (xr.DataArray): Model variable DataArray to interpolate\nnew_times (array): New times to interpolate to (array of datetimes)\ninterp_method (str): Interpolation method\nReturns\n-------\nInterpolated DataArray\nGridded.construct_density() def Gridded.construct_density(self, eos=EOS10, rhobar=False, Zd_mask=unknown, CT_AS=False, pot_dens=False, Tbar=True, Sbar=True):                 Constructs the in-situ density using the salinity, temperture and\n                depth_0 fields and adds a density attribute to the t-grid dataset\n                Requirements: The supplied t-grid dataset must contain the\n                Practical Salinity and the Potential Temperature variables. The depth_0\n                field must also be supplied. The GSW package is used to calculate\n                The Absolute Pressure, Absolute Salinity and Conservate Temperature.\n                Note that currently density can only be constructed using the EOS10\n                equation of state.\nParameters\n----------\neos : equation of state, optional\n                DESCRIPTION. The default is 'EOS10'.\nrhobar : Calculate density with depth mean T and S\n                DESCRIPTION. The default is 'False'.\nZd_mask : Provide a 3D mask for rhobar calculation\n                Calculate using calculate_vertical_mask\n                DESCRIPTION. The default is empty.\nCT_AS : Conservative Temperature and Absolute Salinity already provided\n                DESCRIPTION. The default is 'False'.\npot_dens :Calculation at zero pressure\n                DESCRIPTION. The default is 'False'.\nTbar and Sbar : If rhobar is True then these can be switch to False to allow one component to\n                                                                remain depth varying. So Tbar=Flase gives temperature component, Sbar=Flase gives Salinity component\n                DESCRIPTION. The default is 'True'.\nReturns\n-------\nNone.\nadds attribute NEMO.dataset.density\nGridded.trim_domain_size() def Gridded.trim_domain_size(self, dataset_domain): Trim the domain variables if the dataset object is a spatial subset\nNote: This breaks if the SW \u0026 NW corner values of nav_lat and nav_lon\nare masked, as can happen if on land...\nGridded.copy_domain_vars_to_dataset() def Gridded.copy_domain_vars_to_dataset(self, dataset_domain, grid_vars): Map the domain coordinates and metric variables to the dataset object.\nExpects the source and target DataArrays to be same sizes.\nGridded.differentiate() def Gridded.differentiate(self, in_var_str, config_path=None, dim=z_dim, out_var_str=None, out_obj=None): Derivatives are computed in x_dim, y_dim, z_dim (or i,j,k) directions\nwrt lambda, phi, or z coordinates (with scale factor in metres not degrees).\nDerivatives are calculated using the approach adopted in NEMO,\nspecifically using the 1st order accurate central difference\napproximation. For reference see section 3.1.2 (sec. Discrete operators)\nof the NEMO v4 Handbook.\nCurrently the method does not accomodate all possible eventualities. It\ncovers:\n1) d(grid_t)/dz --\u003e grid_w\nReturns an object (with the appropriate target grid_ref) containing\nderivative (out_var_str) as xr.DataArray\nThis is hardwired to expect:\n1) depth_0 and e3_0 fields exist\n2) xr.DataArrays are 4D\n3) self.filename_domain if out_obj not specified\n4) If out_obj is not specified, one is built that is the size of\n                self.filename_domain. I.e. automatic subsetting of out_obj is not\n                supported.\nExample usage:\n--------------\n# Initialise DataArrays\nnemo_t = coast.NEMO( fn_data, fn_domain, grid_ref='t-grid' )\n# Compute dT/dz\nnemo_w_1 = nemo_t.differentiate( 'temperature', dim='z_dim' )\n# For f(z)=-z. Compute df/dz = -1. Surface value is set to zero\nnemo_t.dataset['depth4D'],_ = xr.broadcast( nemo_t.dataset['depth_0'], nemo_t.dataset['temperature'] )\nnemo_w_4 = nemo_t.differentiate( 'depth4D', dim='z_dim', out_var_str='dzdz' )\nProvide an existing target NEMO object and target variable name:\nnemo_w_1 = nemo_t.differentiate( 'temperature', dim='z_dim', out_var_str='dTdz', out_obj=nemo_w_1 )\nParameters\n----------\nin_var_str : str, name of variable to differentiate\nconfig_path : str, path to the w grid config file\ndim : str, dimension to operate over. E.g. {'z_dim', 'y_dim', 'x_dim', 't_dim'}\nout_var_str : str, (optional) name of the target xr.DataArray\nout_obj : exiting NEMO obj to store xr.DataArray (optional)\nGridded.apply_doodson_x0_filter() def Gridded.apply_doodson_x0_filter(self, var_str): Applies Doodson X0 filter to a variable.\nInput variable is expected to be hourly.\nOutput is saved back to original dataset as {var_str}_dxo\n!!WARNING: Will load in entire variable to memory. If dataset large,\nthen subset before using this method or ensure you have enough free\nRAM to hold the variable (twice).\nDB:: Currently not tested in unit_test.py\nGridded.get_e3_from_ssh() @staticmethod def Gridded.get_e3_from_ssh(nemo_t, e3t=True, e3u=False, e3v=False, e3f=False, e3w=False, dom_fn=None): Where the model has been run with a nonlinear free surface\nand z* variable volumne (ln_vvl_zstar=True) then the vertical scale factors\nwill vary in time (and space). This function will compute the vertical\nscale factors e3t, e3u, e3v, e3f and e3w by using the sea surface height\nfield (ssh variable) and initial scale factors from the domain_cfg file.\nThe vertical scale factors will be computed at the same model time as the\nssh and if the ssh field is averaged in time then the scale factors will\nalso be time averages.\nA t-grid NEMO object containing the ssh variable must be passed in. Either\nthe domain_cfg path must have been passed in as an argument when the NEMO\nobject was created or it must be passed in here using the dom_fn argument.\ne.g. e3t,e3v,e3f = coast.NEMO.get_e3_from_ssh(nemo_t,true,false,true,true,false)\nParameters\n----------\nnemo_t : (Coast.NEMO), NEMO object on the t-grid containing the ssh variable\ne3t : (boolean), true if e3t is to be returned. Default True.\ne3u : (boolean), true if e3u is to be returned. Default False.\ne3v : (boolean), true if e3v is to be returned. Default False.\ne3f : (boolean), true if e3f is to be returned. Default False.\ne3w : (boolean), true if e3w is to be returned. Default False.\ndom_fn : (str), Optional, path to domain_cfg file.\nReturns\n-------\nTuple of xarray.DataArrays\n(e3t, e3u, e3v, e3f, e3w)\nOnly those requested will be returned, but the ordering is always the same.\nGridded.harmonics_combine() def Gridded.harmonics_combine(self, constituents, components=unknown): Contains a new NEMO object containing combined harmonic information\nfrom the original object.\nNEMO saves harmonics to individual variables such as M2x, M2y... etc.\nThis routine will combine these variables (depending on constituents)\ninto a single data array. This new array will have the new dimension\n'constituent' and a new data coordinate 'constituent_name'.\nParameters\n----------\nconstituents : List of strings containing constituent names to combine.\n                                                The case of these strings should match that used in\n                                                NEMO output. If a constituent is not found, no problem,\n                                                it just won't be in the combined dataset.\ncomponents : List of strings containing harmonic components to look\n                                                for. By default, this looks for the complex components\n                                                'x' and 'y'. E.g. if constituents = ['M2'] and\n                                                components is left as default, then the routine looks\n                                                for ['M2x', and 'M2y'].\nReturns\n-------\nNEMO() object, containing combined harmonic variables in a new dataset.\nGridded.harmonics_convert() def Gridded.harmonics_convert(self, direction=cart2polar, x_var=harmonic_x, y_var=harmonic_y, a_var=harmonic_a, g_var=harmonic_g, degrees=True): Converts NEMO harmonics from cartesian to polar or vice versa.\nMake sure this NEMO object contains combined harmonic variables\nobtained using harmonics_combine().\n*Note:\nParameters\n----------\ndirection (str) : Choose 'cart2polar' or 'polar2cart'. If 'cart2polar'\n                                                                Then will look for variables x_var and y_var. If\n                                                                polar2cart, will look for a_var (amplitude) and\n                                                                g_var (phase).\nx_var (str)                : Harmonic x variable name in dataset (or output)\n                                                                default = 'harmonic_x'.\ny_var (str)                : Harmonic y variable name in dataset (or output)\n                                                                default = 'harmonic_y'.\na_var (str)                : Harmonic amplitude variable name in dataset (or output)\n                                                                default = 'harmonic_a'.\ng_var (str)                : Harmonic phase variable name in dataset (or output)\n                                                                default = 'harmonic_g'.\ndegrees (bool) : Whether input/output phase are/will be in degrees.\n                                                                Default is True.\nReturns\n-------\nModifies NEMO() dataset in place. New variables added.\nGridded.time_slice() def Gridded.time_slice(self, date0, date1): Return new Gridded object, indexed between dates date0 and date1\nGridded.calculate_vertical_mask() def Gridded.calculate_vertical_mask(self, Zmax): Calculates a 3D mask to a specified level Zmax. 1 for sea; 0 for below sea bed\nand linearly ramped for last level\n","categories":"","description":"Docstrings for the Gridded class\n","excerpt":"Docstrings for the Gridded class\n","ref":"/COAsT/docs/reference/gridded/","tags":"","title":"Gridded"},{"body":"Objects setup_dask_client()\nIndexed()\nIndexed.apply_config_mappings()\nIndexed.insert_dataset()\nIndex class.\nsetup_dask_client() def setup_dask_client(workers=2, threads=2, memory_limit_per_worker=2GB): None\nIndexed() class Indexed(Coast): None Indexed.apply_config_mappings() def Indexed.apply_config_mappings(self): Applies json configuration and mappings\nIndexed.insert_dataset() def Indexed.insert_dataset(self, dataset, apply_config_mappings=False): Insert a dataset straight into this object instance\n","categories":"","description":"Docstrings for the Index class\n","excerpt":"Docstrings for the Index class\n","ref":"/COAsT/docs/reference/index_class/","tags":"","title":"Index"},{"body":"Objects Lagrangian()\nLagrangian class\nLagrangian() class Lagrangian(Indexed): Parent class for subclasses OCEANPARCELS ... Common methods .... ","categories":"","description":"Docstrings for the Lagrangian class\n","excerpt":"Docstrings for the Lagrangian class\n","ref":"/COAsT/docs/reference/lagrangian/","tags":"","title":"Lagrangian"},{"body":"Objects get_logger()\ncreate_handler()\nsetup_logging()\nget_slug()\nget_source()\nadd_info()\ndebug()\ninfo()\nwarning()\nwarn()\nerror()\nA logging unilty file\nget_logger() def get_logger(name=None, level=unknown): None\ncreate_handler() def create_handler(logger, stream=unknown, format_string=unknown): None\nsetup_logging() def setup_logging(name=None, level=unknown, stream=unknown, format_string=unknown): None\nget_slug() def get_slug(obj): None\nget_source() def get_source(level=1): None\nadd_info() def add_info(msg, level=3): None\ndebug() def debug(msg, *args, **kwargs): None\ninfo() def info(msg, *args, **kwargs): None\nwarning() def warning(msg, *args, **kwargs): None\nwarn() def warn(msg, *args, **kwargs): None\nerror() def error(msg, *args, **kwargs): None\n","categories":"","description":"Docstrings for the Logging_util class\n","excerpt":"Docstrings for the Logging_util class\n","ref":"/COAsT/docs/reference/logging_util/","tags":"","title":"Logging_util"},{"body":"Objects MaskMaker()\nMaskMaker.make_mask_dataset()\nMaskMaker.fill_polygon_by_index()\nMaskMaker.fill_polygon_by_lonlat()\nMaskMaker.region_def_nws_north_sea()\nMaskMaker.region_def_nws_outer_shelf()\nMaskMaker.region_def_nws_norwegian_trench()\nMaskMaker.region_def_nws_english_channel()\nMaskMaker.region_def_south_north_sea()\nMaskMaker.region_def_off_shelf()\nMaskMaker.region_def_irish_sea()\nMaskMaker.region_def_kattegat()\nMaskMaker.make_region_from_vertices()\nMaskMaker.quick_plot()\nMask maker\nMaskMaker() class MaskMaker(): MaskMasker is a class of methods to assist with making regional masks within COAsT. Presently these masks are external to MaskMaker. It constructs a gridded boolean numpy array for each region, which are stacked over a dim_mask dimension and stored as an xarray object. A typical workflow might be: # Define vertices vertices_lon = [-5, -5, 5, 5] vertices_lat = [40, 60, 60, 40] # input lat/lon as xr.DataArray or numpy arrays. Return gridded boolean mask np.array on target grid filled = mm.make_region_from_vertices( sci.dataset.longitude, sci.dataset.latitude, vertices_lon, vertices_lat) # make xr.Dataset of masks from gridded mask array or list of mask arrays gridded_mask = mm.make_mask_dataset(sci.dataset.longitude.values, sci.dataset.latitude.values, filled) # quick plot mm.quick_plot(gridded_mask) TO DO: * Sort out region naming to be consistently applied and associated with the masks E.g. defined regions, or user defined masks * Create final mask as a xr.DataArray, not a xr.Dataset MaskMaker.make_mask_dataset() @staticmethod def MaskMaker.make_mask_dataset(longitude, latitude, mask_list, mask_names=None): create xr.Dataset for mask with latitude and longitude coordinates. If mask_names are given\ncreate a dim_mask coordinate of names\nMaskMaker.fill_polygon_by_index() @staticmethod def MaskMaker.fill_polygon_by_index(array_to_fill, vertices_r, vertices_c, fill_value=1, additive=False): Draws and fills a polygon onto an existing numpy array based on array\nindices. To create a new mask, give np.zeros(shape) as input.\nPolygon vertices are drawn in the order given.\nParameters\n----------\narray_to_fill (2D array): Array onto which to fill polygon\nvertices_r (1D array): Row indices for polygon vertices\nvertices_c (1D_array): Column indices for polygon vertices\nfill_value (float, bool or int): Fill value for polygon (Default: 1)\nadditive (bool): If true, add fill value to existing array. Otherwise\n                                                                indices will be overwritten. (Default: False)\nReturns\n-------\nFilled 2D array\nMaskMaker.fill_polygon_by_lonlat() @staticmethod def MaskMaker.fill_polygon_by_lonlat(array_to_fill, longitude, latitude, vertices_lon, vertices_lat, fill_value=1, additive=False): Draws and fills a polygon onto an existing numpy array based on\nvertices defined by longitude and latitude locations. This does NOT\ndraw a polygon on a sphere, but instead based on straight lines\nbetween points. This is OK for small regional areas, but not advisable\nfor large and global regions.\nPolygon vertices are drawn in the order given.\nParameters\n----------\narray_to_fill (2D array): Array onto which to fill polygon\nvertices_r (1D array): Row indices for polygon vertices\nvertices_c (1D_array): Column indices for polygon vertices\nfill_value (float, bool or int): Fill value for polygon (Default: 1)\nadditive (bool): If true, add fill value to existing array. Otherwise\n                                                                indices will be overwritten. (Default: False)\nReturns\n-------\nFilled 2D np.array\nMaskMaker.region_def_nws_north_sea() @classmethod def MaskMaker.region_def_nws_north_sea(cls, longitude, latitude, bath): Regional definition for the North Sea (Northwest European Shelf)\nLongitude, latitude and bath should be 2D arrays corresponding to model\ncoordinates and bathymetry. Bath should be positive with depth.\nMaskMaker.region_def_nws_outer_shelf() @classmethod def MaskMaker.region_def_nws_outer_shelf(cls, longitude, latitude, bath): Regional definition for the Outer Shelf (Northwest European Shelf)\nLongitude, latitude and bath should be 2D arrays corresponding to model\ncoordinates and bathymetry. Bath should be positive with depth.\nMaskMaker.region_def_nws_norwegian_trench() @classmethod def MaskMaker.region_def_nws_norwegian_trench(cls, longitude, latitude, bath): Regional definition for the Norwegian Trench (Northwest European Shelf)\nLongitude, latitude and bath should be 2D arrays corresponding to model\ncoordinates and bathymetry. Bath should be positive with depth.\nMaskMaker.region_def_nws_english_channel() @classmethod def MaskMaker.region_def_nws_english_channel(cls, longitude, latitude, bath): Regional definition for the English Channel (Northwest European Shelf)\nLongitude, latitude and bath should be 2D arrays corresponding to model\ncoordinates and bathymetry. Bath should be positive with depth.\nMaskMaker.region_def_south_north_sea() @classmethod def MaskMaker.region_def_south_north_sea(cls, longitude, latitude, bath): None\nMaskMaker.region_def_off_shelf() @classmethod def MaskMaker.region_def_off_shelf(cls, longitude, latitude, bath): None\nMaskMaker.region_def_irish_sea() @classmethod def MaskMaker.region_def_irish_sea(cls, longitude, latitude, bath): None\nMaskMaker.region_def_kattegat() @classmethod def MaskMaker.region_def_kattegat(cls, longitude, latitude, bath): None\nMaskMaker.make_region_from_vertices() @classmethod def MaskMaker.make_region_from_vertices(cls, longitude, latitude, vertices_lon, vertices_lat): Construct mask on supplied longitude, latitude grid with input lists of lon and lat polygon vertices\n:param longitude: np.array/xr.DataArray of longitudes on target grid\n:param latitude: np.array/xr.DataArray of latitudes on target grid\n:param vertices_lon: list of vertices for bounding polygon\n:param vertices_lat: list of vertices for bounding polygon\n:return: mask: np.array(boolean) on target grid. Ones are bound by polygon vertices\nMaskMaker.quick_plot() @classmethod def MaskMaker.quick_plot(cls, mask): Plot a map of masks in the MaskMaker object\nAdd labels\n","categories":"","description":"Docstrings for the Mask_maker class\n","excerpt":"Docstrings for the Mask_maker class\n","ref":"/COAsT/docs/reference/mask_maker/","tags":"","title":"Mask_maker"},{"body":"Objects Oceanparcels()\nOceanparcels.load_single()\nOceanparcels class for reading ocean parcels data.\nOceanparcels() class Oceanparcels(Lagrangian): Reading ocean parcels data (netcdf format) into an xarray object. Oceanparcels.load_single() def Oceanparcels.load_single(self, file_path): Loads a single file into object's dataset variable.\nArgs:\n                file_path (str): path to data file\n","categories":"","description":"Docstrings for the Oceanparcels class\n","excerpt":"Docstrings for the Oceanparcels class\n","ref":"/COAsT/docs/reference/oceanparcels/","tags":"","title":"Oceanparcels"},{"body":"Objects OpendapInfo()\nOpendapInfo.get_store()\nOpendapInfo.open_dataset()\nOpendapInfo.from_cas()\nFunctionality for accessing OPeNDAP datasets.\nOpendapInfo() class OpendapInfo(): A class for accessing streamable OPeNDAP data. OpendapInfo.get_store() def OpendapInfo.get_store(self): Access an OPeNDAP data store.\nReturns:\n                The OPeNDAP data store accessed from the instance's URL.\nOpendapInfo.open_dataset() def OpendapInfo.open_dataset(self, chunks=None): Open the remote XArray dataset for streaming.\nArgs:\n                chunks: Chunks to use in Dask.\nReturns:\n                The opened XArray dataset.\nOpendapInfo.from_cas() @classmethod def OpendapInfo.from_cas(cls, url, cas_url, username, password): Instantiate OpendapInfo with a session authenticated against CAS.\nArgs:\n                url: The OPeNDAP dataset URL.\n                cas_url: The CAS login URL.\n                username: The username to authenticate with.\n                password: The password to authenticate with.\nReturns:\n                The instantiated OPeNDAP accessor.\n","categories":"","description":"Docstrings for the Opendap class\n","excerpt":"Docstrings for the Opendap class\n","ref":"/COAsT/docs/reference/opendap/","tags":"","title":"Opendap"},{"body":"Objects r2_lin()\nscatter_with_fit()\ncreate_geo_subplots()\ncreate_geo_axes()\nts_diagram()\ngeo_scatter()\ndetermine_colorbar_extension()\ndetermine_clim_by_standard_deviation()\nPython definitions used to help with plotting routines.\nMethods Overview -\u003e geo_scatter(): Geographical scatter plot.\nr2_lin() def r2_lin(x, y, fit): For calculating r-squared of a linear fit. Fit should be a python polyfit object.\nscatter_with_fit() def scatter_with_fit(x, y, s=10, c=k, yex=True, dofit=True): Does a scatter plot with a linear fit. Will also draw y=x for\ncomparison.\nParameters\n----------\nx                : (array) Values for the x-axis\ny                : (array) Values for the y-axis\ns                : (float or array) Marker size(s)\nc                : (float or array) Marker colour(s)\nyex : (bool) True to plot y=x\ndofit : (bool) True to calculate and plot linear fit\nReturns\n-------\nFigure and axis objects for further customisation\nExample Useage\n-------\nx = np.arange(0,50)\ny = np.arange(0,50)/1.5\nf,a = scatter_with_fit(x,y)\na.set_title('Example scatter with fit')\na.set_xlabel('Example x axis')\na.set_ylabel('Example y axis')\ncreate_geo_subplots() def create_geo_subplots(lonbounds, latbounds, n_r=1, n_c=1, figsize=unknown): A routine for creating an axis for any geographical plot. Within the\nspecified longitude and latitude bounds, a map will be drawn up using\ncartopy. Any type of matplotlib plot can then be added to this figure.\nFor example:\nExample Useage\n#############\n                f,a = create_geo_axes(lonbounds, latbounds)\n                sca = a.scatter(stats.longitude, stats.latitude, c=stats.corr,\n                                                                                vmin=.75, vmax=1,\n                                                                                edgecolors='k', linewidths=.5, zorder=100)\n                f.colorbar(sca)\n                a.set_title('SSH correlations Monthly PSMSL tide gauge vs CO9_AMM15p0',\n                                                                fontsize=9)\n* Note: For scatter plots, it is useful to set zorder = 100 (or similar\n                                positive number)\ncreate_geo_axes() def create_geo_axes(lonbounds, latbounds): A routine for creating an axis for any geographical plot. Within the\nspecified longitude and latitude bounds, a map will be drawn up using\ncartopy. Any type of matplotlib plot can then be added to this figure.\nFor example:\nExample Useage\n#############\n                f,a = create_geo_axes(lonbounds, latbounds)\n                sca = a.scatter(stats.longitude, stats.latitude, c=stats.corr,\n                                                                                vmin=.75, vmax=1,\n                                                                                edgecolors='k', linewidths=.5, zorder=100)\n                f.colorbar(sca)\n                a.set_title('SSH correlations Monthly PSMSL tide gauge vs CO9_AMM15p0',\n                                                                fontsize=9)\n* Note: For scatter plots, it is useful to set zorder = 100 (or similar\n                                positive number)\nts_diagram() def ts_diagram(temperature, salinity, depth): None\ngeo_scatter() def geo_scatter(longitude, latitude, c=None, s=None, scatter_kwargs=None, coastline_kwargs=None, gridline_kwargs=None, figure_kwargs=unknown, title=, figsize=None): Uses CartoPy to create a geographical scatter plot with land boundaries.\n                Parameters\n                ----------\n                longitude : (array) Array of longitudes of marker locations\n                latitude : (array) Array of latitudes of marker locations\n                colors                : (array) Array of values to use for colouring markers\n                title                : (str) Plot title, to appear at top of figure\n                xlim                : (tuple) Tuple of limits to apply to the x-axis (longitude axis)\n                ylim                : (tuple) Limits to apply to the y-axis (latitude axis)\n                Returns\n                -------\n                Figure and axis objects for further customisation\ndetermine_colorbar_extension() def determine_colorbar_extension(color_data, vmin, vmax): Can be used to automatically determine settings for colorbar\nextension arrows. Color_data is the data used for the colormap, vmin\nand vmax are the colorbar limits. Will output a string: \"both\", \"max\",\n\"min\" or \"neither\", which can be inserted straight into a call to\nmatplotlib.pyplot.colorbar().\ndetermine_clim_by_standard_deviation() def determine_clim_by_standard_deviation(color_data, n_std_dev=2.5): Automatically determine color limits based on number of standard\ndeviations from the mean of the color data (color_data). Useful if there\nare outliers in the data causing difficulties in distinguishing most of\nthe data. Outputs vmin and vmax which can be passed to plotting routine\nor plt.clim().\n","categories":"","description":"Docstrings for the Plot_util class\n","excerpt":"Docstrings for the Plot_util class\n","ref":"/COAsT/docs/reference/plot_util/","tags":"","title":"Plot_util"},{"body":"Objects Profile()\nProfile.read_en4()\nProfile.read_wod()\nProfile.subset_indices_lonlat_box()\nProfile.plot_profile()\nProfile.plot_map()\nProfile.plot_ts_diagram()\nProfile.process_en4()\nProfile.calculate_all_en4_qc_flags()\nProfile.obs_operator()\nProfile.reshape_2d()\nProfile.time_slice()\nProfile Class\nProfile() class Profile(Indexed): INDEXED type class for storing data from a CTD Profile (or similar down and up observations). The structure of the class is based around having discrete profile locations with independent depth dimensions and coords. The class dataset should contain two dimensions: \u003e id_dim :: The profiles dimension. Each element of this dimension contains data (e.g. cast) for an individual location. \u003e z_dim :: The dimension for depth levels. A profile object does not need to have shared depths, so NaNs might be used to pad any depth array. Alongside these dimensions, the following minimal coordinates should also be available: \u003e longitude (id_dim) :: 1D array of longitudes, one for each id_dim \u003e latitude (id_dim) :: 1D array of latitudes, one for each id_dim \u003e time (id_dim) :: 1D array of times, one for each id_dim \u003e depth (id_dim, z_dim) :: 2D array of depths, with different depth levels being provided for each profile. Note that these depth levels need to be stored in a 2D array, so NaNs can be used to pad out profiles with shallower depths. \u003e id_name (id_dim) :: [Optional] Name of id_dim/case or id_dim number. You may create an empty profile object by using profile = coast.Profile(). You may then add your own dataset to the object profile or use one of the functions within Profile() for reading common profile datasets: \u003e read_en4() \u003e read_wod() Optionally, you may pass a dataset to the Profile object on creation: profile = coast.Profile(dataset = profile_dataset) A config file can also be provided, in which case any netcdf read functions will rename dimensions and variables as dictated. Profile.read_en4() def Profile.read_en4(self, fn_en4, chunks=unknown, multiple=False): Reads a single or multiple EN4 netCDF files into the COAsT profile\ndata structure.\nParameters\n----------\nfn_en4 : TYPE\n                path to data file.\nchunks : dict, optional\n                Chunking specification\nmultiple : TYPE, optional\n                True if reading multiple files otherwise False\nReturns\n-------\nNone. Populates dataset within Profile object.\nProfile.read_wod() def Profile.read_wod(self, fn_wod, chunks=unknown): Reads a single World Ocean Database netCDF files into the COAsT profile data structure.\nArgs:\n                fn_wod (str): path to data file\n                chunks (dict): chunks\nProfile.subset_indices_lonlat_box() def Profile.subset_indices_lonlat_box(self, lonbounds, latbounds): Get a subset of this Profile() object in a spatial box.\nlonbounds -- Array of form [min_longitude=-180, max_longitude=180]\nlatbounds -- Array of form [min_latitude, max_latitude]\nreturn: A new profile object containing subsetted data\nProfile.plot_profile() def Profile.plot_profile(self, var, profile_indices=None): None\nProfile.plot_map() def Profile.plot_map(self, var_str=None): None\nProfile.plot_ts_diagram() def Profile.plot_ts_diagram(self, profile_index, var_t=potential_temperature, var_s=practical_salinity): None\nProfile.process_en4() def Profile.process_en4(self, sort_time=True): VERSION 1.4 (05/07/2021)\nPREPROCESSES EN4 data ready for comparison with model data.\nThis routine will cut out a desired geographical box of EN4 data and\nthen apply quality control according to the available flags in the\nnetCDF files. Quality control happens in two steps:\n                1. Where a whole data profile is flagged, it is completely removed\n                from the dataset\n                2. Where a single datapoint is rejected in either temperature or\n                salinity, it is set to NaN.\nThis routine attempts to use xarray/dask chunking magic to keep\nmemory useage low however some memory is still needed for loading\nflags etc. May be slow if using large EN4 datasets.\nRoutine will return a processed profile object dataset and can write\nthe new dataset to file if fn_out is defined. If saving to the\nPROFILE object, be aware that DASK computations will not have happened\nand will need to be done using .load(), .compute() or similar before\naccessing the values. IF using multiple EN4 files or large dataset,\nmake sure you have chunked the data over N_PROF dimension.\nINPUTS\nfn_out (str)                : Full path to a desired output file. If unspecified\n                                                                                then nothing is written.\nEXAMPLE USEAGE:\nprofile = coast.PROFILE()\nprofile.read_EN4(fn_en4, chunks={'N_PROF':10000})\nfn_out = '~/output_file.nc'\nnew_profile = profile.preprocess_en4(fn_out = fn_out,\n                                                                                                                                                lonbounds = [-10, 10],\n                                                                                                                                                latbounds = [45, 65])\nProfile.calculate_all_en4_qc_flags() @classmethod def Profile.calculate_all_en4_qc_flags(cls): Brute force method for identifying all rejected points according to\nEN4 binary integers. It can be slow to convert large numbers of integers\nto a sequence of bits and is actually quicker to just generate every\ncombination of possible QC integers. That's what this routine does.\nUsed in PROFILE.preprocess_en4().\nINPUTS\nNO INPUTS\nOUTPUTS\nqc_integers_tem : Array of integers signifying the rejection of ONLY\n                                                                                temperature datapoints\nqc_integers_sal : Array of integers signifying the rejection of ONLY\n                                                                                salinity datapoints\nqc_integers_both : Array of integers signifying the rejection of BOTH\n                                                                                temperature and salinity datapoints.\nProfile.obs_operator() def Profile.obs_operator(self, gridded, mask_bottom_level=True): VERSION 2.0 (04/10/2021)\nAuthor: David Byrne\nDoes a spatial and time interpolation of a gridded object's data.\nA nearest neighbour approach is used for both interpolations. Both\ndatasets (the Profile and Gridded objects) must contain longitude,\nlatitude and time coordinates. This routine expects there to be a\nlandmask variable in the gridded object. This is is not available,\nthen place an array of zeros into the dataset, with dimensions\n(y_dim, x_dim).\nThis routine will do the interpolation based on the chunking applied\nto the Gridded object. Please ensure you have the available memory to\nhave an entire Gridded chunk loaded to memory. If multiple files are\nused, then using one chunk per file will be most efficient. Time\nchunking is generally the better option for this routine.\nINPUTS:\ngridded (Gridded)                                : gridded object created on t-grid\nmask_bottom_level (bool) : Whether or not to mask any data below the\n                                                                                                                model's bottom level. If True, then ensure\n                                                                                                                the Gridded object's dataset contain's a\n                                                                                                                bottom_level variable with dims\n                                                                                                                (y_dim, x_dim).\nOUTPUTS:\nReturns a new PROFILE object containing a computed dataset of extracted\nprofiles.\nProfile.reshape_2d() def Profile.reshape_2d(self, var_user_want): OBSERVATION type class for reshaping World Ocean Data (WOD) or similar that\ncontains 1D profiles (profile * depth levels) into a 2D array.\nNote that its variable has its own dimention and in some profiles\nonly some variables are present. WOD can be observed depth or a\nstandard depth as regrided by NOAA.\nArgs:\n                \u003e X                --                The variable (e.g,Temperatute, Salinity, Oxygen, DIC ..)\n                \u003e X_N                --                Dimensions of observed variable as 1D\n                                                                                (essentially number of obs variable = casts * osberved depths)\n                \u003e casts --                Dimension for locations of observations (ie. profiles)\n                \u003e z_N --                Dimension for depth levels of all observations as 1D\n                                                                                (essentially number of depths = casts * osberved depths)\n                \u003e X_row_size -- Gives the vertical index (number of depths)\n                                                                                for each variable\nProfile.time_slice() def Profile.time_slice(self, date0, date1): Return new Gridded object, indexed between dates date0 and date1\n","categories":"","description":"Docstrings for the Profile class\n","excerpt":"Docstrings for the Profile class\n","ref":"/COAsT/docs/reference/profile/","tags":"","title":"Profile"},{"body":"Objects ProfileAnalysis()\nProfileAnalysis.depth_means()\nProfileAnalysis.bottom_means()\nProfileAnalysis.determine_mask_indices()\nProfileAnalysis.mask_means()\nProfileAnalysis.difference()\nProfileAnalysis.interpolate_vertical()\nProfileAnalysis.average_into_grid_boxes()\nProfile Class\nProfileAnalysis() class ProfileAnalysis(Indexed): A set of analysis routines suitable for datasets in a Profile object. See individual docstrings in each method for more info. ProfileAnalysis.depth_means() @classmethod def ProfileAnalysis.depth_means(cls, profile, depth_bounds): Calculates a mean of all variable data that lie between two depths.\nReturns a new Profile() object containing the meaned data\nINPUTS:\ndataset (Dataset)                : A dataset from a Profile object.\ndepth_bounds (Tuple) : A tuple of length 2 describing depth bounds\n                                                                                                Should be of form: (lower, upper) and in metres\nProfileAnalysis.bottom_means() @classmethod def ProfileAnalysis.bottom_means(cls, profile, layer_thickness, depth_thresholds=unknown): Averages profile data in some layer above the bathymetric depth. This\nroutine requires there to be a 'bathymetry' variable in the Profile dataset.\nIt can apply a constant averaging layer thickness across all profiles\nor a bespoke thickness dependent on the bathymetric depth. For example,\nyou may want to define the 'bottom' as the average of 100m above the\nbathymetry in very deep ocean but only 10m in the shallower ocean.\nIf there is no data available in the layer specified (e.g. CTD cast not\ndeep enough or model bathymetry wrong) then it will be NaN\nTo apply constant thickness, you only need to provide a value (in metre)\nfor layer_thickness. For different thicknesses, you also need to give\ndepth_thresholds. The last threshold must always be np.inf, i.e. all\ndata below a specific bathymetry depth.\nFor example, to apply 10m to everywhere \u003c100m, 50m to 100m -\u003e 500m and\n100m elsewhere, use:\n                layer_thickness = [10, 50, 100]\n                depth_thresholds = [100, 500, np.inf]\nThe bottom bound is always assumed to be 0.\n*NOTE: If time related issues arise, then remove any time variables\nfrom the profile dataset before running this routine.\nINPUTS:\nlayer_thickness (array) : A scalar layer thickness or list of values\ndepth_thresholds (array) : Optional. List of bathymetry thresholds.\nOUTPUTS:\nNew profile object containing bottom averaged data.\nProfileAnalysis.determine_mask_indices() @classmethod def ProfileAnalysis.determine_mask_indices(cls, profile, mask_dataset): Determines whether each profile is within a mask (region) or not.\nThese masks should be in Dataset form, as returned by\nMask_maker().make_mask_dataset(). I.E, each mask\nshould be a 2D array with corresponding 2D longitude and latitude\narrays. Multiple masks should be stored along a dim_mask dimension.\nParameters\n----------\ndataset : xarray.Dataset\n                A dataset from a profile object\nmask_dataset : xarray.Dataset\n                Dataset with dimensions (dim_mask, x_dim, y_dim).\n                Should contain longitude, latitude and mask. Mask has dimensions\n                (dim_mask, y_dim, x_dim). Spatial dimensions should align with\n                longitude and latitude\nReturns\n-------\nDataset describing which profiles are in which mask/region.\nReady for input to Profile.mask_means()\nProfileAnalysis.mask_means() @classmethod def ProfileAnalysis.mask_means(cls, profile, mask_indices): Averages all data inside a given profile dataset across a regional mask\nor for multiples regional masks.\nParameters\n----------\ndataset : xarray.Dataset\n                The profile dataset to average.\nmask_indices : xarray.Dataset\n                Describes which profiles are in which region. Returned from\n                profile_analysis.determine_mask_indices().\nReturns\n-------\nxarray.Dataset containing meaned data.\nProfileAnalysis.difference() @classmethod def ProfileAnalysis.difference(cls, profile1, profile2, absolute_diff=True, square_diff=True): Calculates differences between all matched variables in two Profile\ndatasets. Difference direction is dataset1 - dataset2.\nParameters\n----------\ndataset1 : xarray.Dataset\n                First profile dataset\ndataset2 : xarray.Dataset\n                Second profile dataset\nabsolute_diff : bool, optional\n                Whether to calculate absolute differences. The default is True.\nsquare_diff : bool, optional\n                Whether to calculate square differences. The default is True.\nReturns\n-------\nNew Profile object containing differenced dataset.\nDifferences have suffix diff_\nAbsolute differences have suffix abs_\nSquare differences have suffic square_\nProfileAnalysis.interpolate_vertical() @classmethod def ProfileAnalysis.interpolate_vertical(cls, profile, new_depth, interp_method=linear, print_progress=False): (04/10/2021)\nAuthor: David Byrne\nFor vertical interpolation of all profiles within this object. User\nshould pass an array describing the new depths or another profile object\ncontaining the same number of profiles as this object.\nIf a 1D numpy array is passed then all profiles will be interpolated\nonto this single set of depths. If a xarray.DataArray is passed, it\nshould have dimensions (id_dim, z_dim) and contain a variable called\ndepth. This DataArray should contain the same number of profiles as\nthis object and will map profiles in order for interpolation. If\nanother profile object is passed, profiles will be mapped and\ninterpolated onto the other objects depth array.\nINPUTS:\nnew_depth (array or dataArray) : new depths onto which to interpolate\n                                                                                                                                see description above for more info.\ninterp_method (str)                                                : Any scipy interpolation string.\nOUTPUTS:\nReturns a new PROFILE object containing the interpolated dataset.\nProfileAnalysis.average_into_grid_boxes() @classmethod def ProfileAnalysis.average_into_grid_boxes(cls, profile, grid_lon, grid_lat, min_datapoints=1, season=None, var_modifier=): Takes the contents of this Profile() object and averages each variables\ninto geographical grid boxes. At the moment, this expects there to be\nno vertical dimension (z_dim), so make sure to slice the data out you\nwant first using isel, Profile.depth_means() or Profile.bottom_means().\nINPUTS\ngrid_lon (array)                : 1d array of longitudes\ngrid_lat (array)                : 1d array of latitude\nmin_datapoints (int) : Minimum N of datapoints at which to average\n                                                                                                into box. Will return Nan in boxes with smaller N.\n                                                                                                NOTE this routine will also return the variable\n                                                                                                grid_N, which tells you how many points were\n                                                                                                averaged into each box.\nseason (str)                                : 'DJF','MAM','JJA' or 'SON'. Will only average\n                                                                                                data from specified season.\nvar_modifier (str)                : Suffix to add to all averaged variables in the\n                                                                                                output dataset. For example you may want to add\n                                                                                                _DJF to all vars if restricting only to winter.\nOUTPUTS\nCOAsT Gridded object containing averaged data.\n","categories":"","description":"Docstrings for the Profile_analysis class\n","excerpt":"Docstrings for the Profile_analysis class\n","ref":"/COAsT/docs/reference/profile_analysis/","tags":"","title":"Profile_analysis"},{"body":"Objects Module with attributes defining month ranges for the four seasons.\nUsed for convenience with Climatology.multiyear_averages(). Note: Summer is defined as JJAS, as opposed to the meteorological seasons of JJA.\n","categories":"","description":"Docstrings for the Seasons class\n","excerpt":"Docstrings for the Seasons class\n","ref":"/COAsT/docs/reference/seasons/","tags":"","title":"Seasons"},{"body":"Objects quadratic_spline_roots()\nfind_maxima()\ndoodson_x0_filter()\nPython definitions used to aid with statistical calculations.\nMethods Overview -\u003e normal_distribution(): Create values for a normal distribution -\u003e cumulative_distribution(): Integration udner a PDF -\u003e empirical_distribution(): Estimates CDF empirically\nquadratic_spline_roots() def quadratic_spline_roots(spl): A custom function for the roots of a quadratic spline. Cleverness found at\nhttps://stackoverflow.com/questions/50371298/find-maximum-minimum-of-a-1d-interpolated-function\nUsed in find_maxima().\nExample usage:\nsee example_scripts/tidegauge_tutorial.py\nfind_maxima() def find_maxima(x, y, method=comp, **kwargs): Finds maxima of a time series y. Returns maximum values of y (e.g heights)\nand corresponding values of x (e.g. times).\n**kwargs are dependent on method.\n                Methods:\n                'comp' :: Find maxima by comparison with neighbouring values.\n                                                Uses scipy.signal.find_peaks. **kwargs passed to this routine\n                                                will be passed to scipy.signal.find_peaks.\n                'cubic' :: Find the maxima and minima by fitting a cubic spline and\n                                                                finding the roots of its derivative.\n                                                                Expect input as xr.DataArrays\n                DB NOTE: Currently only the 'comp' and 'cubic' method are implemented.\n                                                Future methods include linear interpolation.\n                JP NOTE: Cubic method:\n                                i) has intelligent fix for NaNs,\nExample usage:\nsee example_scripts/tidegauge_tutorial.py\ndoodson_x0_filter() def doodson_x0_filter(elevation, ax=0): The Doodson X0 filter is a simple filter designed to damp out the main\ntidal frequencies. It takes hourly values, 19 values either side of the\ncentral one and applies a weighted average using:\n                                (1010010110201102112 0 2112011020110100101)/30.\n( http://www.ntslf.org/files/acclaimdata/gloup/doodson_X0.html )\nIn \"Data Analaysis and Methods in Oceanography\":\n\"The cosine-Lanczos filter, the transform filter, and the\nButterworth filter are often preferred to the Godin filter,\nto earlier Doodson filter, because of their superior ability\nto remove tidal period variability from oceanic signals.\"\nThis routine can be used for any dimension input array.\nParameters\n----------\n                elevation (ndarray) : Array of hourly elevation values.\n                axis (int) : Time axis of input array. This axis must have \u003e= 39\n                elements\nReturns\n-------\n                Filtered array of same rank as elevation.\n","categories":"","description":"Docstrings for the Stats_util class\n","excerpt":"Docstrings for the Stats_util class\n","ref":"/COAsT/docs/reference/stats_util/","tags":"","title":"Stats_util"},{"body":"Objects Tidegauge()\nTidegauge.read_gesla_v3()\nTidegauge.read_gesla()\nTidegauge._read_gesla_header_v5()\nTidegauge._read_gesla_header_v3()\nTidegauge._read_gesla_data()\nTidegauge.read_hlw()\nTidegauge._read_hlw_header()\nTidegauge._read_hlw_data()\nTidegauge.show()\nTidegauge.get_tide_table_times()\nTidegauge.read_ea_api_to_xarray()\nTidegauge.read_bodc()\nTidegauge._read_bodc_header()\nTidegauge._read_bodc_data()\nTidegauge.plot_timeseries()\nTidegauge.plot_on_map()\nTidegauge.plot_on_map_multiple()\nTidegauge.obs_operator()\nTidegauge.time_slice()\nTidegauge.subset_indices_lonlat_box()\nTide Gauge class\nTidegauge() class Tidegauge(Timeseries): This is an object for storage and manipulation of tide gauge data in a single dataset. This may require some processing of the observations such as interpolation to a common time step. This object's dataset should take the form (as with Timeseries): Dimensions: id_dim : The locations dimension. Each time series has an index time : The time dimension. Each datapoint at each port has an index Coordinates: longitude (id_dim) : Longitude values for each port index latitude (id_dim) : Latitude values for each port index time (time) : Time values for each time index (datetime) id_name (id_dim) : Name of index, e.g. port name or mooring id. An example data variable could be ssh, or ntr (non-tidal residual). This object can also be used for other instrument types, not just tide gauges. For example moorings. Every id index for this object should use the same time coordinates. Therefore, timeseries need to be aligned before being placed into the object. If there is any padding needed, then NaNs should be used. NaNs should also be used for quality control/data rejection. Tidegauge.read_gesla_v3() def Tidegauge.read_gesla_v3(self, fn_gesla, date_start=None, date_end=None): Depreciated method.\nCall generalised method.\nReturns eiter a tidegauge object or a list of tidegauge objects\nTidegauge.read_gesla() def Tidegauge.read_gesla(self, fn_gesla, date_start=None, date_end=None, format=v3): For reading from a GESLA2 (Format version 3.0) or GESLA3 (Format v5.0)\nfile(s) into an xarray dataset.\nv3 formatting according to Woodworth et al. (2017).\nv5 formatting ....\nWebsite: https://www.gesla.org/\nIf no data lies between the specified dates, a dataset is still created\ncontaining information on the tide gauge, but the time dimension will\nbe empty.\nParameters\n----------\nfn_gesla (str) : path to gesla tide gauge file, list of files or a glob\ndate_start (datetime) : start date for returning data\ndate_end (datetime) : end date for returning data\nformat (str) : accepts \"v3\" or \"v5\"\nReturns\n-------\nCreates xarray.dataset within tidegauge object containing loaded data.\nIf multiple files are provided then instead returns a list of NEW\ntidegauge objects.\nTidegauge._read_gesla_header_v5() @classmethod def Tidegauge._read_gesla_header_v5(cls, fn_gesla): Reads header from a GESLA file (format version 5.0).\nParameters\n----------\nfn_gesla (str) : path to gesla tide gauge file\nReturns\n-------\ndictionary of attributes\nTidegauge._read_gesla_header_v3() @classmethod def Tidegauge._read_gesla_header_v3(cls, fn_gesla): Reads header from a GESLA file (format version 3.0).\nParameters\n----------\nfn_gesla (str) : path to gesla tide gauge file\nReturns\n-------\ndictionary of attributes\nTidegauge._read_gesla_data() @classmethod def Tidegauge._read_gesla_data(cls, fn_gesla, date_start=None, date_end=None, header_length=32): Reads observation data from a GESLA file (format version 3.0 and 5.0).\nParameters\n----------\nfn_gesla (str) : path to gesla tide gauge file\ndate_start (datetime) : start date for returning data\ndate_end (datetime) : end date for returning data\nheader_length (int) : number of lines in header (to skip when reading)\nReturns\n-------\nxarray.Dataset containing times, sealevel and quality control flags\nTidegauge.read_hlw() def Tidegauge.read_hlw(self, fn_hlw, date_start=None, date_end=None): For reading from a file of tidetable High and Low Waters (HLW) data into an\nxarray dataset. File contains high water and low water heights and times\nIf no data lies between the specified dates, a dataset is still created\ncontaining information on the tide gauge, but the time dimension will\nbe empty.\nThe data takes the form:\nLIVERPOOL (GLADSTONE DOCK)                TZ: UT(GMT)/BST                Units: METRES                Datum: Chart Datum\n01/10/2020 06:29                1.65\n01/10/2020 11:54                9.01\n01/10/2020 18:36                1.87\n...\nParameters\n----------\nfn_hlw (str) : path to tabulated High Low Water file\ndate_start (datetime) : start date for returning data\ndate_end (datetime) : end date for returning data\nReturns\n-------\nxarray.Dataset object.\nTidegauge._read_hlw_header() @classmethod def Tidegauge._read_hlw_header(cls, filnam): Reads header from a HWL file.\nThe data takes the form:\nLIVERPOOL (GLADSTONE DOCK) TZ: UT(GMT)/BST Units: METRES Datum: Chart Datum\n01/10/2020 06:29                1.65\n01/10/2020 11:54                9.01\n01/10/2020 18:36                1.87\n...\nParameters\n----------\nfilnam (str) : path to file\nReturns\n-------\ndictionary of attributes\nTidegauge._read_hlw_data() @classmethod def Tidegauge._read_hlw_data(cls, filnam, header_dict, date_start=None, date_end=None, header_length=1): Reads HLW data from a tidetable file.\nParameters\n----------\nfilnam (str) : path to HLW tide gauge file\ndate_start (np.datetime64) : start date for returning data.\ndate_end (np.datetime64) : end date for returning data.\nheader_length (int) : number of lines in header (to skip when reading)\nReturns\n-------\nxarray.Dataset containing times, High and Low water values\nTidegauge.show() def Tidegauge.show(self, timezone=None): Print out the values in the xarray\nDisplays with specified timezone\nTidegauge.get_tide_table_times() def Tidegauge.get_tide_table_times(self, time_guess=None, time_var=time, measure_var=ssh, method=window, winsize=None): Get tide times and heights from tide table.\ninput:\ntime_guess : np.datetime64 or datetime\n                                assumes utc\ntime_var : name of time variable [default: 'time']\nmeasure_var : name of ssh variable [default: 'ssh']\nmethod =\n                window: +/- hours window size, winsize, (int) return values in that window\n                                uses additional variable winsize (int) [default 2hrs]\n                nearest_1: return only the nearest event, if in winsize [default:None]\n                nearest_2: return nearest event in future and the nearest in the past (i.e. high and a low), if in winsize [default:None]\n                nearest_HW: return nearest High Water event (computed as the max of `nearest_2`), if in winsize [default:None]\nreturns: xr.DataArray( measure_var, coords=time_var)\n                E.g. ssh (m), time (utc)\n                If value is not found, it returns a NaN with time value as the\n                guess value.\nTidegauge.read_ea_api_to_xarray() @classmethod def Tidegauge.read_ea_api_to_xarray(cls, n_days=5, date_start=None, date_end=None, station_id=E70124): load gauge data via environment.data.gov.uk EA API\nEither loads last n_days, or from date_start:date_end\nAPI Source:\nhttps://environment.data.gov.uk/flood-monitoring/doc/reference\nDetails of available tidal stations are recovered with:\nhttps://environment.data.gov.uk/flood-monitoring/id/stations?type=TideGauge\nRecover the \"stationReference\" for the gauge of interest and pass as\nstation_id:str. The default station_id=\"E70124\" is Liverpool.\nINPUTS:\n                n_days : int. Extact the last n_days from now.\n                date_start : datetime. UTC format string \"yyyy-MM-dd\" E.g 2020-01-05\n                date_end : datetime\n                station_id : int. Station id. Also referred to as stationReference in\n                EA API. Default value is for Liverpool.\nOUTPUT:\n                ssh, time : xr.Dataset\nTidegauge.read_bodc() def Tidegauge.read_bodc(self, fn_bodc, date_start=None, date_end=None): For reading from a single BODC (processed) file into an\nxarray dataset.\nIf no data lies between the specified dates, a dataset is still created\ncontaining information on the tide gauge, but the time dimension will\nbe empty.\nData name: UK Tide Gauge Network, processed data.\nSource: https://www.bodc.ac.uk/\nSee data notes from source for description of QC flags.\nThe data takes the form:\n                Port:                                                P234\n                Site:                                                Liverpool, Gladstone Dock\n                Latitude:                                53.44969\n                Longitude:                                -3.01800\n                Start Date:                                01AUG2020-00.00.00\n                End Date:                                31AUG2020-23.45.00\n                Contributor:                National Oceanography Centre, Liverpool\n                Datum information: The data refer to Admiralty Chart Datum (ACD)\n                Parameter code:                ASLVBG02 = Surface elevation (unspecified datum)\n                of the water body by bubbler tide gauge (second sensor)\n                Cycle                Date                Time                ASLVBG02 Residual\n                Number yyyy mm dd hh mi ssf                                f                                f\n                                1) 2020/08/01 00:00:00                5.354M                0.265M\n                                2) 2020/08/01 00:15:00                5.016M                0.243M\n                                3) 2020/08/01 00:30:00                4.704M                0.241M\n                                4) 2020/08/01 00:45:00                4.418M                0.255M\n                                5) 2020/08/01 01:00:00                4.133                0.257\n                                ...\nParameters\n----------\nfn_bodc (str) : path to bodc tide gauge file\ndate_start (datetime) : start date for returning data\ndate_end (datetime) : end date for returning data\nReturns\n-------\nxarray.Dataset object.\nTidegauge._read_bodc_header() @staticmethod def Tidegauge._read_bodc_header(fn_bodc): Reads header from a BODC file (format version 3.0).\nParameters\n----------\nfn_bodc (str) : path to bodc tide gauge file\nReturns\n-------\ndictionary of attributes\nTidegauge._read_bodc_data() @staticmethod def Tidegauge._read_bodc_data(fn_bodc, date_start=None, date_end=None, header_length=11): Reads observation data from a BODC file.\nParameters\n----------\nfn_bodc (str) : path to bodc tide gauge file\ndate_start (datetime) : start date for returning data\ndate_end (datetime) : end date for returning data\nheader_length (int) : number of lines in header (to skip when reading)\nReturns\n-------\nxarray.Dataset containing times, sealevel and quality control flags\nTidegauge.plot_timeseries() def Tidegauge.plot_timeseries(self, var_list=unknown, date_start=None, date_end=None, plot_line=False): Quick plot of time series stored within object's dataset\nParameters\n----------\ndate_start (datetime) : Start date for plotting\ndate_end (datetime) : End date for plotting\nvar_list (str) : List of variables to plot. Default: just ssh\nplot_line (bool) : If true, draw line between markers\nReturns\n-------\nmatplotlib figure and axes objects\nTidegauge.plot_on_map() def Tidegauge.plot_on_map(self): Show the location of a tidegauge on a map.\nExample usage:\n--------------\n# For a TIDEGAUGE object tg\ntg.plot_map()\nTidegauge.plot_on_map_multiple() @classmethod def Tidegauge.plot_on_map_multiple(cls, tidegauge_list, color_var_str=None): Show the location of a tidegauge on a map.\nExample usage:\n--------------\n# For a TIDEGAUGE object tg\ntg.plot_map()\nTidegauge.obs_operator() def Tidegauge.obs_operator(self, gridded, time_interp=nearest): Regrids a Gridded object onto a tidegauge_multiple object. A nearest\nneighbour interpolation is done for spatial interpolation and time\ninterpolation can be specified using the time_interp argument. This\ntakes any scipy interpolation string. If Gridded object contains a\nlandmask variables, then the nearest WET point is taken for each tide\ngauge.\nOutput is a new tidegauge_multiple object containing interpolated data.\nTidegauge.time_slice() def Tidegauge.time_slice(self, date0, date1): Return new Gridded object, indexed between dates date0 and date1\nTidegauge.subset_indices_lonlat_box() def Tidegauge.subset_indices_lonlat_box(self, lonbounds, latbounds): Get a subset of this Profile() object in a spatial box.\nlonbounds -- Array of form [min_longitude=-180, max_longitude=180]\nlatbounds -- Array of form [min_latitude, max_latitude]\nreturn: A new profile object containing subsetted data\n","categories":"","description":"Docstrings for the Tidegauge class\n","excerpt":"Docstrings for the Tidegauge class\n","ref":"/COAsT/docs/reference/tidegauge/","tags":"","title":"Tidegauge"},{"body":"Objects TidegaugeAnalysis()\nTidegaugeAnalysis.match_missing_values()\nTidegaugeAnalysis.harmonic_analysis_utide()\nTidegaugeAnalysis.reconstruct_tide_utide()\nTidegaugeAnalysis.calculate_non_tidal_residuals()\nTidegaugeAnalysis.threshold_statistics()\nTidegaugeAnalysis.demean_timeseries()\nTidegaugeAnalysis.difference()\nTidegaugeAnalysis.find_high_and_low_water()\nTidegaugeAnalysis.doodson_x0_filter()\nTidegaugeAnalysis.crps()\nTidegaugeAnalysis.time_mean()\nTidegaugeAnalysis.time_std()\nTidegaugeAnalysis.time_slice()\nTidegaugeAnalysis.resample_mean()\nAn analysis class for tide gauge.\nTidegaugeAnalysis() class TidegaugeAnalysis(): This contains analysis methods suitable for use with the dataset structure of Tidegauge() TidegaugeAnalysis.match_missing_values() @classmethod def TidegaugeAnalysis.match_missing_values(cls, data_array1, data_array2, fill_value=unknown): Will match any missing values between two tidegauge_multiple datasets.\nWhere missing values (defined by fill_value) are found in either dataset\nthey are also placed in the corresponding location in the other dataset.\nReturns two new tidegauge objects containing only the new\nmasked data arrays.\nTidegaugeAnalysis.harmonic_analysis_utide() @classmethod def TidegaugeAnalysis.harmonic_analysis_utide(cls, data_array, min_datapoints=1000, nodal=False, trend=False, method=ols, conf_int=linear, Rayleigh_min=0.95): Does a harmonic analysis for each timeseries inside this object using\nthe utide library. All arguments except min_datapoints are arguments\nthat are passed to ut.solve(). Please see the utide website for more\ninformation:\n                https://pypi.org/project/UTide/\nUtide will by default do it's harmonic analysis using a set of harmonics\ndetermined using the Rayleigh criterion. This changes the number of\nharmonics depending on the length and frequency of the time series.\nOutput from this routine is not a new dataset, but a list of utide\nanalysis object. These are structures containing, amongst other things,\namplitudes, phases, constituent names and confidence intervals. This\nlist can be passed to reconstruct_tide_utide() in this object to create\na new TidegaugeMultiple object containing reconstructed tide data.\nINPUTS\ndata_array                : Xarray data_array from a coast.Tidegauge() object\n                                                                e.g. tidegauge.dataset.ssh\nmin_datapoints : If a time series has less than this value number of\n                                                                datapoints, then omit from the analysis.\n: Inputs to utide.solve(). See website above.\nOUTPUTS\nA list of utide structures from the solve() routine. If a location\nis omitted, it will contain [] for it's entry.\nTidegaugeAnalysis.reconstruct_tide_utide() @classmethod def TidegaugeAnalysis.reconstruct_tide_utide(cls, data_array, utide_solution_list, constit=None, output_name=reconstructed): Use the tarray of times to reconstruct a time series series using a\nlist of utide analysis objects. This list can be obtained\nusing harmonic_analysis_utide(). Specify constituents to use in the\nreconstruction by passing a list of strings such as 'M2' to the constit\nargument. This won't work if a specified constituent is not present in\nthe analysis.\nTidegaugeAnalysis.calculate_non_tidal_residuals() @classmethod def TidegaugeAnalysis.calculate_non_tidal_residuals(cls, data_array_ssh, data_array_tide, apply_filter=True, window_length=25, polyorder=3): Calculate non tidal residuals by subtracting values in data_array_tide\nfrom data_array_ssh. You may optionally apply a filter to the non\ntidal residual data by setting apply_filter = True. This uses the\nscipy.signal.savgol_filter function, which you may pass window_length\nand poly_order.\nTidegaugeAnalysis.threshold_statistics() @classmethod def TidegaugeAnalysis.threshold_statistics(cls, dataset, thresholds=unknown, peak_separation=12): Do some threshold statistics for all variables with a time dimension\ninside this tidegauge_multiple object. Specifically, this routine will\ncalculate:\n                                peak_count                                : The number of indepedent peaks over\n                                                                                                                each specified threshold. Independent peaks\n                                                                                                                are defined using the peak_separation\n                                                                                                                argument. This is the number of datapoints\n                                                                                                                either side of a peak within which data\n                                                                                                                is ommited for further peak search.\n                                time_over_threshold : The total time spent over each threshold\n                                                                                                                This is NOT an integral, but simple a count\n                                                                                                                of all points over threshold.\n                                dailymax_count                : A count of the number of daily maxima over\n                                                                                                                each threshold\n                                monthlymax_count                : A count of the number of monthly maxima\n                                                                                                                over each threshold.\nOutput is a xarray dataset containing analysed variables. The name of\neach analysis variable is constructed using the original variable name\nand one of the above analysis categories.\nTidegaugeAnalysis.demean_timeseries() @staticmethod def TidegaugeAnalysis.demean_timeseries(dataset): Subtract time means from all variables within this tidegauge_multiple\nobject. This is done independently for each id_dim location.\nTidegaugeAnalysis.difference() @classmethod def TidegaugeAnalysis.difference(cls, dataset1, dataset2, absolute_diff=True, square_diff=True): Calculates differences between two tide gauge objects datasets. Will calculate\ndifferences, absolute differences and square differences between all\ncommon variables within each object. Each object should have the same\nsized dimensions. When calling this routine, the differencing is done\nas follows:\n                dataset1.difference(dataset2)\nThis will do dataset1 - dataset2.\nOutput is a new tidegauge object containing differenced variables.\nTidegaugeAnalysis.find_high_and_low_water() @staticmethod def TidegaugeAnalysis.find_high_and_low_water(data_array, method=comp, **kwargs): Finds high and low water for a given variable.\nReturns in a new TIDEGAUGE object with similar data format to\na TIDETABLE. If this Tidegauge object contains more than one location\n(id_dim \u003e 1) then a list of Tidegauges will be returned.\nMethods:\n'comp' :: Find maxima by comparison with neighbouring values.\n                                Uses scipy.signal.find_peaks. **kwargs passed to this routine\n                                will be passed to scipy.signal.find_peaks.\n'cubic':: Find the maxima using the roots of cubic spline.\n                                Uses scipy.interpolate.InterpolatedUnivariateSpline\n                                and scipy.signal.argrelmax. **kwargs are not activated.\nNOTE: Currently only the 'comp' and 'cubic' methods implemented. Future\n                                methods include linear interpolation or refinements.\nTidegaugeAnalysis.doodson_x0_filter() @staticmethod def TidegaugeAnalysis.doodson_x0_filter(dataset, var_str): Applies doodson X0 filter to a specified TIDEGAUGE variable\nInput ius expected to be hourly. Use resample_mean to average data\nto hourly frequency.\nTidegaugeAnalysis.crps() @classmethod def TidegaugeAnalysis.crps(cls, tidegauge_data, gridded_data, nh_radius=20, time_interp=linear): Comparison of observed variable to modelled using the Continuous\nRanked Probability Score. This is done using this TIDEGAUGE object.\nThis method specifically performs a single-observation neighbourhood-\nforecast method.\nParameters\n----------\nmodel_object (model) : Model object (NEMO) containing model data\nmodel_var_name (str) : Name of model variable to compare.\nobs_var_name (str) : Name of observed variable to compare.\nnh_radius (float)                : Neighbourhood rad\ncdf_type (str)                : Type of cumulative distribution to use for the\n                                                                                model data ('empirical' or 'theoretical').\n                                                                                Observations always use empirical.\ntime_interp (str)                : Type of time interpolation to use (s)\ncreate_new_obj (bool): If True, save output to new TIDEGAUGE obj.\n                                                                                Otherwise, save to this obj.\nReturns\n-------\nxarray.Dataset containing times, sealevel and quality control flags\nExample Useage\n-------\n# Compare modelled 'sossheig' with 'ssh' using CRPS\ncrps = altimetry.crps(nemo, 'sossheig', 'ssh')\nTidegaugeAnalysis.time_mean() @classmethod def TidegaugeAnalysis.time_mean(cls, dataset, date0=None, date1=None): Time mean of all variables between dates date0, date1\nTidegaugeAnalysis.time_std() @classmethod def TidegaugeAnalysis.time_std(cls, dataset, date0=None, date1=None): Time st. dev of variable var_str between dates date0 and date1\nTidegaugeAnalysis.time_slice() @classmethod def TidegaugeAnalysis.time_slice(cls, dataset, date0=None, date1=None): None\nTidegaugeAnalysis.resample_mean() @classmethod def TidegaugeAnalysis.resample_mean(cls, dataset, time_freq, **kwargs): Resample a TIDEGAUGE variable in time by calculating the mean\n                of all data points at a given frequency.\nParameters\n----------\ntime_freq (str) : Time frequency. e.g. '1H' for hourly, '1D' for daily\n                                                                Can also be a timedelta object. See Pandas resample\n                                                                method for more info.\n**kwargs (other) : Other arguments to pass to xarray.Dataset.resample\n(http://xarray.pydata.org/en/stable/generated/xarray.Dataset.resample.html)\nReturns\n-------\nNew Tidegauge() object containing resampled data\n","categories":"","description":"Docstrings for the Tidegauge_analysis class\n","excerpt":"Docstrings for the Tidegauge_analysis class\n","ref":"/COAsT/docs/reference/tidegauge_analysis/","tags":"","title":"Tidegauge_analysis"},{"body":"Objects Timeseries()\nTimeseries Class\nTimeseries() class Timeseries(Indexed): Parent class for Tidegauge and other timeseries type datasets Common methods ... ","categories":"","description":"Docstrings for the Timeseries class\n","excerpt":"Docstrings for the Timeseries class\n","ref":"/COAsT/docs/reference/timeseries/","tags":"","title":"Timeseries"},{"body":"Objects Track()\nTrack class\nTrack() class Track(Indexed): Parent class for subclasses Altimetry Common methods .... ","categories":"","description":"Docstrings for the Track class\n","excerpt":"Docstrings for the Track class\n","ref":"/COAsT/docs/reference/track/","tags":"","title":"Track"},{"body":"Objects xesmf_convert()\nxesmf_convert._get_xesmf_datasets()\nxesmf_convert.to_gridded()\nA class to convert from coast gridded to xesmf.\nxesmf_convert() class xesmf_convert(): Converts the main dataset within a COAsT.Gridded object to be suitable for input to XESMF for regridding to either a curvilinear or rectilienar grid. All you need to do if provide a Gridded object and a grid type when creating a new instance of this class. It will then contain an appropriate input dataset. You may also provide a second COAsT gridded object if regridding between two objects. For using xesmf, please see the package's documentation website here: https://xesmf.readthedocs.io/en/latest/index.html You can install XESMF using: conda install -c conda-forge xesmf. The setup used by this class has been tested for xesmf v0.6.2 alongside esmpy v8.0.0. It was installed using: conda install -c conda-forge xesmf esmpy=8.0.0 INPUTS input_gridded_obj (Gridded) :: Gridded object to be regridded output_gridded_obj(Gridded) :: (optional) Gridded object to regrid TO reorder_dims (bool) :: Xesmf requires that lat/lon dimensions are the last dimensions. If this is True, then will attempt to reorder dimensions. Not recommended for large datasets. [Default = False] \u003e\u003e\u003e EXAMPLE USEAGE \u003c\u003c\u003c If regridding a Gridded object to an arbitrarily defined rectilinear or curvilinear grid, you just need to do the following: import xesmf as xe # Create your gridded object gridded = coast.Gridded(*args, **kwargs) # Pass the gridded object over to xesmf_convert xesmf_ready = coast.xesmf_convert(gridded, input_grid_type = 'curvilinear') # Now this object will contain a dataset called xesmf_input, which can # be passed over to xesmf. E.G: destination_grid = xesmf.util.grid_2d(-15, 15, 1, 45, 65, 1) regridder = xe.Regridder(xesmf_ready.input_grid, destination_grid, \"bilinear\") regridded_dataset = regridder(xesmf_ready.input_data) XESMF contains a couple of difference functions for quickly creating output grids, such as xesmf.util.grid_2d and xesmf.util.grid_global(). See their website for more info. The process is almost the same if regridding from one COAsT.Gridded object to another (gridded0 -\u003e gridded1): xesmf_ready = coast.xesmf_convert(gridded0, gridded1, input_grid_type = \"curvilinear\", output_grid_type = \"curvilinear\") regridder = xe.Regridder(xesmf_ready.input_grid, xesmf_ready.output_grid, \"bilinear\") regridded_dataset = regridder(xesmf_ready.input_data) Note that you can select which variables you want to regrid, either prior to using this tool or by indexing the input_data dataset. e.g.: regridded_dataset = regridder(xesmf_ready.input_data['temperature']) If your input datasets were lazy loaded, then so will the regridded dataset. At this point you can either load the data or (recomended) save the regridded data to file: regridded_dataset.to_netcdf(\u003cfilename_to_save\u003e) Before saving back to file, call xesmf_ready.to_gridded() to convert the regridded xesmf object back to a gridded object xesmf_convert._get_xesmf_datasets() @classmethod def xesmf_convert._get_xesmf_datasets(cls, dataset, grid_type, reorder_dims=False): For a given dataset taken from a Gridded object and a grid_type\n(curvilinear or rectilinear), determine the xesmf formatted dataset.\nThis method does some checks to make sure the dataset is suitable and\nrenames the relevant dimensions/coordinates. Any vars that don't have\nboth x_dim and y_dim will be dropped. If x_dim and y_dim are present\nBUT they are not the last dimensions AND reorder_dims=True then\nthe dimensions will be reordered (not good for lazy loading/chunking).\nxesmf_convert.to_gridded() @staticmethod def xesmf_convert.to_gridded(xesmf_dataset): Converts an xesmf_dataset back to a Coast.Gridded() object. Returns\na Gridded object.\n","categories":"","description":"Docstrings for the Xesmf_convert class\n","excerpt":"Docstrings for the Xesmf_convert class\n","ref":"/COAsT/docs/reference/xesmf_convert/","tags":"","title":"Xesmf_convert"},{"body":" COAsT Coastal Ocean Assessment Toolbox A Python toolbox for assessing kilometer-scale ocean models, conducting statistical analyses, and facilitating model validation. About Documentation Download About COAsT is Diagnostic and Assessment toolbox for kilometric scale regional models. It's aim is to deliver a flexible, community-ready framework for assessing kilometric scale ocean models.\nThe focus, initially, is be on delivering novel diagnostics for processes that are emergent at the kilometric scale and with NEMO model output.\nJoin the COAsT community COAsT is an open source project that anyone in the community can use, improve, and enjoy. We'd love you to join us! Here's a few ways to find out what's happening and get involved.\nLearn and Connect\tUsing or want to use COAsT? Find out more here:\nGithub Pypi Anaconda Develop and Contribute If you want to get more involved by contributing to COAsT, join us on our Github Repository You can find out how to contribute to these docs in our Contribution Guidelines. ","categories":"","description":"","excerpt":" COAsT Coastal Ocean Assessment Toolbox A Python toolbox for assessing …","ref":"/COAsT/","tags":"","title":"COAsT"},{"body":"","categories":"","description":"","excerpt":"","ref":"/COAsT/search/","tags":"","title":"Search Results"}]