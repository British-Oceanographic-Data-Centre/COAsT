[{"body":"","excerpt":"","ref":"/COAsT/docs/examples/notebooks/gridded/","title":"Gridded"},{"body":"","excerpt":"","ref":"/COAsT/docs/examples/notebooks/","title":"Notebooks"},{"body":"COAsT (Coastal Ocean Assessment Toolkit) is a diagnostics and assessment python toolbox for kilometric scale regional models. The aim is that this toolbox is community-ready and flexible.\nThe initial focus will be on delivering a limited number of novel diagnostics for NEMO configurations, but that the toolbox would be expanded to include other diagnostics and other ocean models.\n","excerpt":"COAsT (Coastal Ocean Assessment Toolkit) is a diagnostics and assessment python toolbox for …","ref":"/COAsT/docs/overview/","title":"Overview"},{"body":"Python as a language comes with more stringent recommendations than most when it comes to code styling. This is advantageous in our case as it gives us an obvious set of guidelines to adopt.\nWhen it comes to simple code styling, much of what\u0026rsquo;s recommended here will be copied from Python Enhancement Proposal (PEP) 8, an officially proposed and accepted Python style guide.\nCode Styling Conventions Let\u0026rsquo;s keep things simple to start with\u0026hellip;\n  Indentation should be achieved with spaces rather than tabs and each new level of indentation should be indented by four columns (i.e four spaces).\n  Any single line, including its indentation characters, should not exceed 79 characters in length.\n  Top-level (i.e at the module/file level rather than inside a function or class) function and class definitions should be separated by two blank lines.\n  Method (functions within a class) definitions are separated by a single blank line.\n  Usually, \u0026ldquo;import\u0026rdquo; statements should be on separate lines, that is to say that you should have one line per distinct module or package import. An exception to this rule is when multiple objects are imported from a single module or package, using a \u0026ldquo;from\u0026rdquo; statement, in which case individual objects can be imported on the same line, separated by commas.\n  PEP 8 does not make a recommendation relating to the use of double or single quotes in general use, but for the sake of consistency, this document suggests the use of double quotes wherever practical. This recommendation is intended for the sake of consistency with triple-quoted strings, as per Docstring Conventions (PEP 257).\n  Operators should be separated by single columns (i.e one space) either side, unless inside parentheses, in which case no whitespace is required.\n  Comments (beginning with the # character) should be indented as if they were code. In the case of inline comments, separate the comment with two spaces following the code it shares the line with.\n  All functions should contain a docstring, which provides basic information on its usage. For this project, the reStructuredText docstring format is suggested.\n  When it comes to naming variables and functions, snake case (lower_case_words_separated_by_underscores) is preferred. There are however a few exceptions to this rule: Class names should be styled as camel case (EveryNewWordIsCapitalised). Constants (Variables that should not be changed) can be indicated by the use of screaming snake case (UPPER_CASE_WORDS_SEPARATED_BY_UNDERSCORES). Note that this library currently targets Python 3.7, so the use of typing.Final official support for constant variables, new as of Python 3.8: is not currently supported.\n  In general, it is suggested to avoid the use of single-character variable names, but this is acceptable in certain cases, such as when defining coordinates (such as x, y and z), as these will be commonly recognized and enforcing different rules could cause confusion. PEP 8 advises the following regarding names to avoid: \u0026ldquo;Never use the characters \u0026lsquo;l\u0026rsquo; (lowercase letter el), \u0026lsquo;O\u0026rsquo; (uppercase letter oh), or \u0026lsquo;I\u0026rsquo; (uppercase letter eye) as single character variable names.\u0026rdquo; These specific characters should be avoided because they present an accessibility issue, as under many fonts these characters may be difficult to distinguish or completely indistinguishable from numerals one (1) and zero (0).\n  In the interest of readability, where named iterator variables are required, this document suggests the use of double characters (e.g. \u0026ldquo;ii\u0026rdquo; rather than \u0026ldquo;i\u0026rdquo;).\n  Object-Oriented Programming The general principles of OOP are fairly straightforward and well documented, so I won\u0026rsquo;t waste your precious time by regurgitating that particular wall of text here. Instead, I\u0026rsquo;ll focus on some general pointers specific to this language and use case.\n  In Python, all class attributes are technically public, but semantically, attributes can be designated as non-public by including leading underscores in the name. For instance, \u0026ldquo;my_variable\u0026rdquo; becomes \u0026ldquo;_my_variable\u0026rdquo;. These attributes are generally referred to as \u0026ldquo;protected\u0026rdquo;.\n  When you define a Python class, it is a best practice to inherit from the base object type. This convention stems from Python 2.X, as classes and types were not originally synonymous. This behaviour is implicit in Python 3.X but the convention has persisted nonetheless. Classes defined this way are referred to as \u0026ldquo;new-style\u0026rdquo; classes.\n  When defining a class that inherits from another, it is important to remember that overridden methods (in particular, this behaviour is important when dealing with __init__ methods) do not implicitly call the parent method. What this means is that unless you want to deliberately prevent the behaviour of the parent class (this is a very niche use-case), it is important to include a reference to the parent method. An example of this is: super().__init__() This functionality is advantageous as it prevents unnecessary duplication of code, which is a key tenet of object-oriented software.\n  ","excerpt":"Python as a language comes with more stringent recommendations than most when it comes to code …","ref":"/COAsT/docs/contributing_package/python_style/","title":"Python: Style"},{"body":"** Notes on Object Structure and Loading (for contributors):\nCOAsT is an object-orientated package, meaning that data is stored within Python object structures. In addition to data storage, these objects contain methods (subroutines) which allow for manipulation of this data. An example of such an object is the Gridded object, which allows for the storage and manipulation of e.g. NEMO output and domain data. It is important to understand how to load data using COAsT and the structure of the resulting objects.\nA Gridded object is created and initialised by passing it the paths of the domain and data files. Ideally, the grid type should also be specified (T, U, V or F in the case of NEMO). For example, to load in data from a file containing data on a NEMO T-grid:\nimport coast fn_data = \u0026quot;\u0026lt;path to T-grid data file(s)\u0026gt;\u0026quot; fn_domain = \u0026quot;\u0026lt;path to domain file\u0026gt;\u0026quot; fn_config = \u0026quot;\u0026lt;path to json config file\u0026gt;\u0026quot; data = coast.Gridded(fn_data, fn_domain, fn_config) Ideally, Gridded model output data should be in grid-specific files, i.e. containing output variables situated on a NEMO T, U, V or F grid, whereas the grid variables are in a single domain file. On loading into COAsT, only the grid specific variables appropriate for the paired data are placed into the Gridded object. A Gridded object therefore contains grid-specific data and all corresponding grid variables. One of the file names can be omitted (to get a data-only or grid only object), however functionality in this case will be limited.\nOnce loaded, data is stored inside the object using an xarray.dataset object. Following on from the previous code example, this can be viewed by calling:\ndata.dataset This reveals all netcdf-type aspects of the data and domain variables that were loaded, including dimensions, coordinates, variables and attributes. For example:\n\u0026lt;xarray.Dataset\u0026gt; Dimensions: (axis_nbounds: 2, t_dim: 7, x_dim: 297, y_dim: 375, z_dim: 51) Coordinates: time (t_dim) datetime64[ns] 2007-01-01T11:58:56 ... 2007-01-31T11:58:56 longitude (y_dim, x_dim) float32 ... latitude (y_dim, x_dim) float32 ... Dimensions without coordinates: axis_nbounds, t_dim, x_dim, y_dim, z_dim Data variables: deptht_bounds (z_dim, axis_nbounds) float32 ... sossheig (t_dim, y_dim, x_dim) float32 ... time_counter_bounds (t_dim, axis_nbounds) datetime64[ns] ... time_instant (t_dim) datetime64[ns] ... temperature (t_dim, z_dim, y_dim, x_dim) float32 ... e1 (y_dim, x_dim) float32 ... e2 (y_dim, x_dim) float32 ... e3_0 (z_dim, y_dim, x_dim) float32 1.0 1.0 1.0 ... 1.0 1.0 Variables may be obtained in a number of ways. For example, to get temperature data, the following are all equivalent:\ntemp = data.dataset.temperature temp = data.dataset['temperature'] temp = data['temperature'] These commands will all return an xarray.dataarray object. Manipulation of this object can be done using xarray commands, for example indexing using [] or xarray.isel. Be aware that indexing will preserve lazy loading, however and direct access or modifying of the data will not. For this reason, if you require a subset of the data, it is best to index first.\nThe names of common grid variables are standardised within the COAsT package using JSON configuration files. For example, the following lists COAsT internal variable followed by the typical NEMO variable names:\n longitude [glamt / glamu / glamv / glamf] latitude [gphit / gphiu / gphiv / gphif] time [time_counter] e1 [e1t / e1u / e1v / e1f] (dx variable) e2 [e1t / e1u / e1v / e1f] (dy variable) e3_0 [e3t_0 / e3u_0 / e3v_0 / e3f_0] (dz variable at time 0)  Longitude, latitude and time are also set as coordinates. You might notice that dimensions are also standardised:\n x_dim The dimension for the x-axis (longitude) y_dim The dimension for the y-axis (latitude) t_dim The dimension for the time axis z_dim The dimension for the depth axis.  Wherever possible, the aim is to ensure that all of the above is consistent across the whole COAsT toolbox. Therefore, you will also find the same names and dimensions in, for example observation objects. Future objects, where applicable, will also follow these conventions. If you (as a contributor) add new objects to the toolbox, following the above template is strongly encouraged. This includes using xarray dataset/dataarray objects where possible, adopting an object oriented approach and adhering to naming conventions.\n","excerpt":"** Notes on Object Structure and Loading (for contributors):\nCOAsT is an object-orientated package, …","ref":"/COAsT/docs/contributing_package/python_structure/","title":"Python: Structure"},{"body":"","excerpt":"","ref":"/COAsT/docs/examples/notebooks/general/","title":"General utility and analysis tools"},{"body":"Prerequisites This package requires;\n a linux environment or docker for Windows python version 3.8.10 Miniconda  Basic use installation via conda or pip This package should be installed by run;\nconda install -c bodc coast However, there is also the option of;\npip install COAsT Development use installation If you would prefer to work with a clone of the repository in a development python environment do the following. First clone the repository in the place where you want to work:\ngit clone https://github.com/British-Oceanographic-Data-Centre/COAsT.git cd COAsT Then build a python environment:\nconda env update --prune --file environment.yml conda activate coast Building the docker image and executing an interactive environment Warning, building the image is resource heavy.\nAfter cloning the repo (as above).\ndocker build . --tag coast docker compose up -d docker compose exec coast bash You can now start a python session and import coast. docker compose mounts 3 directories from you host machine onto the docker container:\n./example_files:/example_files\n./config:/config\n./example_scripts:/example_scripts\nObtaining Example files In order to try the Examples, example data files and configuration files are recommended.\nExample data files Download example files and link them into a new directory:\nwget -c https://linkedsystems.uk/erddap/files/COAsT_example_files/COAsT_example_files.zip \u0026amp;\u0026amp; unzip COAsT_example_files.zip Example configuration files To facilitate loading different types of data, key information is passed to COAsT using configuration files. The config files used in the Examples are in the repository, or can be downloaded as static files:\nwget -c https://github.com/British-Oceanographic-Data-Centre/COAsT/archive/refs/heads/master.zip \u0026amp;\u0026amp; unzip master.zip Test it! The below example works best with the COAsT example data. Start by opening a python terminal and then importing COAsT:\nimport coast Before using coast, we will just check that Anaconda has installed correct package versions. In the python console copy the following:\nimport gsw import matplotlib print(gsw.__version__) print(matplotlib.__version__) The output should be\n3.4.0 3.5.1 or later. If it is, great carry on. If it is not, problems may occur with some functionality in coast. Please get in contact using the contacts in the workshop email.\nTake a look at the example pages for more information on specific objects and methods.\n","excerpt":"Prerequisites This package requires;\n a linux environment or docker for Windows python version …","ref":"/COAsT/docs/getting-started/","title":"Getting Started"},{"body":"For use on Liverpool servers only\nPrerequisites This package requires;\n python version 3.8+ Anaconda version 4.10+  Are there any system requirements for using this project? What languages are supported (if any)? Do users need to already have any software or tools installed?\nBasic use installation via conda or pip This package should be installed by run;\nconda install -c bodc coast However, there is also the option of;\npip install COAsT if you wish to install from source then got to GitHub and follow the README instructions\nThe base package should now be installed on your system. The following packages might be required for some of the advanced plotting features;\n cartopy  Development use installation If you would prefer to work with a clone of the repository in a development python environment do the following. First clone the repoitory in the place where you want to work:\ngit clone https://github.com/British-Oceanographic-Data-Centre/COAsT.git Then start building a python environment. Here (for example) called coast_dev:\nmodule load anaconda/5-2021 # or whatever it takes to activate conda conda config --add channels conda-forge # add conda-forge to your conda channels conda create -n coast_dev python=3.8 # create a new environment. E.g. `coast_dev` conda activate coast_dev # activate new environment Install packages to the environment:\ncd COAsT conda install --file conda_dev_requirements.txt Obtaining Example files In order to try the Examples, example data files and configuration files are recommended.\nExample data files Download example files and link them into a new directory:\nrm -rf coast_demo mkdir coast_demo cd coast_demo wget -c https://linkedsystems.uk/erddap/files/COAsT_example_files/COAsT_example_files.zip \u0026amp;\u0026amp; unzip COAsT_example_files.zip ln -s COAsT_example_files example_files Example configuration files To facilitate loading different types of data, key information is passed to COAsT using configuration files. The config files used in the Examples are in the repository, or can be downloaded as static files:\ncd ../coast_demo wget -c https://github.com/British-Oceanographic-Data-Centre/COAsT/archive/refs/heads/master.zip \u0026amp;\u0026amp; unzip master.zip ln -s COAsT-master/config config Preparation for Workshop Package Installation with conda Assuming a linux environment and that you have anaconda on your system:\n## Fresh build in new conda environment module load anaconda/5-2021 # or whatever it takes to activate conda yes | conda env remove --name workshop_env # remove environment \u0026#39;workshop_env\u0026#39; if it exists yes | conda create --name workshop_env python=3.8 # create a new environment conda activate workshop_env # activate new environment yes | conda install -c bodc coast=2.0.3 # install COAsT within new environment yes | conda install -c conda-forge cartopy=0.20.2 # install cartopy Then obtain the Example data and configuration files (as above).\nExternal Requirements All required packages should be defined in the environment.yml.\nTest it! The below example works best with the COAsT example data. Start by opening a python terminal and then importing COAsT:\nimport coast Before using coast, we will just check that Anaconda has installed correct package versions. In the python console copy the following:\nimport gsw import matplotlib print(gsw.__version__) print(matplotlib.__version__) The output should be\n3.4.0 3.5.1 or later. If it is, great carry on. If it is not, problems may occur with some functionality in coast. Please get in contact using the contacts in the workshop email.\nTake a look at the example pages for more information on specific objects and methods.\n","excerpt":"For use on Liverpool servers only\nPrerequisites This package requires;\n python version 3.8+ Anaconda …","ref":"/COAsT/docs/getting-started/getting-started-at-liverpool/","title":"Getting Started at Liverpool"},{"body":"GitHub actions diagram This is a collection of flowcharts for all the GitHub actions used across the COAsT and COAsT-site repos\nCOAsT building Packages   let isDark = window.matchMedia('(prefers-color-scheme: dark)').matches; let mermaidTheme = (isDark) ? 'dark' : 'default'; let mermaidConfig = { theme: mermaidTheme, logLevel: 'fatal', securityLevel: 'strict', startOnLoad: true, arrowMarkerAbsolute: false, er: { diagramPadding: 20, layoutDirection: 'TB', minEntityWidth: 100, minEntityHeight: 75, entityPadding: 15, stroke: 'gray', fill: 'honeydew', fontSize: 12, useMaxWidth: true, }, flowchart: { diagramPadding: 8, htmlLabels: true, curve: 'basis', }, sequence: { diagramMarginX: 50, diagramMarginY: 10, actorMargin: 50, width: 150, height: 65, boxMargin: 10, boxTextMargin: 5, noteMargin: 10, messageMargin: 35, messageAlign: 'center', mirrorActors: true, bottomMarginAdj: 1, useMaxWidth: true, rightAngles: false, showSequenceNumbers: false, }, gantt: { titleTopMargin: 25, barHeight: 20, barGap: 4, topPadding: 50, leftPadding: 75, gridLineStartPadding: 35, fontSize: 11, fontFamily: '\"Open-Sans\", \"sans-serif\"', numberSectionStyles: 4, axisFormat: '%Y-%m-%d', topAxis: false, }, }; mermaid.initialize(mermaidConfig);  graph LR; subgraph publish_package - runs on push to master A1[Setup python]-- 3.8 --B1; B1[Install dependencies]--C1; C1[Setup Enviroment]--D1; D1[Build package]--E1; E1[Test Package Install]--F1 F1[Publish to pypi]--G1 G1[Generate Conda Metadata]--H1 H1[Publish to Anaconda] end; subgraph build_package - runs on push to non-master A[Setup python]-- 3.8 and 3.9 --B; B[Install dependencies]--C; C[Setup Enviroment]--D; D[Build package]--E; E[Test Package Install]--F F[Generate Conda Metadata] end;  Verification and Formatting   let isDark = window.matchMedia('(prefers-color-scheme: dark)').matches; let mermaidTheme = (isDark) ? 'dark' : 'default'; let mermaidConfig = { theme: mermaidTheme, logLevel: 'fatal', securityLevel: 'strict', startOnLoad: true, arrowMarkerAbsolute: false, er: { diagramPadding: 20, layoutDirection: 'TB', minEntityWidth: 100, minEntityHeight: 75, entityPadding: 15, stroke: 'gray', fill: 'honeydew', fontSize: 12, useMaxWidth: true, }, flowchart: { diagramPadding: 8, htmlLabels: true, curve: 'basis', }, sequence: { diagramMarginX: 50, diagramMarginY: 10, actorMargin: 50, width: 150, height: 65, boxMargin: 10, boxTextMargin: 5, noteMargin: 10, messageMargin: 35, messageAlign: 'center', mirrorActors: true, bottomMarginAdj: 1, useMaxWidth: true, rightAngles: false, showSequenceNumbers: false, }, gantt: { titleTopMargin: 25, barHeight: 20, barGap: 4, topPadding: 50, leftPadding: 75, gridLineStartPadding: 35, fontSize: 11, fontFamily: '\"Open-Sans\", \"sans-serif\"', numberSectionStyles: 4, axisFormat: '%Y-%m-%d', topAxis: false, }, }; mermaid.initialize(mermaidConfig);  graph LR subgraph formatting - runs on pull requests A[Setup python]-- 3.9 --B; B[Install black]--C; C[Check formatting]-- D; D[Apply formatting] end; subgraph verifiy_package - runs for every push A1[Setup python]-- 3.8 and 3.9 --B1; B1[Install dependencies]--C1; C1[Lint]--D1; D1[Test] end; click B1 \"https://www.github.com\" \"tooltip\"  interactions with other repos   let isDark = window.matchMedia('(prefers-color-scheme: dark)').matches; let mermaidTheme = (isDark) ? 'dark' : 'default'; let mermaidConfig = { theme: mermaidTheme, logLevel: 'fatal', securityLevel: 'strict', startOnLoad: true, arrowMarkerAbsolute: false, er: { diagramPadding: 20, layoutDirection: 'TB', minEntityWidth: 100, minEntityHeight: 75, entityPadding: 15, stroke: 'gray', fill: 'honeydew', fontSize: 12, useMaxWidth: true, }, flowchart: { diagramPadding: 8, htmlLabels: true, curve: 'basis', }, sequence: { diagramMarginX: 50, diagramMarginY: 10, actorMargin: 50, width: 150, height: 65, boxMargin: 10, boxTextMargin: 5, noteMargin: 10, messageMargin: 35, messageAlign: 'center', mirrorActors: true, bottomMarginAdj: 1, useMaxWidth: true, rightAngles: false, showSequenceNumbers: false, }, gantt: { titleTopMargin: 25, barHeight: 20, barGap: 4, topPadding: 50, leftPadding: 75, gridLineStartPadding: 35, fontSize: 11, fontFamily: '\"Open-Sans\", \"sans-serif\"', numberSectionStyles: 4, axisFormat: '%Y-%m-%d', topAxis: false, }, }; mermaid.initialize(mermaidConfig);  flowchart LR subgraph b1[push_notebooks - runs on push to develop] direction LR subgraph b2[COAsT site - markdown ] direction TB a[checkout docsy site] --b b[checkout coast] --c c[create environment] --d d[execute notebooks] --e e[covert notebooks to MD] --f f[move images to static dir] --g g[commit changes] end t[Repository Dispatch] -- event pushed -- b2 end click a \"https://github.com/British-Oceanographic-Data-Centre/COAsT-site\" \"Docsy site for COAsT repo\"    let isDark = window.matchMedia('(prefers-color-scheme: dark)').matches; let mermaidTheme = (isDark) ? 'dark' : 'default'; let mermaidConfig = { theme: mermaidTheme, logLevel: 'fatal', securityLevel: 'strict', startOnLoad: true, arrowMarkerAbsolute: false, er: { diagramPadding: 20, layoutDirection: 'TB', minEntityWidth: 100, minEntityHeight: 75, entityPadding: 15, stroke: 'gray', fill: 'honeydew', fontSize: 12, useMaxWidth: true, }, flowchart: { diagramPadding: 8, htmlLabels: true, curve: 'basis', }, sequence: { diagramMarginX: 50, diagramMarginY: 10, actorMargin: 50, width: 150, height: 65, boxMargin: 10, boxTextMargin: 5, noteMargin: 10, messageMargin: 35, messageAlign: 'center', mirrorActors: true, bottomMarginAdj: 1, useMaxWidth: true, rightAngles: false, showSequenceNumbers: false, }, gantt: { titleTopMargin: 25, barHeight: 20, barGap: 4, topPadding: 50, leftPadding: 75, gridLineStartPadding: 35, fontSize: 11, fontFamily: '\"Open-Sans\", \"sans-serif\"', numberSectionStyles: 4, axisFormat: '%Y-%m-%d', topAxis: false, }, }; mermaid.initialize(mermaidConfig);  flowchart LR subgraph b3[push_docstrings - runs on push to master] direction LR subgraph b4[COAsT site - docstrings ] direction TB a1[checkout docsy site] --b1 b1[checkout coast] --c1 c1[add python] --d1 d1[covert docstrings] --e1 e1[commit changes] end r[Repository Dispatch] -- event pushed -- b4 end click a1 \"https://github.com/British-Oceanographic-Data-Centre/COAsT-site\" \"Docsy site for COAsT repo\"  Generate unit test contents file   let isDark = window.matchMedia('(prefers-color-scheme: dark)').matches; let mermaidTheme = (isDark) ? 'dark' : 'default'; let mermaidConfig = { theme: mermaidTheme, logLevel: 'fatal', securityLevel: 'strict', startOnLoad: true, arrowMarkerAbsolute: false, er: { diagramPadding: 20, layoutDirection: 'TB', minEntityWidth: 100, minEntityHeight: 75, entityPadding: 15, stroke: 'gray', fill: 'honeydew', fontSize: 12, useMaxWidth: true, }, flowchart: { diagramPadding: 8, htmlLabels: true, curve: 'basis', }, sequence: { diagramMarginX: 50, diagramMarginY: 10, actorMargin: 50, width: 150, height: 65, boxMargin: 10, boxTextMargin: 5, noteMargin: 10, messageMargin: 35, messageAlign: 'center', mirrorActors: true, bottomMarginAdj: 1, useMaxWidth: true, rightAngles: false, showSequenceNumbers: false, }, gantt: { titleTopMargin: 25, barHeight: 20, barGap: 4, topPadding: 50, leftPadding: 75, gridLineStartPadding: 35, fontSize: 11, fontFamily: '\"Open-Sans\", \"sans-serif\"', numberSectionStyles: 4, axisFormat: '%Y-%m-%d', topAxis: false, }, }; mermaid.initialize(mermaidConfig);  graph LR subgraph generate-test-contents - runs on pull_request A[checkout COAsT]--B; B[install package]--C; C[make example files dir]-- D; D[run generate_unit_test_contents.py]--E E[commit changes] end;  COAsT-site These are the actions used on the COAsT-site repo.\nConvert to markdown See Interactions with other repos for the related markdown and docstring workflows\nBuild site   let isDark = window.matchMedia('(prefers-color-scheme: dark)').matches; let mermaidTheme = (isDark) ? 'dark' : 'default'; let mermaidConfig = { theme: mermaidTheme, logLevel: 'fatal', securityLevel: 'strict', startOnLoad: true, arrowMarkerAbsolute: false, er: { diagramPadding: 20, layoutDirection: 'TB', minEntityWidth: 100, minEntityHeight: 75, entityPadding: 15, stroke: 'gray', fill: 'honeydew', fontSize: 12, useMaxWidth: true, }, flowchart: { diagramPadding: 8, htmlLabels: true, curve: 'basis', }, sequence: { diagramMarginX: 50, diagramMarginY: 10, actorMargin: 50, width: 150, height: 65, boxMargin: 10, boxTextMargin: 5, noteMargin: 10, messageMargin: 35, messageAlign: 'center', mirrorActors: true, bottomMarginAdj: 1, useMaxWidth: true, rightAngles: false, showSequenceNumbers: false, }, gantt: { titleTopMargin: 25, barHeight: 20, barGap: 4, topPadding: 50, leftPadding: 75, gridLineStartPadding: 35, fontSize: 11, fontFamily: '\"Open-Sans\", \"sans-serif\"', numberSectionStyles: 4, axisFormat: '%Y-%m-%d', topAxis: false, }, }; mermaid.initialize(mermaidConfig);  graph LR subgraph hugo - runs on push to master A[checkout site]--B; B[Setup Hugo] -- v0.70.0 --C; C[Setup Nodejs]-- v12 -- D; D[Build]--E E[Deploy] end;  ","excerpt":"GitHub actions diagram This is a collection of flowcharts for all the GitHub actions used across the …","ref":"/COAsT/docs/contributing-docs/github_actions_flowchart/","title":"Github Actions Flowchart"},{"body":"For COAsT development we use a Github workflow to manage version control and collaboration. Git allows use to keep track of changes made to the COAsT code base, avoid breaking existing code and work as a group on a single package. Any contributor needs to use this workflow to add their code. Below is some guidance on using git with COAsT, including a typical workflow and cheat sheet.\nFor more information on git, see:\nGithub (https://github.com/)\nThe Github page for this package can be found:\nhere\nKey Ideas   The COAsT repository has two core branches: master and develop. The master branch contains the tested code that you install when using Anaconda. This is updated less frequently, and is the \u0026ldquo;user-facing\u0026rdquo; branch of code. Most contributors do not need to edit this branch. The develop branch is the \u0026lsquo;pre-master\u0026rsquo; branch, where working code is kept. This is the leading branch, with the most up-to-date code, although it is not necessarily user-facing. When writing code into your own branch (see below), it is \u0026lsquo;branched\u0026rsquo; from develop and then eventually merged back into develop. You should never make changes directly to either master or develop.\n  There is a \u0026lsquo;local\u0026rsquo; and \u0026lsquo;remote\u0026rsquo; copy of the COAsT repository. The local repository exists only on your machine. The remote repository is the one you see on the Github website and exists separately. The two versions of the repository can be synchronised at a single point using commands such as git pull git push and git fetch (see below). After cloning (downloading) the repository, all modifications you make/add/commit will only be local until you push them to the remote repository.\n  Typical Workflow A typical workflow for editting COAsT in git might look like:\n  Clone Repository: git clone git@github.com:British-Oceanographic-Data-Centre/COAsT.git. This will create a new copy of COAsT on your local system which you can use to interact with git and view/edit the source code. This only needs to be done once.\n  Checkout develop: git checkout develop. Before creating a new branch for your code, you should checkout the develop branch. This will switch your local repository to the develop branch. You can check what branch your current local repository is in by entering git branch \u0026ndash; it should now say develop\n  Create/checkout your new branch: git checkout -b new_branch_name. This will create and checkout your new branch \u0026ndash; right now it is an identical copy of develop. However, any changes you commit to your local repository will be saved into your branch. Once you have created your branch, you can open it as before, using git checkout new_branch_name.\n  Make changes/additions to code: Make any changes you like to COAsT. At this point it is separate from the main branches and it is safe to do so. If in doubt, enter git branch again to ensure you are within your own branch.\n  Add changes to branch: git add modified_file. Using this command will tell git that you have changed/added this file and you want to save it to the branch you are currently in. Upon entering this command, the file changes/additions are not saved to the branch and won\u0026rsquo;t be until the next step. You can remove an added file by entering git reset modified_file and can check which files have changed by typing git status.\n  Commit changes to branch: git commit -m \u0026quot;type a message in quotations\u0026quot;. Entering this command will \u0026ldquo;save\u0026rdquo; the changes you added using git add  in the step above to the branch you are currently in. Once entered, git will identify what has changed since the previous commit. If this is the first commit in your new branch then since the version of develop that you branch from. This will not change any other branch except the one you are in and you can/should do this often with an appropriate message. At this point, all changes are still only on your local machine and will not change the remote repository. It is also possible to undo a commit using git revert, so nothing is unfixable.\n  Continue modifying code: At this point, you may want to continue modifying the code, repeatedly adding changes and commiting them to your local repository, as above.\n  Push your local repository to the remote: git push origin. This will upload the changes you have made in the branch you are in (and only this branch) to the remote (website) repository. If this is the first time you have pushed this branch then an error may appear telling you to repush with the --set-upstream flag enable. Simply copy and paste this command back into the terminal. This will \u0026ldquo;create\u0026rdquo; your branch in the remote repository. Once pushed, github will do some auto-checks to make sure the code works (which it may not, but that is fine). You can continue to modify the code at any point, and push multiple times. This is encouraged if sharing with other collaboraters.\n  Once you are satisfied with your changes, move onto the next steps.\n Make sure your local branch is up to date with the remote: git pull origin when in your branch. This is to ensure that nobody else has changed your branch, or if they have to update your local branch with the changes on the remote.\n  Update your branch with develop:. Before requesting that your branch and its changes be merged back into the develop branch, it is good practice to first merge develop back into your branch. This is because develop may have changed since you started working on your branch and these changes should be merged into your branch to ensure that conflicts are resolved. To do this, first update develop by entering git checkout develop and git pull. This will update the develop branch on your local machine. Then merge develop back into your branch by entering git checkout your_branch and git merge develop. This may say up-to-date (in which case GREAT), or successful (in which case GREAT) or may say there are some conflicts. This happens when more than one person has changed the same piece of code.\n  Resolve Conflicts: This step may not be necessary if there are no conflicts. If git tells you there are conflicts, it will also tell you which files they occur in. For more information/help with conflict resolution see here\n  Create a pull request for your branch. First your most up to date branch using git push origin, even after merging develop in step 9/10. On the website you may then create a \u0026lsquo;pull request\u0026rsquo; which is a formal way of saying you want to merge your branch back into develop. A pull request allows you to ask people to \u0026lsquo;review\u0026rsquo; your branch, share your code, view the changes in your branch and other things. To make a pull request, go to the website, click on the pull requests tab and click Create new pull request. Then select your branch in the right drop down menu and develop in the left. You may then enter a description of the changes you have made and anything else you would like reviewers to see.\n  Reviewers review the code: Requested reviewers take a look at your changes and run the unit_test. Once they are satisfied, they will approve the pull request, or add comments about any problems.\n  Merge branch into develop: Once reviewers are satisfied, you may click Merge branch at the bottom of the pull request. Now your changes will be added into develop! Again, this is fine as the branch has been inspected by reviewers and any change can be reverted using git revert (although this is not encouraged for the develop branch).\n  **Note: After creating a pull request, Github will automatically apply \u0026ldquo;black formatting\u0026rdquo; to the code. This will commit new (small) changes to the branch so you should always do a git pull on your branch to make sure your local version is up to date with the remote.\nCondensed Workflow  git clone git@github.com:British-Oceanographic-Data-Centre/COAsT.git. git checkout develop git checkout -b new_branch_name Make changes git add changed_file git commit -m \u0026quot;what changes have you made\u0026quot; git push origin If your branch changed by anyone else, git pull Repeat steps 4-8 git checkout develop git pull git checkout your_branch git merge develop git push origin Create pull request from your_branch to develop, include description and request reviewers. Reviewers accept, Merge branch.  ","excerpt":"For COAsT development we use a Github workflow to manage version control and collaboration. Git …","ref":"/COAsT/docs/contributing-docs/github_workflow/","title":"Github Workflow"},{"body":"COAsT utilises Python’s default logging library and includes a simple setup function for those unfamiliar with how to use it.\nimport coast coast.logging_util.setup_logging() This is all you need to enable full logging output to the console.\nBy default, setup_logging will use the \u0026ldquo;DEBUG\u0026rdquo; logging level, if you want to adjust this, you can use the flags from the logging library.\nimport coast import logging coast.logging_util.setup_logging(level=logging.INFO) Alternative logging levels in increasing levels of severity. Note logs are reported at the chosen severity level and higher:\n..., level=logging.DEBUG) # Detailed information, typically of interest only when diagnosing problems. ..., level=logging.INFO) # Confirmation that things are working as expected. ..., level=logging.WARNING) # An indication that something unexpected happened, or indicative of some problem in the near future (e.g. ‘disk space low’). The software is still working as expected. ..., level=logging.ERROR) # Due to a more serious problem, the software has not been able to perform some function ..., level=logging.CRITICAL) # A serious error, indicating that the program itself may be unable to continue running For more info on logging levels, see the relevant Python documentation.\nLogging output will be printed in the console once enabled by default, but output can be directed to any Stream, for instance, to an opened file.\nimport coast file = open(\u0026#34;coast.log\u0026#34;, \u0026#34;w\u0026#34;) coast.logging_util.setup_logging(stream=file) coast.logging_util.info(\u0026#34;Hello World!\u0026#34;) # Your use of COAsT would go here, this line is included as an example file.close() ","excerpt":"COAsT utilises Python’s default logging library and includes a simple setup function for those …","ref":"/COAsT/docs/contributing_package/python_logging/","title":"Logging"},{"body":"The examples in Notebooks are tutorials automatically rendered from the python notebooks in COAsT:examples_scripts/notebook_tutorials. These can be downloaded and run locally with the example data.\nWithin COAsT, configuration files are used to pass information about the example data files. The configuration files used with the example data can be downloaded or linked to a local version of the COAsT repository. These files should be placed in a config directory in your working directory, and form a useful template for loading new data files.\nThis Examples section is split into:\n","excerpt":"The examples in Notebooks are tutorials automatically rendered from the python notebooks in …","ref":"/COAsT/docs/examples/","title":"Examples"},{"body":"Here you will find information needed to contribute code changes to the COAsT package.\n","excerpt":"Here you will find information needed to contribute code changes to the COAsT package.","ref":"/COAsT/docs/contributing_package/","title":"Contributing: COAsT"},{"body":"We use Hugo Extended Version to format and generate our website, the Docsy theme for styling and site structure, and GitHub pages to manage the deployment of the site. Hugo is an open-source static site generator that provides us with templates, content organisation in a standard directory structure, and a website generation engine. You write the pages in Markdown (or HTML if you want), and Hugo wraps them up into a website.\nAll submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests.\nUpdating a single page If you\u0026rsquo;ve just spotted something you\u0026rsquo;d like to change while using the docs, Docsy has a shortcut for you:\n Click Edit this page in the top right hand corner of the page. If you don\u0026rsquo;t already have an up to date fork of the project repo, you are prompted to get one - click Fork this repository and propose changes or Update your Fork to get an up to date version of the project to edit. The appropriate page in your fork is displayed in edit mode. make your edit submit a pull request with a summary of the changes  Previewing your changes locally If you want to run your own local Hugo server to preview your changes as you work:\n  Follow the instructions in Getting started to install Hugo and any other tools you need. You\u0026rsquo;ll need at least Hugo version 0.45 (we recommend using the most recent available version), and it must be the extended version, which supports SCSS.\n  Fork the COAsT-site repo repo into your own project, then create a local copy using git clone. Don’t forget to use --recurse-submodules or you won’t pull down some of the code you need to generate a working site.\ngit clone --recurse-submodules --depth 1 https://github.com/British-Oceanographic-Data-Centre/COAsT-site.git   Run npm install to install Node.js dependencies.\n  Run hugo server in the site root directory. By default your site will be available at http://localhost:1313/COAsT. Now that you\u0026rsquo;re serving your site locally, Hugo will watch for changes to the content and automatically refresh your site.\n  Continue with the usual GitHub workflow to edit files, commit them, push the changes up to your fork, and create a pull request.\n  Creating an issue If you\u0026rsquo;ve found a problem in the docs, but you\u0026rsquo;re not sure how to fix it yourself, please create an issue in the COAsT-site repo. You can also create an issue about a specific page by clicking the Create Issue button in the top right hand corner of the page.\nUseful resources  Docsy user guide: All about Docsy, including how it manages navigation, look and feel, and multi-language support. Hugo documentation: Comprehensive reference for Hugo. Github Hello World!: A basic introduction to GitHub concepts and workflow.  ","excerpt":"We use Hugo Extended Version to format and generate our website, the Docsy theme for styling and …","ref":"/COAsT/docs/contributing-docs/","title":"Contributing: Documentation"},{"body":"What is lazy\u0026hellip; \u0026hellip;loading Lazy loading determines if data is read into memory straight away (on that line of code execution) or if the loading is delayed until the data is physical altered by some function (normally mathematical in nature)\n\u0026hellip;evaluation Lazy evaluation is about delaying the execution of a method/function call until the value is physical required, normally as a graph or printed to screen. Lazy evaluation can also help with memory management, useful with large dataset, by allowing for optimisation on the chained methods calls.\nLazy loading and Lazy evaluation are offer used together, though it is not mandatory and always worth checking that both are happening.\nBeing Lazy in COAsT There are two way to be Lazy within the COAsT package.\n xarray Dask  xarray COAsT uses xarray to load NetCDF files in, by default this will be Lazy, the raw data values will not be brought into memory.\nyou can slice and subset the data while still having the lazy loading honoured, it is not until the data is altered, say via a call to NumPy.cumsum, that the required data will be loaded into memory.\nNote the data on disk (in the NetCDF file) is never altered, only the values in memory are changed.\nimport xarray as xr import NumPy as np dataset_domain = xr.open_dataset(fn_domain) e3w_0 = dataset_domain.e3w_0 # still lazy loaded e3w_0_cs = np.cumsum(e3w_0[1:, :, :], axis=0) # now in memory Dask When in use Dask will provide lazy evaluation on top of the lazy loading.\nusing the same example as above, a file loaded in using xarray, this time with the chunks option set, will not only lazy load the data, but will turn on Dask, now using either the xarray or Dask wrapper functions will mean the NumPy cumsum call is not evaluated right way, in fact it will not be evaluated until either the compute function is called, or a greedy method from another library is used.\nimport xarray as xr dataset_domain = xr.open_dataset(fn_domain, chunks={\u0026#34;t\u0026#34;: 1}) e3w_0 = dataset_domain.e3w_0 # still lazy loaded e3w_0_cs = e3w_0[1:, :, :].cumsum(axis=0) # Dask backed Lazy evaluation We discuss Dask even more here.\n","excerpt":"What is lazy\u0026hellip; \u0026hellip;loading Lazy loading determines if data is read into memory straight …","ref":"/COAsT/docs/contributing_package/lazy-loading/","title":"working Lazily"},{"body":"What is Dask Dask is a python library that allows code to be run in parallel based on the hardware your running on. This means Dask works just as well on your laptop as on your large server.\nUsing Dask Dask is included in the xarray library. When loading a data source (file/NumPy array) Dask is automatically initiated with the chunks variable in the config file. However the chunking may not be optimal but you can adjust it before computation are made.\nnemo_t = coast.Gridded( fn_data=dn_files+fn_nemo_grid_t_dat, fn_domain=dn_files+fn_nemo_dom, config=fn_config) chunks = { \u0026#34;x_dim\u0026#34;: 10, \u0026#34;y_dim\u0026#34;: 10, \u0026#34;t_dim\u0026#34;: 10, } # Chunks are prescribed in the config json file, but can be adjusted while the data is lazy loaded. nemo_t.dataset.chunk(chunks) chunks tell Dask where to break your data across the different processor tasks.\nDirect Dask Dask can be imported and used directly\nimport Dask.array as da big_array = da.multiple(array1,array2) Dask arrays follow the NumPy API. This means that most NumPy functions have a Dask version.\nPotential Issues Dask objects are immutable. This means that the classic approach, pre-allocation follow by modification will not work.\nThe following code will error.\nimport Dask.array as da e3w_0 = da.squeeze(dataset_domain.e3w_0) depth_0 = da.zero_like(e3w_0) depth_0[0, :, :] = 0.5 * e3w_0[0, :, :] # this line will error out option 1 Continue using NumPy function but wrapping the final value in a Dask array. This final Dask object will still be in-memory.\ne3w_0 = np.squeeze(dataset_domain.e3w_0) depth_0 = np.zeros_like(e3w_0) depth_0[0, :, :] = 0.5 * e3w_0[0, :, :] depth_0[1:, :, :] = depth_0[0, :, :] + np.cumsum(e3w_0[1:, :, :], axis=0) depth_0 = da.array(depth_0) option 2 Dask offers a feature called delayed. This can be used as a modifier on your complex methods as follows;\n@Dask.delayed def set_timezero_depths(self, dataset_domain): # complex workings these do not return the computed answer, rather it returns a delayed object. These delayed object get stacked, as more delayed methods are called. When the value is needed, it can be computed like so;\nne = coast.Gridded(...) # come complex delayed methods called ne.data_variable.compute() Dask will now work out a computing path via all the required methods using as many processor tasks as possible.\nVisualising the Graph Dask is fundamentally a computational graph library, to understand what is happening in the background it can help to see these graphs (on smaller/simpler problems). This can be achieved by running;\nne = coast.Gridded(...) # come complex delayed methods called ne.data_variable.visualize() this will output a png image of the graph in the calling directory and could look like this;\n  ","excerpt":"What is Dask Dask is a python library that allows code to be run in parallel based on the hardware …","ref":"/COAsT/docs/contributing_package/dask/","title":"Dask"},{"body":"This is a demonstration script for using the Altimetry object in the COAsT package. This object has strict data formatting requirements, which are outlined in altimetry.py.\nRelevant imports and filepath configuration # Begin by importing coast and other packages import coast root = \u0026#34;./\u0026#34; # And by defining some file paths dn_files = root + \u0026#34;./example_files/\u0026#34; fn_nemo_dat = dn_files + \u0026#34;coast_example_nemo_data.nc\u0026#34; fn_nemo_dom = dn_files + \u0026#34;coast_example_nemo_domain.nc\u0026#34; fn_nemo_config = root + \u0026#34;./config/example_nemo_grid_t.json\u0026#34; fn_altimetry = dn_files + \u0026#34;coast_example_altimetry_data.nc\u0026#34; fn_altimetry_config = root + \u0026#34;./config/example_altimetry.json\u0026#34; Load data # We need to load in a NEMO object for doing NEMO things. nemo = coast.Gridded(fn_nemo_dat, fn_nemo_dom, config=fn_nemo_config) # And now we can load in our Altimetry data. By default, Altimetry is set up # to read in CMEMS netCDF files. However, if no path is supplied, then the # object\u0026#39;s dataset will be initialised as None. Custom data can then be loaded # if desired, as long as it follows the data formatting for Altimetry. # altimetry = coast.Altimetry(fn_altimetry) altimetry = coast.Altimetry(fn_altimetry, config=fn_altimetry_config) ././config/example_altimetry.json Altimetry object at 0x561214f5efc0 initialised  Subsetting # Before going any further, lets just cut out the bit of the altimetry that # is over the model domain. This can be done using `subset_indices_lonlat_box` # to find relevant indices and then `isel` to extract them. The data here is then also # also thinned slightly. ind = altimetry.subset_indices_lonlat_box([-10, 10], [45, 60]) ind = ind[::4] altimetry = altimetry.isel(t_dim=ind) Subsetting Altimetry object at 0x561214f5efc0 indices in [-10, 10], [45, 60]  Model interpolation # Before comparing our observations to the model, we will interpolate a model # variable to the same time and geographical space as the altimetry. This is # done using the obs_operator() method: altimetry.obs_operator(nemo, mod_var_name=\u0026#34;ssh\u0026#34;, time_interp=\u0026#34;nearest\u0026#34;) # Doing this has created a new interpolated variable called interp_ssh and # saved it back into our Altimetry object. Take a look at altimetry.dataset # to see for yourself. Interpolating Gridded object at 0x561214f5efc0 \u0026quot;ssh\u0026quot; with time_interp \u0026quot;nearest\u0026quot;  #altimetry.dataset # uncomment to print data object summary Interpolated vs observed # Next we will compare this interpolated variable to an observed variable # using some basic metrics. The basic_stats() routine can be used for this, # which calculates some simple metrics including differences, RMSE and # correlations. NOTE: This may not be a wise choice of variables. stats = altimetry.basic_stats(\u0026#34;ocean_tide_standard_name\u0026#34;, \u0026#34;interp_ssh\u0026#34;) Altimetry object at 0x561214f5efc0 initialised /usr/share/miniconda/envs/coast/lib/python3.8/site-packages/coast/data/altimetry.py:351: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning. corr = pdvar.corr(method=method) /usr/share/miniconda/envs/coast/lib/python3.8/site-packages/coast/data/altimetry.py:365: FutureWarning: The default value of numeric_only in DataFrame.cov is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning. cov = pdvar.cov()  # Take a look inside stats.dataset to see all of the new variables. When using # basic stats, the returned object is also an Altimetry object, so all of the # same methods can be applied. Alternatively, if you want to save the new # metrics to the original altimetry object, set \u0026#39;create_new_object = False\u0026#39;. #stats.dataset # uncomment to print data object summary # Now we will do a more complex comparison using the Continuous Ranked # Probability Score (CRPS). For this, we need to hand over the model object, # a model variable and an observed variable. We also give it a neighbourhood # radius in km (nh_radius). crps = altimetry.crps(nemo, model_var_name=\u0026#34;ssh\u0026#34;, obs_var_name=\u0026#34;ocean_tide_standard_name\u0026#34;, nh_radius=20) # Again, take a look inside `crps.dataset` to see some new variables. Similarly # to basic_stats, `create_new_object` keyword arg can be set to `false` to save output to # the original altimetry object. #crps.dataset # uncomment to print data object summary Altimetry object at 0x561214f5efc0 initialised  Plotting data # Altimetry has a ready built quick_plot() routine for taking a look at any # of the observed or derived quantities above. So to take a look at the # \u0026#39;ocean_tide_standard_name\u0026#39; variable: fig, ax = altimetry.quick_plot(\u0026#34;ocean_tide_standard_name\u0026#34;) /usr/share/miniconda/envs/coast/lib/python3.8/site-packages/cartopy/io/__init__.py:241: DownloadWarning: Downloading: https://naturalearth.s3.amazonaws.com/50m_physical/ne_50m_coastline.zip warnings.warn(f'Downloading: {url}', DownloadWarning)  # As stats and crps are also `altimetry` objects, quick_plot() can also be used: fig, ax = crps.quick_plot(\u0026#34;crps\u0026#34;) # stats quick_plot: fig, ax = stats.quick_plot(\u0026#34;absolute_error\u0026#34;) ","excerpt":"This is a demonstration script for using the Altimetry object in the COAsT package. This object has …","ref":"/COAsT/docs/examples/notebooks/altimetry/altimetry_tutorial/","title":"Altimetry tutorial"},{"body":"","excerpt":"","ref":"/COAsT/docs/examples/notebooks/altimetry/","title":"Altimety"},{"body":"This demonstration has two parts:\n  Climatology.make_climatology(): This demonstration uses the COAsT package to calculate a climatological mean of an input dataset at a desired output frequency. Output can be written straight to file.\n  Climatology.make_multiyear_climatology(): This demonstrations uses the COAsT package to calculate a climatological mean of an input dataset at a desired output frequency, over multiple years, but will work with single year datasets too.\n  COAsT and xarray should preserve any lazy loading and chunking. If defined properly in the read function, memory issues can be avoided and parallel processes will automatically be used.\nimport coast Usage of coast.Climatology.make_climatology(). Calculates mean over a given period of time. This doesn\u0026rsquo;t take different years into account, unless using the \u0026lsquo;years\u0026rsquo; frequency.\nroot = \u0026#34;./\u0026#34; # Paths to a single or multiple data files. dn_files = root + \u0026#34;./example_files/\u0026#34; fn_nemo_dat = dn_files + \u0026#34;coast_example_nemo_data.nc\u0026#34; fn_nemo_config = root + \u0026#34;./config/example_nemo_grid_t.json\u0026#34; # Set path for domain file if required. fn_nemo_dom = dn_files + \u0026#34;coast_example_nemo_domain.nc\u0026#34; # Define output filepath (optional: None or str) fn_out = None # Read in multiyear data (This example uses NEMO data from a single file.) nemo_data = coast.Gridded(fn_data=fn_nemo_dat, fn_domain=fn_nemo_dom, config=fn_nemo_config, ).dataset Calculate the climatology for temperature and sea surface height (ssh) as an example:\n# Select specific data variables. data = nemo_data[[\u0026#34;temperature\u0026#34;, \u0026#34;ssh\u0026#34;]] # Define frequency -- Any xarray time string: season, month, etc climatology_frequency = \u0026#34;month\u0026#34; # Calculate the climatology and write to file. clim = coast.Climatology() clim_mean = clim.make_climatology(data, climatology_frequency, fn_out=fn_out) Below shows the structure of a dataset returned, containing 1 month worth of meaned temperature and sea surface height data:\n#clim_mean # uncomment to print data object summary Usage of coast.Climatology.multiyear_averages(). Calculates the mean over a specified period and groups the data by year-period. Here a fully working example is not available as multi-year example data is not in the example_files. However a working example using synthetic data is given in: tests/test_climatology.py. This method is designed to be compatible with multi-year datasets, but will work with single year datasets too.\n# Paths to a single or multiple data files. fn_nemo_data = \u0026quot;/path/to/nemo/*.nc\u0026quot; # Set path for domain file if required. fn_nemo_domain = None # Set path to configuration file fn_nemo_config = \u0026quot;/path/to/nemo/*.json\u0026quot; # Read in multiyear data (This example uses NEMO data from multiple datafiles.) nemo_data = coast.Gridded(fn_data=fn_nemo_data, fn_domain=fn_nemo_domain, config=fn_nemo_config, multiple=True).dataset Now calculate temperature and ssh means of each season across multiple years for specified data, using seasons module to specify time period.\nfrom coast._utils import seasons # Select specific data variables. data = nemo_data[[\u0026quot;temperature\u0026quot;, \u0026quot;ssh\u0026quot;]] clim = coast.Climatology() # SPRING, SUMMER, AUTUMN, WINTER, ALL are valid values for seasons. clim_multiyear = clim.multiyear_averages(data, seasons.ALL, time_var='time', time_dim='t_dim') # Or explicitly defining specific month periods. # A list of tuples defining start and end month integers. The start months should be in chronological order. # (you may need to read/load the data again if it gives an error) month_periods = [(1,2), (12,2)] # Specifies January -\u0026gt; February and December -\u0026gt; February for each year of data. clim_multiyear = clim.multiyear_averages(data, month_periods , time_var='time', time_dim='t_dim') ","excerpt":"This demonstration has two parts:\n  Climatology.make_climatology(): This demonstration uses the …","ref":"/COAsT/docs/examples/notebooks/general/climatology_tutorial/","title":"Climatology tutorial"},{"body":"Contour subsetting (a vertical slice of data along a contour).\nThis is a demonstration script for using the Contour class in the COAsT package. This object has strict data formatting requirements, which are outlined in contour.py.\nThe code is taken directly from unit_tesing/unit_test.py\nIn this tutorial we take a look the following Isobath Contour Methods:\na. Extract isbath contour between two points b. Plot contour on map c. Calculate pressure along contour d. Calculate flow across contour e. Calculate pressure gradient driven flow across contour  Load packages and define some file paths. import coast import matplotlib.pyplot as plt # Define some file paths root = \u0026#34;./\u0026#34; dn_files = root + \u0026#34;./example_files/\u0026#34; fn_nemo_dat_t = dn_files + \u0026#34;nemo_data_T_grid.nc\u0026#34; fn_nemo_dat_u = dn_files + \u0026#34;nemo_data_U_grid.nc\u0026#34; fn_nemo_dat_v = dn_files + \u0026#34;nemo_data_V_grid.nc\u0026#34; fn_nemo_dom = dn_files + \u0026#34;coast_example_nemo_domain.nc\u0026#34; # Configuration files describing the data files fn_config_t_grid = root + \u0026#34;./config/example_nemo_grid_t.json\u0026#34; fn_config_f_grid = root + \u0026#34;./config/example_nemo_grid_f.json\u0026#34; fn_config_u_grid = root + \u0026#34;./config/example_nemo_grid_u.json\u0026#34; fn_config_v_grid = root + \u0026#34;./config/example_nemo_grid_v.json\u0026#34; Extract isobath contour between two points and create contour object. Create a gridded object with the grid only.\nnemo_f = coast.Gridded(fn_domain=fn_nemo_dom, config=fn_config_f_grid) Then create a contour object on the 200m isobath.\ncontours, no_contours = coast.Contour.get_contours(nemo_f, 200) Extract the indices for the contour in a specified box.\ny_ind, x_ind, contour = coast.Contour.get_contour_segment(nemo_f, contours[0], [50, -10], [60, 3]) Extract the contour for the specified indices.\ncont_f = coast.ContourF(nemo_f, y_ind, x_ind, 200) Plot contour on map plt.figure() coast.Contour.plot_contour(nemo_f, contour) plt.show() \u0026lt;Figure size 640x480 with 0 Axes\u0026gt;  Calculate pressure along contour. Repeat the above procedure but on t-points.\nnemo_t = coast.Gridded(fn_data=fn_nemo_dat_t, fn_domain=fn_nemo_dom, config=fn_config_t_grid) contours, no_contours = coast.Contour.get_contours(nemo_t, 200) y_ind, x_ind, contour = coast.Contour.get_contour_segment(nemo_t, contours[0], [50, -10], [60, 3]) cont_t = coast.ContourT(nemo_t, y_ind, x_ind, 200) Now contruct pressure along this contour segment.\ncont_t.construct_pressure(1027) # This creates ``cont_t.data_contour.pressure_s`` and ``cont_t.data_contour.pressure_h_zlevels`` fields. Calculate flow across contour. Create the contour segement on f-points again.\nnemo_f = coast.Gridded(fn_domain=fn_nemo_dom, config=fn_config_f_grid) nemo_u = coast.Gridded(fn_data=fn_nemo_dat_u, fn_domain=fn_nemo_dom, config=fn_config_u_grid) nemo_v = coast.Gridded(fn_data=fn_nemo_dat_v, fn_domain=fn_nemo_dom, config=fn_config_v_grid) contours, no_contours = coast.Contour.get_contours(nemo_f, 200) y_ind, x_ind, contour = coast.Contour.get_contour_segment(nemo_f, contours[0], [50, -10], [60, 3]) cont_f = coast.ContourF(nemo_f, y_ind, x_ind, 200) Calculate the flow across the contour, pass u- and v- gridded velocity objects.\ncont_f.calc_cross_contour_flow(nemo_u, nemo_v) # This creates fields ``cont_f.data_cross_flow.normal_velocities`` and ## ``cont_f.data_cross_flow.depth_integrated_normal_transport`` Calculate pressure gradient driven flow across contour. The \u0026ldquo;calc_geostrophic_flow()\u0026rdquo; operates on f-grid objects and requires configuration files for the u- and v- grids.\ncont_f.calc_geostrophic_flow(nemo_t, config_u=fn_config_u_grid, config_v=fn_config_v_grid, ref_density=1027) \u0026#34;\u0026#34;\u0026#34; This constructs: cont_f.data_cross_flow.normal_velocity_hpg cont_f.data_cross_flow.normal_velocity_spg cont_f.data_cross_flow.transport_across_AB_hpg cont_f.data_cross_flow.transport_across_AB_spg \u0026#34;\u0026#34;\u0026#34; '\\n This constructs:\\n cont_f.data_cross_flow.normal_velocity_hpg\\n cont_f.data_cross_flow.normal_velocity_spg\\n cont_f.data_cross_flow.transport_across_AB_hpg\\n cont_f.data_cross_flow.transport_across_AB_spg\\n'  ","excerpt":"Contour subsetting (a vertical slice of data along a contour).\nThis is a demonstration script for …","ref":"/COAsT/docs/examples/notebooks/gridded/contour_tutorial/","title":"Contour tutorial"},{"body":"The notebook proves a template and some instruction on how to create a dask wrapper\nMotivation Start with an xarray.DataArray object called myDataArray, that we want to pass into a function. That function will perform eager evaluation and return a numpy array, but we want lazy evaluation with the possibility to allow dask parallelism. See worked example in Process_data.seasonal_decomposition.\nImport dependencies import dask.array as da from dask import delayed import xarray as xr import numpy as np Step 1. (optional: allows dask to distribute computation across multiple cores, if not interested see comment 2) Partition data in myDataArray by chunking it up as desired. Note that chunking dimensions need to make sense for your particular problem! Here we just chunk along dim_2\nmyDataArray = myDataArray.chunk({\u0026quot;dim_1\u0026quot;: myDataArray.dim_1.size, \u0026quot;dim_2\u0026quot;: chunksize}) # can be more dimensions Then create a list containing all the array chunks as dask.delayed objects (e.g. 4 chunks =\u0026gt; list contain 4 delayed objects)\nmyDataArray_partitioned = myDataArray.data.to_delayed().ravel() Comment 1 There are different ways to partition your data. For example, if you start off with a numpy array rather than an xarray DataArray you can just iterate over the array and partition it that way (the partitions do NOT need to be dask.delayed objects). For example see the very simple case here: https://docs.dask.org/en/stable/delayed.html\nThe method described in 1 is just very convenient for DataArrays where the multi-dimensional chunks may be the desired way to partition the data.\nStep 2. Call your eager evaluating function using dask.delayed and pass in your data. This returns a list containing the outputs from the function as dask.delayed objects. The list will have the same length as myDataArray_partitioned\ndelayed_myFunction_output = [ delayed(myFunction)(aChunk, other_args_for_myFunction) for aChunk in myDataArray_partitioned ] Step 3. Convert the lists of delayed objects to lists of dask arrays to allow array operations. It\u0026rsquo;s possible this step is not necessary!\ndask_array_list = [] for chunk_idx, aChunk in enumerate(delayed_myFunction_output): # When converting from dask.delayed to dask.array, you must know the shape of the # array. In this example we know this from the chunk sizes of the original DataArray chunk_shape = (myDataArray.chunks[0][0], myDataArray.chunks[1][chunk_idx]) dask_array_list.append(da.from_delayed(aChunk, shape=chunk_shape, dtype=float)) Step 4. Concatenate the array chunks together to get a single dask.array. This can be assigned to a new DataArray as desired.\nmyOutputArray = da.concatenate(dask_array_list, axis=1) Comment 2 If you skipped step 1., i.e. just want a lazy operation and no parallelism, you can just do this\nmyOutputArray = da.from_delayed( delayed(myFunction)(myDataArray, other_args_for_myFunction), shape=myDataArray.shape, dtype=float ) ","excerpt":"The notebook proves a template and some instruction on how to create a dask wrapper\nMotivation Start …","ref":"/COAsT/docs/examples/notebooks/general/dask_wrapper_template_tutorial/","title":"Dask wrapper template tutorial"},{"body":"Using COAsT to compute the Empirical Orthogonal Functions (EOFs) of your data\nRelevant imports and filepath configuration # Begin by importing coast and other packages import coast import xarray as xr import matplotlib.pyplot as plt # Define some file paths root = \u0026#34;./\u0026#34; dn_files = root + \u0026#34;./example_files/\u0026#34; fn_nemo_dat_t = dn_files + \u0026#34;nemo_data_T_grid.nc\u0026#34; fn_nemo_dom = dn_files + \u0026#34;coast_example_nemo_domain.nc\u0026#34; fn_nemo_config = root + \u0026#34;./config/example_nemo_grid_t.json\u0026#34; Loading data # Load data variables that are on the NEMO t-grid nemo_t = coast.Gridded( fn_data = fn_nemo_dat_t, fn_domain = fn_nemo_dom, config = fn_nemo_config ) Compute EOF For a variable (or subset of a variable) with two spatial dimensions and one temporal dimension, i.e. (x,y,t), the EOFs, temporal projections and variance explained can be computed by calling the ‘eofs’ method, and passing in the ssh DataArray as an argument. For example, for the sea surface height field, we can do\neof_data = coast.compute_eofs( nemo_t.dataset.ssh ) The method returns an xarray dataset that contains the EOFs, temporal projections and variance as DataArrays\n#eof_data # uncomment to print data object summary Inspect EOFs The variance explained of the first four modes is\n# eof_data.variance.sel(mode=[1,2,3,4]) ## uncomment Plotting And the EOFs and temporal projections can be quick plotted:\neof_data.EOF.sel(mode=[1,2,3,4]).plot.pcolormesh(col=\u0026#39;mode\u0026#39;,col_wrap=2,x=\u0026#39;longitude\u0026#39;,y=\u0026#39;latitude\u0026#39;) \u0026lt;xarray.plot.facetgrid.FacetGrid at 0x7f2f88570df0\u0026gt;  eof_data.temporal_proj.sel(mode=[1,2,3,4]).plot(col=\u0026#39;mode\u0026#39;,col_wrap=2,x=\u0026#39;time\u0026#39;) \u0026lt;xarray.plot.facetgrid.FacetGrid at 0x7f2f447cb970\u0026gt;  Complex EOFs The more exotic hilbert complex EOFs can also be computed to investigate the propagation of variability, for example:\nheof_data = coast.compute_hilbert_eofs( nemo_t.dataset.ssh ) #heof_data # uncomment to print data object summary now with the modes expressed by their amplitude and phase, the spatial propagation of the variability can be examined through the EOF_phase.\n","excerpt":"Using COAsT to compute the Empirical Orthogonal Functions (EOFs) of your data\nRelevant imports and …","ref":"/COAsT/docs/examples/notebooks/general/eof_tutorial/","title":"Eof tutorial"},{"body":"This is a demonstration script for how to export intermediate data from COAsT to netCDF files for later analysis or storage. The tutorial showcases the xarray.to_netcdf() method. http://xarray.pydata.org/en/stable/generated/xarray.Dataset.to_netcdf.html\nBegin by importing COAsT and other packages import coast import xarray as xr Now define some file paths root = \u0026#34;./\u0026#34; # And by defining some file paths dn_files = root + \u0026#34;./example_files/\u0026#34; fn_nemo_dat = dn_files + \u0026#34;coast_example_nemo_data.nc\u0026#34; fn_nemo_dom = dn_files + \u0026#34;coast_example_nemo_domain.nc\u0026#34; config = root + \u0026#34;./config/example_nemo_grid_t.json\u0026#34; ofile = \u0026#34;example_export_output.nc\u0026#34; # The target filename for output We need to load in a NEMO object for doing NEMO things nemo = coast.Gridded(fn_nemo_dat, fn_nemo_dom, config=config) We can export the whole xr.DataSet to a netCDF file Other file formats are available. From the documentation:\n NETCDF4: Data is stored in an HDF5 file, using netCDF4 API features. NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only netCDF 3 compatible API features. NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format, which fully supports 2+ GB files, but is only compatible with clients linked against netCDF version 3.6.0 or later. NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not handle 2+ GB files very well.  Mode - \u0026lsquo;w\u0026rsquo; (write) is the default. Other options from the documentation:\n mode ({\u0026ldquo;w\u0026rdquo;, \u0026ldquo;a\u0026rdquo;}, default: \u0026ldquo;w\u0026rdquo;) – Write (‘w’) or append (‘a’) mode. If mode=’w’, any existing file at this location will be overwritten. If mode=’a’, existing variables will be overwritten.  Similarly xr.DataSets collections of variables or xr.DataArray variables can be exported to netCDF for objects in the TRANSECT, TIDEGAUGE, etc classes.\nnemo.dataset.to_netcdf(ofile, mode=\u0026#34;w\u0026#34;, format=\u0026#34;NETCDF4\u0026#34;) Alternatively a single variable (an xr.DataArray object) can be exported nemo.dataset[\u0026#34;temperature\u0026#34;].to_netcdf(ofile, format=\u0026#34;NETCDF4\u0026#34;) Check the exported file is as you expect Perhaps by using ncdump -h example_export_output.nc, or load the file and see that the xarray structure is preserved.\nobject = xr.open_dataset(ofile) object.close() # close file associated with this object ","excerpt":"This is a demonstration script for how to export intermediate data from COAsT to netCDF files for …","ref":"/COAsT/docs/examples/notebooks/general/export_to_netcdf_tutorial/","title":"Export to netcdf tutorial"},{"body":"This page will walk you though a simple setup for hugo extended - which is needed if want to view any changes you make to this site locally.\nFor more details please read this.\nInstallation Manual  Download hugo extended from GitHub Unzip into preferred location (I use C:\\hugo) Add to OS PATH  optional but makes usage easier    Via a Package Manager On Windows you can use Chocolately to install with:\nchoco install hugo-extended Or on macOS/Linux you can use Homebrew to install with:\nbrew install hugo Try it out! You should now be able to try the following in a terminal\n$ hugo --help if you have cloned the COAsT-site repo you should also now be able to;\n$ cd COAsT-site $ hugo server the above will start a local hugo powered version of the website. you can edit any of the files under /content and see your changes at http://localhost:1313/COAsT/\n","excerpt":"This page will walk you though a simple setup for hugo extended - which is needed if want to view …","ref":"/COAsT/docs/contributing-docs/hugo/","title":"setting up Hugo"},{"body":"An introduction to the Gridded class. Loading variables and grid information.\nThis is designed to be a brief introduction to the Gridded class including: 1. Creation of a Gridded object 2. Loading data into the Gridded object. 3. Combining Gridded output and Gridded domain data. 4. Interrogating the Gridded object. 5. Basic manipulation ans subsetting 6. Looking at the data with matplotlib\nLoading and Interrogating Begin by importing COAsT and define some file paths for NEMO output data and a NEMO domain, as an example of model data suitable for the Gridded object.\nimport coast import matplotlib.pyplot as plt import datetime import numpy as np # Define some file paths root = \u0026#34;./\u0026#34; dn_files = root + \u0026#34;./example_files/\u0026#34; fn_nemo_dat = dn_files + \u0026#34;coast_example_nemo_data.nc\u0026#34; fn_nemo_dom = dn_files + \u0026#34;coast_example_nemo_domain.nc\u0026#34; fn_config_t_grid = root + \u0026#34;./config/example_nemo_grid_t.json\u0026#34; We can create a new Gridded object by simple calling coast.Gridded(). By passing this a NEMO data file and a NEMO domain file, COAsT will combine the two into a single xarray dataset within the Gridded object. Each individual Gridded object should be for a specified NEMO grid type, which is specified in a configuration file which is also passed as an argument. The Dask library is switched on by default, chunking can be specified in the configuration file.\nnemo_t = coast.Gridded(fn_data = fn_nemo_dat, fn_domain = fn_nemo_dom, config=fn_config_t_grid) Our new Gridded object nemo_t contains a variable called dataset, which holds information on the two files we passed. Let’s have a look at this:\n#nemo_t.dataset # uncomment to print data object summary This is an xarray dataset, which has all the information on netCDF style structures. You can see dimensions, coordinates and data variables. At the moment, none of the actual data is loaded to memory and will remain that way until it needs to be accessed.\nAlong with temperature (which has been renamed from votemper) a number of other things have happen under the hood:\n The dimensions have been renamed to t_dim, x_dim, y_dim, z_dim The coordinates have been renamed to time, longitude, latitude and depth_0. These are the coordinates for this grid (the t-grid). Also depth_0 has been calculated as the 3D depth array at time zero. The variables e1, e2 and e3_0 have been created. These are the metrics for the t-grid in the x-dim, y-dim and z_dim (at time zero) directions.  So we see that the Gridded class has standardised some variable names and created an object based on this discretisation grid by combining the appropriate grid information with all the variables on that grid.\nWe can interact with this as an xarray Dataset object. So to extract a specific variable (say temperature):\nssh = nemo_t.dataset.ssh #ssh # uncomment to print data object summary Or as a numpy array:\nssh_np = ssh.values #ssh_np.shape # uncomment to print data object summary Then lets plot up a single time snapshot of ssh using matplotlib:\nplt.pcolormesh(nemo_t.dataset.longitude, nemo_t.dataset.latitude, nemo_t.dataset.ssh[0]) \u0026lt;matplotlib.collections.QuadMesh at 0x7fd304256e80\u0026gt;  Some Manipulation There are currently some basic subsetting routines for Gridded objects, to cut out specified regions of data. Fundamentally, this can be done using xarray’s isel or sel routines to index the data. In this case, the Gridded object will pass arguments straight through to xarray.isel.\nLets get the indices of all model points within 111km km of (5W, 55N):\nind_y, ind_x = nemo_t.subset_indices_by_distance(centre_lon=-5, centre_lat=55, radius=111) #ind_x.shape # uncomment to print data object summary Now create a new, smaller subsetted Gridded object by passing those indices to isel.\nnemo_t_subset = nemo_t.isel(x_dim=ind_x, y_dim=ind_y) #nemo_t_subset.dataset # uncomment to print data object summary Alternatively, xarray.isel can be applied directly to the xarray.Dataset object. A longitude/latitude box of data can also be extracted using Gridded.subset_indices().\nPlotting example for NEMO-ERSEM biogechemical variables Import COAsT, define some file paths for NEMO-ERSEM output data and a NEMO domain, and read/load your NEMO-ERSEM data into a gridded object, example:\nimport coast import matplotlib.pyplot as plt # Define some file paths root = \u0026#34;./\u0026#34; dn_files = root + \u0026#34;./example_files/\u0026#34; fn_bgc_dat = dn_files + \u0026#34;coast_example_SEAsia_BGC_1990.nc\u0026#34; fn_bgc_dom = dn_files + \u0026#34;coast_example_domain_SEAsia.nc\u0026#34; fn_config_bgc_grid = root + \u0026#34;./config/example_nemo_bgc.json\u0026#34; nemo_bgc = coast.Gridded(fn_data = fn_bgc_dat, fn_domain = fn_bgc_dom, config=fn_config_bgc_grid) #nemo_bgc.dataset # uncomment to print data object summary As an example plot a snapshot of dissolved inorganic carbon at the sea surface\nfig = plt.figure() plt.pcolormesh( nemo_bgc.dataset.longitude, nemo_bgc.dataset.latitude, nemo_bgc.dataset.dic.isel(t_dim=0).isel(z_dim=0), cmap=\u0026#34;RdYlBu_r\u0026#34;, vmin=1600, vmax=2080, ) plt.colorbar() plt.title(\u0026#34;DIC, mmol/m^3\u0026#34;) plt.xlabel(\u0026#34;longitude\u0026#34;) plt.ylabel(\u0026#34;latitude\u0026#34;) plt.show() /tmp/ipykernel_3888/2498690501.py:2: UserWarning: The input coordinates to pcolormesh are interpreted as cell centers, but are not monotonically increasing or decreasing. This may lead to incorrectly calculated cell edges, in which case, please supply explicit cell edges to pcolormesh. plt.pcolormesh(  ","excerpt":"An introduction to the Gridded class. Loading variables and grid information.\nThis is designed to be …","ref":"/COAsT/docs/examples/notebooks/gridded/introduction_to_gridded_class/","title":"Introduction to gridded class"},{"body":"Example useage of Profile object. Overview INDEXED type class for storing data from a CTD Profile (or similar down and up observations). The structure of the class is based around having discrete profile locations with independent depth dimensions and coords. The class dataset should contain two dimensions:\n\u0026gt; id_dim :: The profiles dimension. Each element of this dimension contains data (e.g. cast) for an individual location. \u0026gt; z_dim :: The dimension for depth levels. A profile object does not need to have shared depths, so NaNs might be used to pad any depth array.  Alongside these dimensions, the following minimal coordinates should also be available:\n\u0026gt; longitude (id_dim) :: 1D array of longitudes, one for each id_dim \u0026gt; latitude (id_dim) :: 1D array of latitudes, one for each id_dim \u0026gt; time (id_dim) :: 1D array of times, one for each id_dim \u0026gt; depth (id_dim, z_dim) :: 2D array of depths, with different depth levels being provided for each profile. Note that these depth levels need to be stored in a 2D array, so NaNs can be used to pad out profiles with shallower depths. \u0026gt; id_name (id_dim) :: [Optional] Name of id_dim/case or id_dim number.  Introduction to Profile and ProfileAnalysis Below is a description of the available example scripts for this class as well as an overview of validation using Profile and ProfileAnalysis.\nExample Scripts Please see COAsT/example_scripts/notesbooks/runnable_notebooks/profile_validation/*.ipynb and COAsT/example_scripts/profile_validation/*.py for some notebooks and equivalent scripts which demonstrate how to use the Profile and ProfileAnalysis classes for model validation.\n  analysis_preprocess_en4.py : If you\u0026rsquo;re using EN4 data, this kind of script might be your first step for analysis.\n  analysis_extract_and_compare.py: This script shows you how to extract the nearest model profiles, compare them with EN4 observations and get errors throughout the vertical dimension and averaged in surface and bottom zones\n  analysis_extract_and_compare_single_process.py: This script does the same as number 2. However, it is modified slightly to take a command line argument which helps it figure out which dates to analyse. This means that this script can act as a template for jug type parallel processing on, e.g. JASMIN.\n  analysis_mask_means.py: This script demonstrates how to use boolean masks to obtain regional averages of profiles and errors.\n  analysis_average_into_grid_boxes.py: This script demonstrates how to average the data inside a Profile object into regular grid boxes and seasonal climatologies.\n  Load and preprocess profile and model data Start by loading python packages\nimport coast from os import path import numpy as np import matplotlib.pyplot as plt We can create a new Profile object easily:\nprofile = coast.Profile() Currently, this object is empty, and contains no dataset. There are some reading routines currently available in Profile for reading EN4 or WOD data files. These can be used to easily read data into your new profile object:\n# Read WOD data into profile object fn_prof = path.join(\u0026#34;example_files\u0026#34;,\u0026#34;WOD_example_ragged_standard_level.nc\u0026#34;) profile.read_wod( fn_prof ) # Read EN4 data into profile object (OVERWRITES DATASET) fn_prof = path.join(\u0026#34;example_files\u0026#34;, \u0026#34;coast_example_en4_201008.nc\u0026#34;) fn_cfg_prof = path.join(\u0026#34;config\u0026#34;,\u0026#34;example_en4_profiles.json\u0026#34;) profile = coast.Profile(config=fn_cfg_prof) profile.read_en4( fn_prof ) config/example_en4_profiles.json  Alternatively, you can pass an xarray.dataset straight to Profile:\nprofile = coast.Profile( dataset = your_dataset, config = config_file [opt] ) If you are using EN4 data, you can use the process_en4() routine to apply quality control flags to the data (replacing with NaNs):\nprocessed_profile = profile.process_en4() profile = processed_profile We can do some simple spatial and temporal manipulations of this data:\n# Cut out a geographical box profile = profile.subset_indices_lonlat_box(lonbounds = [-15, 15], latbounds = [45, 65]) # Cut out a time window profile = profile.time_slice( date0 = np.datetime64(\u0026#39;2010-01-01\u0026#39;), date1 = np.datetime64(\u0026#34;2010-01-20\u0026#34;)) Inspect profile locations Have a look inside the profile.py class to see what it can do\nprofile.plot_map() /usr/share/miniconda/envs/coast/lib/python3.8/site-packages/cartopy/io/__init__.py:241: DownloadWarning: Downloading: https://naturalearth.s3.amazonaws.com/50m_physical/ne_50m_coastline.zip warnings.warn(f'Downloading: {url}', DownloadWarning)  (\u0026lt;Figure size 640x480 with 2 Axes\u0026gt;, \u0026lt;GeoAxes: \u0026gt;)  Direct Model comparison using obs_operator() method There are a number of routines available for interpolating in the horizontal, vertical and in time to do direct comparisons of model and profile data. Profile.obs_operator will do a nearest neighbour spatial interpolation of the data in a Gridded object to profile latitudes/longitudes. It will also do a custom time interpolation.\nFirst load some model data: root = \u0026#34;./\u0026#34; # And by defining some file paths dn_files = root + \u0026#34;./example_files/\u0026#34; fn_nemo_dat = path.join(dn_files, \u0026#34;coast_example_nemo_data.nc\u0026#34;) fn_nemo_dom = path.join(dn_files, \u0026#34;coast_example_nemo_domain.nc\u0026#34;) fn_nemo_config = path.join(root, \u0026#34;./config/example_nemo_grid_t.json\u0026#34;) # Create gridded object: nemo = coast.Gridded(fn_nemo_dat, fn_nemo_dom, multiple=True, config=fn_nemo_config) Create a landmask array in Gridded In this example we add a landmask variable to the Gridded dataset. When this is present, the obs_operator will use this to interpolation to the nearest wet point. If not present, it will just take the nearest grid point (not implemented).\nWe also rename the depth at initial time coordinate depth_0 to depth as this is expected by Profile()\nnemo.dataset[\u0026#34;landmask\u0026#34;] = nemo.dataset.bottom_level == 0 nemo.dataset = nemo.dataset.rename({\u0026#34;depth_0\u0026#34;: \u0026#34;depth\u0026#34;}) # profile methods will expect a `depth` coordinate Interpolate model to horizontal observation locations using obs_operator() method # Use obs operator for horizontal remapping of Gridded onto Profile. model_profiles = profile.obs_operator(nemo) Now that we have interpolated the model onto Profiles, we have a new Profile object called model_profiles. This can be used to do some comparisons with our original processed_profile object, which we created above.\nDiscard profiles where the interpolation distance is too large However maybe we first want to restrict the set of model profiles to those that were close to the observations; perhaps, for example, the observational profiles are beyond the model domain. The model resolution would be an appropriate scale to pick\ntoo_far = 7 # distance km keep_indices = model_profiles.dataset.interp_dist \u0026lt;= too_far model_profiles = model_profiles.isel(id_dim=keep_indices) # Also drop the unwanted observational profiles profile = profile.isel(id_dim=keep_indices) Profile analysis Create an object for Profile analysis Let\u0026rsquo;s make our ProfileAnalysis object:\nanalysis = coast.ProfileAnalysis() We can use ProfileAnalysis.interpolate_vertical to interpolate all variables within a Profile object. This can be done onto a set of reference depths or, matching another object\u0026rsquo;s depth coordinates by passing another profile object. Let\u0026rsquo;s interpolate our model profiles onto observations depths, then interpolate both onto a set of reference depths:\n### Set depth averaging settings ref_depth = np.concatenate((np.arange(1, 100, 2), np.arange(100, 300, 5), np.arange(300, 1000, 50))) # Interpolate model profiles onto observation depths model_profiles_interp = analysis.interpolate_vertical(model_profiles, profile, interp_method=\u0026#34;linear\u0026#34;) # Vertical interpolation of model profiles to reference depths model_profiles_interp_ref = analysis.interpolate_vertical(model_profiles_interp, ref_depth) # Interpolation of obs profiles to reference depths profile_interp_ref = analysis.interpolate_vertical(profile, ref_depth) However, there is a problem here as the interpolate_vertical() method tries to map the whole contents of profile to the ref_depth and the profile object contains some binary data from the original qc flags. The data from the qc flags was mapped using process_en4() so the original qc entries can be removed.\n## Strip out old QC variables profile.dataset = profile.dataset.drop_vars([\u0026#39;qc_potential_temperature\u0026#39;,\u0026#39;qc_practical_salinity\u0026#39;, \u0026#39;qc_depth\u0026#39;,\u0026#39;qc_time\u0026#39;, \u0026#39;qc_flags_profiles\u0026#39;,\u0026#39;qc_flags_levels\u0026#39;]) # Interpolation of obs profiles to reference depths profile_interp_ref = analysis.interpolate_vertical(profile, ref_depth) Differencing Now that we have two Profile objects that are horizontally and vertically comparable, we can use difference() to get some basic errors:\ndifferences = analysis.difference(profile_interp_ref, model_profiles_interp_ref) This will return a new Profile object that contains the variable difference, absolute differences and square differences at all depths and means for each profile.\nType\ndifferences.dataset to see what it returns\n# E.g. plot the differences on ind_dim vs z_dim axes differences.dataset.diff_temperature.plot() \u0026lt;matplotlib.collections.QuadMesh at 0x7fd608509100\u0026gt;  # or a bit prettier on labelled axes cmap=plt.get_cmap(\u0026#39;seismic\u0026#39;) fig = plt.figure(figsize=(8, 3)) plt.pcolormesh( differences.dataset.time, ref_depth, differences.dataset.diff_temperature.T, label=\u0026#39;abs_diff\u0026#39;, cmap=cmap, vmin=-5, vmax=5) plt.ylim([0,200]) plt.gca().invert_yaxis() plt.ylabel(\u0026#39;depth\u0026#39;) plt.colorbar( label=\u0026#39;temperature diff (obs-model)\u0026#39;) \u0026lt;matplotlib.colorbar.Colorbar at 0x7fd6084637c0\u0026gt;  Layer Averaging We can use the Profile object to get mean values between specific depth levels or for some layer above the bathymetric depth. The former can be done using ProfileAnalysis.depth_means(), for example the following will return a new Profile object containing the means of all variables between 0m and 5m:\nprofile_surface = analysis.depth_means(profile, [0, 5]) # 0 - 5 metres But since this can work on any Profile object it would be more interesting to apply it to the differences between the interpolated observations and model points\nsurface_def = 10 # in metres model_profiles_surface = analysis.depth_means(model_profiles_interp_ref, [0, surface_def]) obs_profiles_surface = analysis.depth_means(profile_interp_ref, [0, surface_def]) surface_errors = analysis.difference(obs_profiles_surface, model_profiles_surface) # Plot (observation - model) upper 10m averaged temperatures surface_errors.plot_map(var_str=\u0026#34;diff_temperature\u0026#34;) (\u0026lt;Figure size 640x480 with 2 Axes\u0026gt;, \u0026lt;GeoAxes: \u0026gt;)  This can be done for any arbitrary depth layer defined by two depths.\nHowever, in some cases it may be that one of the depth levels is not defined by a constant, e.g. when calculating bottom means. In this case you may want to calculate averages over a height from the bottom that is conditional on the bottom depth. This can be done using ProfileAnalysis.bottom_means(). For example:\nbottom_height = [10, 50, 100] # Average over bottom heights of 10m, 30m and 100m for... bottom_thresh = [100, 500, np.inf] # ...bathymetry depths less than 100m, 100-500m and 500-infinite model_profiles_bottom = analysis.bottom_means(model_profiles_interp_ref, bottom_height, bottom_thresh) similarly compute the same for the observations\u0026hellip; though first we have to patch in a bathymetry variable that will be expected by the method. Grab it from the model dataset.\nprofile_interp_ref.dataset[\u0026#34;bathymetry\u0026#34;] = ([\u0026#34;id_dim\u0026#34;], model_profiles_interp_ref.dataset[\u0026#34;bathymetry\u0026#34;].values) obs_profiles_bottom = analysis.bottom_means(profile_interp_ref, bottom_height, bottom_thresh) Now the difference can be calculated\nbottom_errors = analysis.difference( obs_profiles_bottom, model_profiles_bottom) # Plot (observation - model) upper 10m averaged temperatures bottom_errors.plot_map(var_str=\u0026#34;diff_temperature\u0026#34;) (\u0026lt;Figure size 640x480 with 2 Axes\u0026gt;, \u0026lt;GeoAxes: \u0026gt;)  NOTE1: The bathymetry variable does not actually need to contain bathymetric depths, it can also be used to calculate means above any non-constant surface. For example, it could be mixed layer depth.\nNOTE2: This can be done for any Profile object. So, you could use this workflow to also average a Profile derived from the difference() routine.\n# Since they are indexed by \u0026#39;id_dim\u0026#39; they can be plotted against time fig = plt.figure(figsize=(8, 3)) plt.plot( surface_errors.dataset.time, surface_errors.dataset.diff_temperature, \u0026#39;.\u0026#39;, label=\u0026#39;surf T\u0026#39; ) plt.plot( bottom_errors.dataset.time, bottom_errors.dataset.diff_temperature, \u0026#39;.\u0026#39;, label=\u0026#39;bed T\u0026#39; ) plt.xlabel(\u0026#39;time\u0026#39;) plt.ylabel(\u0026#39;temperature errors\u0026#39;) plt.legend() plt.title(\u0026#34;Temperature diff (obs-model)\u0026#34;) Text(0.5, 1.0, 'Temperature diff (obs-model)')  Regional (Mask) Averaging We can use Profile in combination with MaskMaker to calculate averages over regions defined by masks. For example, to get the mean errors in the North Sea. Start by creating a list of boolean masks we would like to use:\nmm = coast.MaskMaker() # Define Regional Masks regional_masks = [] # Define convenient aliases based on nemo data lon = nemo.dataset.longitude.values lat = nemo.dataset.latitude.values bathy = nemo.dataset.bathymetry.values # Add regional mask for whole domain regional_masks.append(np.ones(lon.shape)) # Add regional mask for English Channel regional_masks.append(mm.region_def_nws_english_channel(lon, lat, bathy)) region_names = [\u0026#34;whole_domain\u0026#34;,\u0026#34;english_channel\u0026#34;,] Next, we must make these masks into datasets using MaskMaker.make_mask_dataset. Masks should be 2D datasets defined by booleans. In our example here we have used the latitude/longitude array from the nemo object, however it can be defined however you like.\nmask_list = mm.make_mask_dataset(lon, lat, regional_masks) Then we use ProfileAnalysis.determine_mask_indices to figure out which profiles in a Profile object lie within each regional mask:\nmask_indices = analysis.determine_mask_indices(profile, mask_list) This returns an object called mask_indices, which is required to pass to ProfileAnalysis.mask_means(). This routine will return a new xarray dataset containing averaged data for each region:\nmask_means = analysis.mask_means(profile, mask_indices) which can be visualised or further processed\nfor count_region in range(len(region_names)): plt.plot( mask_means.profile_mean_temperature.isel(dim_mask=count_region), mask_means.profile_mean_depth.isel(dim_mask=count_region), label=region_names[count_region], marker=\u0026#34;.\u0026#34;, linestyle=\u0026#39;none\u0026#39;) plt.ylim([10,1000]) plt.yscale(\u0026#34;log\u0026#34;) plt.gca().invert_yaxis() plt.xlabel(\u0026#39;temperature\u0026#39;); plt.ylabel(\u0026#39;depth\u0026#39;) plt.legend() \u0026lt;matplotlib.legend.Legend at 0x7fd6080e2a30\u0026gt;  Gridding Profile Data If you have large amount of profile data you may want to average it into grid boxes to get, for example, mean error maps or climatologies. This can be done using ProfileAnalysis.average_into_grid_boxes().\nWe can create a gridded dataset shape (y_dim, x_dim) from all the data using:\ngrid_lon = np.arange(-15, 15, 0.5) grid_lat = np.arange(45, 65, 0.5) prof_gridded = analysis.average_into_grid_boxes(profile, grid_lon, grid_lat) # NB this method does not separately treat `z_dim`, see docstr lat = prof_gridded.dataset.latitude lon = prof_gridded.dataset.longitude temperature = prof_gridded.dataset.temperature plt.pcolormesh( lon, lat, temperature) plt.title(\u0026#39;gridded mean temperature\u0026#39;) plt.colorbar() \u0026lt;matplotlib.colorbar.Colorbar at 0x7fd600ecd8e0\u0026gt;  Alternatively, we can calculate averages for each season:\nprof_gridded_DJF = analysis.average_into_grid_boxes(profile, grid_lon, grid_lat, season=\u0026#34;DJF\u0026#34;, var_modifier=\u0026#34;_DJF\u0026#34;) prof_gridded_MAM = analysis.average_into_grid_boxes(profile, grid_lon, grid_lat, season=\u0026#34;MAM\u0026#34;, var_modifier=\u0026#34;_MAM\u0026#34;) prof_gridded_JJA = analysis.average_into_grid_boxes(profile, grid_lon, grid_lat, season=\u0026#34;JJA\u0026#34;, var_modifier=\u0026#34;_JJA\u0026#34;) prof_gridded_SON = analysis.average_into_grid_boxes(profile, grid_lon, grid_lat, season=\u0026#34;SON\u0026#34;, var_modifier=\u0026#34;_SON\u0026#34;) Here, season specifies which season to average over and var_modifier is added to the end of all variable names in the object\u0026rsquo;s dataset.\nNB with the example data only DJF has any data.\nThis function returns a new Gridded object. It also contains a new variable called grid_N, which stores how many profiles were averaged into each grid box. You may want to use this when using or extending the analysis. E.g. use it with plot symbol size\ntemperature = prof_gridded_DJF.dataset.temperature_DJF N = prof_gridded_DJF.dataset.grid_N_DJF plt.scatter( lon, lat, c=temperature, s=N) plt.title(\u0026#39;DJF gridded mean temperature\u0026#39;) plt.colorbar() \u0026lt;matplotlib.colorbar.Colorbar at 0x7fd600dc0160\u0026gt;  ","excerpt":"Example useage of Profile object. Overview INDEXED type class for storing data from a CTD Profile …","ref":"/COAsT/docs/examples/notebooks/profile/introduction_to_profile_class/","title":"Introduction to profile class"},{"body":"A demonstration of the MaskMaker class to build and use regional masking\nMaskMasker is a class of methods to assist with making regional masks within COAsT. Presently the mask generated are external to MaskMaker. Masks are constructed as gridded boolean numpy array for each region, which are stacked over a dim_mask dimension. The mask arrays are generated on a supplied horizontal grid. The masks are then stored in xarray objects along with regions names.\nExamples are given working with Gridded and Profile data.\nRelevant imports and filepath configuration import coast import numpy as np from os import path import matplotlib.pyplot as plt import matplotlib.colors as colors # colormap fiddling import xarray as xr # set some paths root = \u0026#34;./\u0026#34; dn_files = root + \u0026#34;./example_files/\u0026#34; fn_nemo_grid_t_dat = dn_files + \u0026#34;nemo_data_T_grid_Aug2015.nc\u0026#34; fn_nemo_dom = dn_files + \u0026#34;coast_example_nemo_domain.nc\u0026#34; config_t = root + \u0026#34;./config/example_nemo_grid_t.json\u0026#34; Loading data # Create a Gridded object and load in the data: nemo = coast.Gridded(fn_nemo_grid_t_dat, fn_nemo_dom, config=config_t) Initialise MaskMaker and define target grid mm = coast.MaskMaker() # Define Regional Masks regional_masks = [] # Define convenient aliases based on nemo data lon = nemo.dataset.longitude.values lat = nemo.dataset.latitude.values bathy = nemo.dataset.bathymetry.values Use MaskMaker to define new regions MaskMaker can build a stack of boolean masks in an xarray dataset for regional analysis. Regions can be supplied by providing vertices coordiates to the make_region_from_vertices method. (Vertices coordinates can be passed as xarray objects or as numpy arrays). The method returns a numpy array of booleans.\n# Draw and fill a square vertices_lon = [-5, -5, 5, 5] vertices_lat = [40, 60, 60, 40] # input lat/lon as xr.DataArray filled1 = mm.make_region_from_vertices(nemo.dataset.longitude, nemo.dataset.latitude, vertices_lon, vertices_lat) # input lat/lon as np.ndarray filled2 = mm.make_region_from_vertices( nemo.dataset.longitude.values, nemo.dataset.latitude.values, vertices_lon, vertices_lat ) check = (filled1 == filled2).all() print(f\u0026#34;numpy array outputs are the same? {check}\u0026#34;) numpy array outputs are the same? True  The boolean numpy array can then be converted to an xarray object using make_mask_dataset() for improved interactions with other xarray objects.\nmask_xr = mm.make_mask_dataset(nemo.dataset.longitude.values, nemo.dataset.latitude.values, filled1) Use MaskMaker for predefined regions The NWS has a number of predefined regions. These are numpy boolean arrays as functions of the specified latitude, longitude and bathymetry. They can be appended into a list of arrays, which can be similarly converted into an xarray object.\nmasks_list = [] # Add regional mask for whole domain masks_list.append(np.ones(lon.shape)) # Add regional mask for English Channel masks_list.append(mm.region_def_nws_north_north_sea(lon, lat, bathy)) masks_list.append(mm.region_def_nws_south_north_sea(lon, lat, bathy)) masks_list.append(mm.region_def_nws_outer_shelf(lon, lat, bathy)) masks_list.append(mm.region_def_nws_norwegian_trench(lon, lat, bathy)) masks_list.append(mm.region_def_nws_english_channel(lon, lat, bathy)) masks_list.append(mm.region_def_nws_off_shelf(lon, lat, bathy)) masks_list.append(mm.region_def_nws_irish_sea(lon, lat, bathy)) masks_list.append(mm.region_def_nws_kattegat(lon, lat, bathy)) masks_list.append(mm.region_def_nws_fsc(lon, lat, bathy)) masks_names = [\u0026#34;whole domain\u0026#34;, \u0026#34;northern north sea\u0026#34;, \u0026#34;southern north sea\u0026#34;, \u0026#34;outer shelf\u0026#34;, \u0026#34;norwegian trench\u0026#34;, \u0026#34;english_channel\u0026#34;, \u0026#34;off shelf\u0026#34;, \u0026#34;irish sea\u0026#34;, \u0026#34;kattegat\u0026#34;, \u0026#34;fsc\u0026#34;] As before the numpy arrays (here as a list) can be converted into an xarray dataset where each mask is separated along the dim_mask dimension\nmask_xr = mm.make_mask_dataset(lon, lat, masks_list, masks_names) Inspect mask xarray object structure\nmask_xr\nPlot masks Inspect the mask with a quick_plot() method.\nmm.quick_plot(mask_xr) NB overlapping regions are not given special treatment, the layers are blindly superimposed on each other. E.g. as demonstrated with \u0026ldquo;Norwegian Trench\u0026rdquo; and \u0026ldquo;off shelf\u0026rdquo;, \u0026ldquo;FSC\u0026rdquo; and \u0026ldquo;off shelf\u0026rdquo;, or \u0026ldquo;whole domain\u0026rdquo; and any other region.\nplt.subplot(2,2,1) mm.quick_plot(mask_xr.sel(dim_mask=[0,3,9])) plt.subplot(2,2,2) mm.quick_plot(mask_xr.sel(dim_mask=[1,2,4,5,6,7,8])) plt.tight_layout() # Show overlap mask_xr.mask.sum(dim=\u0026#39;dim_mask\u0026#39;).plot(levels=(1,2,3,4)) # Save if required #plt.savefig(\u0026#39;tmp.png\u0026#39;) \u0026lt;matplotlib.collections.QuadMesh at 0x7fc5b3db4a60\u0026gt;  Regional analysis with Profile data Apply the regional masks to average SST\n# Read EN4 data into profile object fn_prof = path.join(dn_files, \u0026#34;coast_example_en4_201008.nc\u0026#34;) fn_cfg_prof = path.join(\u0026#34;config\u0026#34;,\u0026#34;example_en4_profiles.json\u0026#34;) profile = coast.Profile(config=fn_cfg_prof) profile.read_en4( fn_prof ) config/example_en4_profiles.json  Then we use ProfileAnalysis.determine_mask_indices() to figure out which profiles in a Profile object lie within each regional mask:\nanalysis = coast.ProfileAnalysis() mask_indices = analysis.determine_mask_indices(profile, mask_xr) This returns an object called mask_indices, which is required to pass to ProfileAnalysis.mask_means(). This routine will return a new xarray dataset containing averaged data for each region:\nprofile_mask_means = analysis.mask_means(profile, mask_indices) This routine operates over all variables in the profile object. It calculates means by region preserving depth information (profile_mean_*) and also averaging over depth information (all_mean_*). The variables are returned with these prefixes accordingly.\nprofile_mask_means             /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { \u0026ndash;xr-font-color0: var(\u0026ndash;jp-content-font-color0, rgba(0, 0, 0, 1)); \u0026ndash;xr-font-color2: var(\u0026ndash;jp-content-font-color2, rgba(0, 0, 0, 0.54)); \u0026ndash;xr-font-color3: var(\u0026ndash;jp-content-font-color3, rgba(0, 0, 0, 0.38)); \u0026ndash;xr-border-color: var(\u0026ndash;jp-border-color2, #e0e0e0); \u0026ndash;xr-disabled-color: var(\u0026ndash;jp-layout-color3, #bdbdbd); \u0026ndash;xr-background-color: var(\u0026ndash;jp-layout-color0, white); \u0026ndash;xr-background-color-row-even: var(\u0026ndash;jp-layout-color1, white); \u0026ndash;xr-background-color-row-odd: var(\u0026ndash;jp-layout-color2, #eeeeee); }\nhtml[theme=dark], body[data-theme=dark], body.vscode-dark { \u0026ndash;xr-font-color0: rgba(255, 255, 255, 1); \u0026ndash;xr-font-color2: rgba(255, 255, 255, 0.54); \u0026ndash;xr-font-color3: rgba(255, 255, 255, 0.38); \u0026ndash;xr-border-color: #1F1F1F; \u0026ndash;xr-disabled-color: #515151; \u0026ndash;xr-background-color: #111111; \u0026ndash;xr-background-color-row-even: #111111; \u0026ndash;xr-background-color-row-odd: #313131; }\n.xr-wrap { display: block !important; min-width: 300px; max-width: 700px; }\n.xr-text-repr-fallback { /* fallback to plain text repr when CSS is not injected (untrusted notebook) */ display: none; }\n.xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(\u0026ndash;xr-border-color); }\n.xr-header \u0026gt; div, .xr-header \u0026gt; ul { display: inline; margin-top: 0; margin-bottom: 0; }\n.xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; }\n.xr-obj-type { color: var(\u0026ndash;xr-font-color2); }\n.xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; }\n.xr-section-item { display: contents; }\n.xr-section-item input { display: none; }\n.xr-section-item input + label { color: var(\u0026ndash;xr-disabled-color); }\n.xr-section-item input:enabled + label { cursor: pointer; color: var(\u0026ndash;xr-font-color2); }\n.xr-section-item input:enabled + label:hover { color: var(\u0026ndash;xr-font-color0); }\n.xr-section-summary { grid-column: 1; color: var(\u0026ndash;xr-font-color2); font-weight: 500; }\n.xr-section-summary \u0026gt; span { display: inline-block; padding-left: 0.5em; }\n.xr-section-summary-in:disabled + label { color: var(\u0026ndash;xr-font-color2); }\n.xr-section-summary-in + label:before { display: inline-block; content: \u0026lsquo;►\u0026rsquo;; font-size: 11px; width: 15px; text-align: center; }\n.xr-section-summary-in:disabled + label:before { color: var(\u0026ndash;xr-disabled-color); }\n.xr-section-summary-in:checked + label:before { content: \u0026lsquo;▼\u0026rsquo;; }\n.xr-section-summary-in:checked + label \u0026gt; span { display: none; }\n.xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; }\n.xr-section-inline-details { grid-column: 2 / -1; }\n.xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; }\n.xr-section-summary-in:checked ~ .xr-section-details { display: contents; }\n.xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; }\n.xr-array-wrap \u0026gt; label { grid-column: 1; vertical-align: top; }\n.xr-preview { color: var(\u0026ndash;xr-font-color3); }\n.xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; }\n.xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; }\n.xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; }\n.xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; }\n.xr-dim-list li { display: inline-block; padding: 0; margin: 0; }\n.xr-dim-list:before { content: \u0026lsquo;('; }\n.xr-dim-list:after { content: \u0026lsquo;)'; }\n.xr-dim-list li:not(:last-child):after { content: \u0026lsquo;,'; padding-right: 5px; }\n.xr-has-index { font-weight: bold; }\n.xr-var-list, .xr-var-item { display: contents; }\n.xr-var-item \u0026gt; div, .xr-var-item label, .xr-var-item \u0026gt; .xr-var-name span { background-color: var(\u0026ndash;xr-background-color-row-even); margin-bottom: 0; }\n.xr-var-item \u0026gt; .xr-var-name:hover span { padding-right: 5px; }\n.xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; div, .xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; label, .xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; .xr-var-name span { background-color: var(\u0026ndash;xr-background-color-row-odd); }\n.xr-var-name { grid-column: 1; }\n.xr-var-dims { grid-column: 2; }\n.xr-var-dtype { grid-column: 3; text-align: right; color: var(\u0026ndash;xr-font-color2); }\n.xr-var-preview { grid-column: 4; }\n.xr-index-preview { grid-column: 2 / 5; color: var(\u0026ndash;xr-font-color2); }\n.xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; }\n.xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; }\n.xr-var-attrs, .xr-var-data, .xr-index-data { display: none; background-color: var(\u0026ndash;xr-background-color) !important; padding-bottom: 5px !important; }\n.xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data, .xr-index-data-in:checked ~ .xr-index-data { display: block; }\n.xr-var-data \u0026gt; table { float: right; }\n.xr-var-name span, .xr-var-data, .xr-index-name div, .xr-index-data, .xr-attrs { padding-left: 25px !important; }\n.xr-attrs, .xr-var-attrs, .xr-var-data, .xr-index-data { grid-column: 1 / -1; }\ndl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; }\n.xr-attrs dt, .xr-attrs dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; }\n.xr-attrs dt { font-weight: normal; grid-column: 1; }\n.xr-attrs dt:hover span { display: inline-block; background: var(\u0026ndash;xr-background-color); padding-right: 10px; }\n.xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; }\n.xr-icon-database, .xr-icon-file-text2, .xr-no-icon { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } \u0026lt;xarray.Dataset\u0026gt; Dimensions: (dim_mask: 8, z_dim: 400) Coordinates: region_names (dim_mask) \u0026lt;U18 'whole domain' \u0026hellip; 'k\u0026hellip; Dimensions without coordinates: dim_mask, z_dim Data variables: profile_mean_depth (dim_mask, z_dim) float32 3.802 \u0026hellip; nan profile_mean_potential_temperature (dim_mask, z_dim) float32 4.629 \u0026hellip; nan profile_mean_temperature (dim_mask, z_dim) float32 4.629 \u0026hellip; nan profile_mean_practical_salinity (dim_mask, z_dim) float32 29.08 \u0026hellip; nan profile_mean_qc_flags_profiles (dim_mask) float64 4.422e+05 \u0026hellip; 1.93\u0026hellip; profile_mean_qc_flags_levels (dim_mask, z_dim) float64 1.693e+07 \u0026hellip;. all_mean_depth (dim_mask) float32 219.3 48.17 \u0026hellip; 86.48 all_mean_potential_temperature (dim_mask) float32 7.458 6.68 \u0026hellip; 7.266 all_mean_temperature (dim_mask) float32 7.48 6.685 \u0026hellip; 7.275 all_mean_practical_salinity (dim_mask) float32 34.57 34.86 \u0026hellip; 33.76 all_mean_qc_flags_profiles (dim_mask) float64 4.422e+05 \u0026hellip; 1.93\u0026hellip; all_mean_qc_flags_levels (dim_mask) float64 3.272e+07 \u0026hellip; 3.68\u0026hellip;xarray.DatasetDimensions:dim_mask: 8z_dim: 400Coordinates: (1)region_names(dim_mask)\u0026lt;U18'whole domain' \u0026hellip; 'kattegat'array(['whole domain', 'northern north sea', 'southern north sea', 'outer shelf', 'norwegian trench', 'english_channel', 'off shelf', 'kattegat'], dtype='\u0026lt;U18')Data variables: (12)profile_mean_depth(dim_mask, z_dim)float323.802 12.43 12.75 \u0026hellip; nan nan nanarray([[3.8015647e+00, 1.2431897e+01, 1.2752748e+01, \u0026hellip;, 6.8166663e+02, 4.4625000e+02, 4.8479999e+02], [5.3978572e+00, 2.4058212e+01, 1.9938543e+01, \u0026hellip;, nan, nan, nan], [4.4636950e+00, 1.6393627e+01, 1.4392214e+01, \u0026hellip;, nan, nan, nan], \u0026hellip;, [1.2563381e+00, 3.3999999e+00, 6.0253515e+00, \u0026hellip;, nan, nan, nan], [1.0883763e+01, 1.9575655e+01, 3.1419313e+01, \u0026hellip;, 1.1761000e+03, nan, nan], [1.1508474e+00, 5.1322031e+00, 8.6844826e+00, \u0026hellip;, nan, nan, nan]], dtype=float32)profile_mean_potential_temperature(dim_mask, z_dim)float324.629 4.718 4.732 \u0026hellip; nan nan nanarray([[ 4.629049 , 4.7177176 , 4.7318735 , \u0026hellip;, 4.4791145 , 7.0863085 , 7.4710603 ], [ 5.4062243 , 5.538929 , 5.3454785 , \u0026hellip;, nan, nan, nan], [ 4.690185 , 4.7648296 , 4.290611 , \u0026hellip;, nan, nan, nan], \u0026hellip;, [ 7.777337 , 7.743324 , 7.730106 , \u0026hellip;, nan, nan, nan], [10.741058 , 10.7420845 , 10.740048 , \u0026hellip;, -0.8672274 , nan, nan], [ 0.6519121 , 0.89932823, 1.4384961 , \u0026hellip;, nan, nan, nan]], dtype=float32)profile_mean_temperature(dim_mask, z_dim)float324.629 4.719 4.733 \u0026hellip; nan nan nanarray([[ 4.629366 , 4.7186904 , 4.7328353 , \u0026hellip;, 4.523333 , 7.13 , 7.52 ], [ 5.4066286 , 5.540855 , 5.3469286 , \u0026hellip;, nan, nan, nan], [ 4.69051 , 4.76605 , 4.291559 , \u0026hellip;, nan, nan, nan], \u0026hellip;, [ 7.777464 , 7.7436633 , 7.7307053 , \u0026hellip;, nan, nan, nan], [10.742379 , 10.744476 , 10.743904 , \u0026hellip;, -0.82 , nan, nan], [ 0.6519491 , 0.89944816, 1.4387244 , \u0026hellip;, nan, nan, nan]], dtype=float32)profile_mean_practical_salinity(dim_mask, z_dim)float3229.08 29.49 30.4 \u0026hellip; nan nan nanarray([[29.07752 , 29.488913, 30.403212, \u0026hellip;, 35.102665, 35.194 , 35.191 ], [34.215603, 34.549515, 34.572178, \u0026hellip;, nan, nan, nan], [34.39857 , 34.319733, 34.34744 , \u0026hellip;, nan, nan, nan], \u0026hellip;, [34.69485 , 34.745506, 34.778313, \u0026hellip;, nan, nan, nan], [35.41276 , 35.531857, 35.523746, \u0026hellip;, 34.912 , nan, nan], [22.681366, 23.651918, 24.719212, \u0026hellip;, nan, nan, nan]], dtype=float32)profile_mean_qc_flags_profiles(dim_mask)float644.422e+05 1.501e+04 \u0026hellip; 1.93e+06array([ 442235.24343675, 15008.98928571, 80146.01273885, 376662.06666667, 938649.58108108, 236414.92957746, 351245.56410256, 1930156.61016949])profile_mean_qc_flags_levels(dim_mask, z_dim)float641.693e+07 1.56e+07 \u0026hellip; 3.356e+07array([[1.69308660e+07, 1.55996508e+07, 1.91944702e+07, \u0026hellip;, 3.35092258e+07, 3.35359264e+07, 3.35359264e+07], [2.41202110e+06, 1.67795573e+06, 1.43838659e+07, \u0026hellip;, 3.35626270e+07, 3.35626270e+07, 3.35626270e+07], [7.82982090e+06, 9.03196737e+06, 2.22584483e+07, \u0026hellip;, 3.35626270e+07, 3.35626270e+07, 3.35626270e+07], \u0026hellip;, [1.43590767e+07, 1.24096114e+07, 1.32338878e+07, \u0026hellip;, 3.35626270e+07, 3.35626270e+07, 3.35626270e+07], [2.49180806e+06, 2.49194591e+06, 1.91836366e+06, \u0026hellip;, 3.32757669e+07, 3.35626270e+07, 3.35626270e+07], [1.07067776e+08, 1.03513576e+08, 9.04351003e+07, \u0026hellip;, 3.35626270e+07, 3.35626270e+07, 3.35626270e+07]])all_mean_depth(dim_mask)float32219.3 48.17 17.66 \u0026hellip; 492.1 86.48array([219.31877 , 48.17008 , 17.655499, 54.261852, 147.37276 , 21.17876 , 492.1379 , 86.48219 ], dtype=float32)all_mean_potential_temperature(dim_mask)float327.458 6.68 5.08 \u0026hellip; 8.609 7.266array([ 7.4579477, 6.6803527, 5.0804653, 10.605105 , 7.5581946, 7.9912376, 8.609107 , 7.266073 ], dtype=float32)all_mean_temperature(dim_mask)float327.48 6.685 5.082 \u0026hellip; 8.658 7.275array([ 7.479547 , 6.684928 , 5.0818586, 10.611826 , 7.572891 , 7.993422 , 8.657866 , 7.2747602], dtype=float32)all_mean_practical_salinity(dim_mask)float3234.57 34.86 34.49 \u0026hellip; 35.33 33.76array([34.574173, 34.85523 , 34.486984, 35.177284, 34.745213, 34.871284, 35.334602, 33.764072], dtype=float32)all_mean_qc_flags_profiles(dim_mask)float644.422e+05 1.501e+04 \u0026hellip; 1.93e+06array([ 442235.24343675, 15008.98928571, 80146.01273885, 376662.06666667, 938649.58108108, 236414.92957746, 351245.56410256, 1930156.61016949])all_mean_qc_flags_levels(dim_mask)float643.272e+07 3.244e+07 \u0026hellip; 3.683e+07array([32717180.43962013, 32440148.97097322, 33282693.25738854, 32052298.20066667, 38954525.82148649, 32985344.45144366, 26732339.47348291, 36830323.95974576])Indexes: (0)Attributes: (0)\nNotice that the number of mask dimensions is not necessarily preserved between the mask and the mask averaged variables. This happens if, for example, there are no profiles in one of the mask regions\ncheck1 = mask_indices.dims[\u0026#34;dim_mask\u0026#34;] == profile_mask_means.dims[\u0026#34;dim_mask\u0026#34;] print(check1) False  The mean profiles can be visualised or further processed (notice the Irish Sea region and FSC are missing because there were no profiles in the example dataset)\nfor count_region in range(profile_mask_means.sizes[\u0026#39;dim_mask\u0026#39;]): plt.plot( profile_mask_means.profile_mean_temperature.isel(dim_mask=count_region), profile_mask_means.profile_mean_depth.isel(dim_mask=count_region), label=profile_mask_means.region_names[count_region].values, marker=\u0026#34;.\u0026#34;, linestyle=\u0026#39;none\u0026#39;) plt.ylim([10,1000]) plt.yscale(\u0026#34;log\u0026#34;) plt.gca().invert_yaxis() plt.xlabel(\u0026#39;temperature\u0026#39;); plt.ylabel(\u0026#39;depth\u0026#39;) plt.legend() \u0026lt;matplotlib.legend.Legend at 0x7fc5b627ceb0\u0026gt;  Regional analysis with Gridded data Apply the regional masks to average SST. This is done manually as there are not yet COAsT methods to broadcast the operations across all variables.\n# Syntax: xr.where(if \u0026lt;first\u0026gt;, then \u0026lt;2nd\u0026gt;, else \u0026lt;3rd\u0026gt;)  mask_SST = xr.where( mask_xr.mask, nemo.dataset.temperature.isel(z_dim=0), np.NaN) # Take the mean over space for each region mask_mean_SST = mask_SST.mean(dim=\u0026#34;x_dim\u0026#34;).mean(dim=\u0026#34;y_dim\u0026#34;) # Inspect the processed data mask_mean_SST.plot() \u0026lt;matplotlib.collections.QuadMesh at 0x7fc5b6117130\u0026gt;  # Plot timeseries per region for count_region in range(mask_mean_SST.sizes[\u0026#39;dim_mask\u0026#39;]): plt.plot( mask_mean_SST.isel(dim_mask=count_region), label=mask_mean_SST.region_names[count_region].values, marker=\u0026#34;.\u0026#34;, linestyle=\u0026#39;none\u0026#39;) plt.xlabel(\u0026#39;time\u0026#39;); plt.ylabel(\u0026#39;SST\u0026#39;) plt.legend() \u0026lt;matplotlib.legend.Legend at 0x7fc5b618b3a0\u0026gt;  ","excerpt":"A demonstration of the MaskMaker class to build and use regional masking\nMaskMasker is a class of …","ref":"/COAsT/docs/examples/notebooks/general/mask_maker_tutorial/","title":"Mask maker tutorial"},{"body":"A demonstration to calculate the Potential Energy Anomaly and demonstrate regional masking with MaskMaker\nRelevant imports and filepath configuration import coast import numpy as np import os import matplotlib.pyplot as plt import matplotlib.colors as colors # colormap fiddling import xarray as xr # set some paths root = \u0026#34;./\u0026#34; dn_files = root + \u0026#34;./example_files/\u0026#34; fn_nemo_grid_t_dat = dn_files + \u0026#34;nemo_data_T_grid_Aug2015.nc\u0026#34; fn_nemo_dom = dn_files + \u0026#34;coast_example_nemo_domain.nc\u0026#34; config_t = root + \u0026#34;./config/example_nemo_grid_t.json\u0026#34; Loading data # Create a Gridded object and load in the data: nemo = coast.Gridded(fn_nemo_grid_t_dat, fn_nemo_dom, config=config_t) Calculates Potential Energy Anomaly The density and depth averaged density can be supplied within gridded_t as density and density_bar DataArrays, respectively. If they are not supplied they will be calculated. density_bar is calcuated using depth averages of temperature and salinity.\n# Compute a vertical max to exclude depths below 200m Zd_mask, kmax, Ikmax = nemo.calculate_vertical_mask(200.) # Initiate a stratification diagnostics object strat = coast.GriddedStratification(nemo) # calculate PEA for unmasked depths strat.calc_pea(nemo, Zd_mask) make a plot strat.quick_plot(\u0026#39;PEA\u0026#39;) (\u0026lt;Figure size 1000x1000 with 2 Axes\u0026gt;, \u0026lt;Axes: title={'center': '01 Aug 2015: Potential Energy Anomaly (J / m^3)'}, xlabel='longitude', ylabel='latitude'\u0026gt;)  strat.dataset             /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { \u0026ndash;xr-font-color0: var(\u0026ndash;jp-content-font-color0, rgba(0, 0, 0, 1)); \u0026ndash;xr-font-color2: var(\u0026ndash;jp-content-font-color2, rgba(0, 0, 0, 0.54)); \u0026ndash;xr-font-color3: var(\u0026ndash;jp-content-font-color3, rgba(0, 0, 0, 0.38)); \u0026ndash;xr-border-color: var(\u0026ndash;jp-border-color2, #e0e0e0); \u0026ndash;xr-disabled-color: var(\u0026ndash;jp-layout-color3, #bdbdbd); \u0026ndash;xr-background-color: var(\u0026ndash;jp-layout-color0, white); \u0026ndash;xr-background-color-row-even: var(\u0026ndash;jp-layout-color1, white); \u0026ndash;xr-background-color-row-odd: var(\u0026ndash;jp-layout-color2, #eeeeee); }\nhtml[theme=dark], body[data-theme=dark], body.vscode-dark { \u0026ndash;xr-font-color0: rgba(255, 255, 255, 1); \u0026ndash;xr-font-color2: rgba(255, 255, 255, 0.54); \u0026ndash;xr-font-color3: rgba(255, 255, 255, 0.38); \u0026ndash;xr-border-color: #1F1F1F; \u0026ndash;xr-disabled-color: #515151; \u0026ndash;xr-background-color: #111111; \u0026ndash;xr-background-color-row-even: #111111; \u0026ndash;xr-background-color-row-odd: #313131; }\n.xr-wrap { display: block !important; min-width: 300px; max-width: 700px; }\n.xr-text-repr-fallback { /* fallback to plain text repr when CSS is not injected (untrusted notebook) */ display: none; }\n.xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(\u0026ndash;xr-border-color); }\n.xr-header \u0026gt; div, .xr-header \u0026gt; ul { display: inline; margin-top: 0; margin-bottom: 0; }\n.xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; }\n.xr-obj-type { color: var(\u0026ndash;xr-font-color2); }\n.xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; }\n.xr-section-item { display: contents; }\n.xr-section-item input { display: none; }\n.xr-section-item input + label { color: var(\u0026ndash;xr-disabled-color); }\n.xr-section-item input:enabled + label { cursor: pointer; color: var(\u0026ndash;xr-font-color2); }\n.xr-section-item input:enabled + label:hover { color: var(\u0026ndash;xr-font-color0); }\n.xr-section-summary { grid-column: 1; color: var(\u0026ndash;xr-font-color2); font-weight: 500; }\n.xr-section-summary \u0026gt; span { display: inline-block; padding-left: 0.5em; }\n.xr-section-summary-in:disabled + label { color: var(\u0026ndash;xr-font-color2); }\n.xr-section-summary-in + label:before { display: inline-block; content: \u0026lsquo;►\u0026rsquo;; font-size: 11px; width: 15px; text-align: center; }\n.xr-section-summary-in:disabled + label:before { color: var(\u0026ndash;xr-disabled-color); }\n.xr-section-summary-in:checked + label:before { content: \u0026lsquo;▼\u0026rsquo;; }\n.xr-section-summary-in:checked + label \u0026gt; span { display: none; }\n.xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; }\n.xr-section-inline-details { grid-column: 2 / -1; }\n.xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; }\n.xr-section-summary-in:checked ~ .xr-section-details { display: contents; }\n.xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; }\n.xr-array-wrap \u0026gt; label { grid-column: 1; vertical-align: top; }\n.xr-preview { color: var(\u0026ndash;xr-font-color3); }\n.xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; }\n.xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; }\n.xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; }\n.xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; }\n.xr-dim-list li { display: inline-block; padding: 0; margin: 0; }\n.xr-dim-list:before { content: \u0026lsquo;('; }\n.xr-dim-list:after { content: \u0026lsquo;)'; }\n.xr-dim-list li:not(:last-child):after { content: \u0026lsquo;,'; padding-right: 5px; }\n.xr-has-index { font-weight: bold; }\n.xr-var-list, .xr-var-item { display: contents; }\n.xr-var-item \u0026gt; div, .xr-var-item label, .xr-var-item \u0026gt; .xr-var-name span { background-color: var(\u0026ndash;xr-background-color-row-even); margin-bottom: 0; }\n.xr-var-item \u0026gt; .xr-var-name:hover span { padding-right: 5px; }\n.xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; div, .xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; label, .xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; .xr-var-name span { background-color: var(\u0026ndash;xr-background-color-row-odd); }\n.xr-var-name { grid-column: 1; }\n.xr-var-dims { grid-column: 2; }\n.xr-var-dtype { grid-column: 3; text-align: right; color: var(\u0026ndash;xr-font-color2); }\n.xr-var-preview { grid-column: 4; }\n.xr-index-preview { grid-column: 2 / 5; color: var(\u0026ndash;xr-font-color2); }\n.xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; }\n.xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; }\n.xr-var-attrs, .xr-var-data, .xr-index-data { display: none; background-color: var(\u0026ndash;xr-background-color) !important; padding-bottom: 5px !important; }\n.xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data, .xr-index-data-in:checked ~ .xr-index-data { display: block; }\n.xr-var-data \u0026gt; table { float: right; }\n.xr-var-name span, .xr-var-data, .xr-index-name div, .xr-index-data, .xr-attrs { padding-left: 25px !important; }\n.xr-attrs, .xr-var-attrs, .xr-var-data, .xr-index-data { grid-column: 1 / -1; }\ndl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; }\n.xr-attrs dt, .xr-attrs dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; }\n.xr-attrs dt { font-weight: normal; grid-column: 1; }\n.xr-attrs dt:hover span { display: inline-block; background: var(\u0026ndash;xr-background-color); padding-right: 10px; }\n.xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; }\n.xr-icon-database, .xr-icon-file-text2, .xr-no-icon { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } \u0026lt;xarray.Dataset\u0026gt; Dimensions: (t_dim: 7, y_dim: 375, x_dim: 297) Coordinates: time (t_dim) datetime64[ns] 2015-08-01T12:00:00 \u0026hellip; 2015-08-07T12:0\u0026hellip; latitude (y_dim, x_dim) float32 40.07 40.07 40.07 40.07 \u0026hellip; 65.0 65.0 65.0 longitude (y_dim, x_dim) float32 -19.89 -19.78 -19.67 \u0026hellip; 12.78 12.89 13.0 Dimensions without coordinates: t_dim, y_dim, x_dim Data variables: PEA (t_dim, y_dim, x_dim) float64 nan nan nan nan \u0026hellip; nan nan nan nanxarray.DatasetDimensions:t_dim: 7y_dim: 375x_dim: 297Coordinates: (3)time(t_dim)datetime64[ns]2015-08-01T12:00:00 \u0026hellip; 2015-08-\u0026hellip;array(['2015-08-01T12:00:00.000000000', '2015-08-02T12:00:00.000000000', '2015-08-03T12:00:00.000000000', '2015-08-04T12:00:00.000000000', '2015-08-05T12:00:00.000000000', '2015-08-06T12:00:00.000000000', '2015-08-07T12:00:00.000000000'], dtype='datetime64[ns]')latitude(y_dim, x_dim)float3240.07 40.07 40.07 \u0026hellip; 65.0 65.0array([[40.066406, 40.066406, 40.066406, \u0026hellip;, 40.066406, 40.066406, 40.066406], [40.13379 , 40.13379 , 40.13379 , \u0026hellip;, 40.13379 , 40.13379 , 40.13379 ], [40.200195, 40.200195, 40.200195, \u0026hellip;, 40.200195, 40.200195, 40.200195], \u0026hellip;, [64.868164, 64.868164, 64.868164, \u0026hellip;, 64.868164, 64.868164, 64.868164], [64.93457 , 64.93457 , 64.93457 , \u0026hellip;, 64.93457 , 64.93457 , 64.93457 ], [65.00098 , 65.00098 , 65.00098 , \u0026hellip;, 65.00098 , 65.00098 , 65.00098 ]], dtype=float32)longitude(y_dim, x_dim)float32-19.89 -19.78 -19.67 \u0026hellip; 12.89 13.0array([[-19.888672, -19.777344, -19.666992, \u0026hellip;, 12.777344, 12.888672, 13. ], [-19.888672, -19.777344, -19.666992, \u0026hellip;, 12.777344, 12.888672, 13. ], [-19.888672, -19.777344, -19.666992, \u0026hellip;, 12.777344, 12.888672, 13. ], \u0026hellip;, [-19.888672, -19.777344, -19.666992, \u0026hellip;, 12.777344, 12.888672, 13. ], [-19.888672, -19.777344, -19.666992, \u0026hellip;, 12.777344, 12.888672, 13. ], [-19.888672, -19.777344, -19.666992, \u0026hellip;, 12.777344, 12.888672, 13. ]], dtype=float32)Data variables: (1)PEA(t_dim, y_dim, x_dim)float64nan nan nan nan \u0026hellip; nan nan nan nanunits :J / m^3standard_name :Potential Energy Anomalyarray([[[ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, 262.43849344, 261.16678604, \u0026hellip;, nan, nan, nan], [ nan, 262.22882652, 292.63335667, \u0026hellip;, nan, nan, nan], \u0026hellip;, [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan]],\n [[ nan, nan, nan, ..., nan, nan, nan], [ nan, 269.18332328, 268.17983174, ..., nan, nan, nan], [ nan, 268.76494494, 226.91876185, ..., nan, nan, nan],  \u0026hellip; [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan]],\n [[ nan, nan, nan, ..., nan, nan, nan], [ nan, 263.17561991, 262.13100791, ..., nan, nan, nan], [ nan, 263.60651849, 268.85516316, ..., nan, nan, nan], ..., [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan]]])\u0026lt;/pre\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li class='xr-section-item'\u0026gt;\u0026lt;input id='section-d1fbae21-8d39-4ae9-92ce-5ee6cdd87758' class='xr-section-summary-in' type='checkbox' disabled \u0026gt;\u0026lt;label for='section-d1fbae21-8d39-4ae9-92ce-5ee6cdd87758' class='xr-section-summary' title='Expand/collapse section'\u0026gt;Indexes: \u0026lt;span\u0026gt;(0)\u0026lt;/span\u0026gt;\u0026lt;/label\u0026gt;\u0026lt;div class='xr-section-inline-details'\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;div class='xr-section-details'\u0026gt;\u0026lt;ul class='xr-var-list'\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li class='xr-section-item'\u0026gt;\u0026lt;input id='section-35cd11f7-b9f6-4ea6-9f65-1b6715761d29' class='xr-section-summary-in' type='checkbox' disabled \u0026gt;\u0026lt;label for='section-35cd11f7-b9f6-4ea6-9f65-1b6715761d29' class='xr-section-summary' title='Expand/collapse section'\u0026gt;Attributes: \u0026lt;span\u0026gt;(0)\u0026lt;/span\u0026gt;\u0026lt;/label\u0026gt;\u0026lt;div class='xr-section-inline-details'\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;div class='xr-section-details'\u0026gt;\u0026lt;dl class='xr-attrs'\u0026gt;\u0026lt;/dl\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/div\u0026gt;  Use MaskMaker to define regions and do regional analysis MaskMaker can build a stack of boolean masks in an xarray dataset for regional analysis. For the NWS we can use some built-in regions.\nmm = coast.MaskMaker() # Define Regional Masks regional_masks = [] # Define convenient aliases based on nemo data lon = nemo.dataset.longitude.values lat = nemo.dataset.latitude.values bathy = nemo.dataset.bathymetry.values # Add regional mask for whole domain regional_masks.append(np.ones(lon.shape)) # Add regional mask for English Channel regional_masks.append(mm.region_def_nws_north_sea(lon, lat, bathy)) regional_masks.append(mm.region_def_nws_outer_shelf(lon, lat, bathy)) regional_masks.append(mm.region_def_nws_norwegian_trench(lon, lat, bathy)) regional_masks.append(mm.region_def_nws_english_channel(lon, lat, bathy)) regional_masks.append(mm.region_def_south_north_sea(lon, lat, bathy)) regional_masks.append(mm.region_def_off_shelf(lon, lat, bathy)) regional_masks.append(mm.region_def_irish_sea(lon, lat, bathy)) regional_masks.append(mm.region_def_kattegat(lon, lat, bathy)) region_names = [\u0026#34;whole domain\u0026#34;, \u0026#34;north sea\u0026#34;, \u0026#34;outer shelf\u0026#34;, \u0026#34;norwegian trench\u0026#34;, \u0026#34;english_channel\u0026#34;, \u0026#34;southern north sea\u0026#34;, \u0026#34;off shelf\u0026#34;, \u0026#34;irish sea\u0026#34;, \u0026#34;kattegat\u0026#34;,] --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) Cell In[8], line 15 12 regional_masks.append(np.ones(lon.shape)) 14 # Add regional mask for English Channel ---\u0026gt; 15 regional_masks.append(mm.region_def_nws_north_sea(lon, lat, bathy)) 16 regional_masks.append(mm.region_def_nws_outer_shelf(lon, lat, bathy)) 17 regional_masks.append(mm.region_def_nws_norwegian_trench(lon, lat, bathy)) AttributeError: 'MaskMaker' object has no attribute 'region_def_nws_north_sea'  Convert this list of masks into a dataset\nmask_list = mm.make_mask_dataset(lon, lat, regional_masks, region_names) --------------------------------------------------------------------------- NameError Traceback (most recent call last) Cell In[9], line 1 ----\u0026gt; 1 mask_list = mm.make_mask_dataset(lon, lat, regional_masks, region_names) NameError: name 'region_names' is not defined  mask_list --------------------------------------------------------------------------- NameError Traceback (most recent call last) Cell In[10], line 1 ----\u0026gt; 1 mask_list NameError: name 'mask_list' is not defined  Inspect the mask with a quick_plot() method.\nmm.quick_plot(mask_list) --------------------------------------------------------------------------- NameError Traceback (most recent call last) Cell In[11], line 1 ----\u0026gt; 1 mm.quick_plot(mask_list) NameError: name 'mask_list' is not defined  NB overlapping regions are not given special treatment, the layers are blindly superimposed on each other. E.g. as demonstrated with \u0026ldquo;Norwegian Trench\u0026rdquo; and \u0026ldquo;off shelf\u0026rdquo;, or \u0026ldquo;whole domain\u0026rdquo; and any other region.\nplt.subplot(2,2,1) mm.quick_plot(mask_list.sel(dim_mask=[0,3])) plt.subplot(2,2,2) mm.quick_plot(mask_list.sel(dim_mask=[1,2,4,5,6,7,8])) plt.tight_layout() --------------------------------------------------------------------------- NameError Traceback (most recent call last) Cell In[12], line 2 1 plt.subplot(2,2,1) ----\u0026gt; 2 mm.quick_plot(mask_list.sel(dim_mask=[0,3])) 4 plt.subplot(2,2,2) 5 mm.quick_plot(mask_list.sel(dim_mask=[1,2,4,5,6,7,8])) NameError: name 'mask_list' is not defined  # Show overlap mask_list.mask.sum(dim=\u0026#39;dim_mask\u0026#39;).plot( levels=(1,2,3,4)) # Save if required #plt.savefig(\u0026#39;tmp.png\u0026#39;) --------------------------------------------------------------------------- NameError Traceback (most recent call last) Cell In[13], line 2 1 # Show overlap ----\u0026gt; 2 mask_list.mask.sum(dim='dim_mask').plot( levels=(1,2,3,4)) 4 # Save if required 5 #plt.savefig('tmp.png') NameError: name 'mask_list' is not defined  Regional analysis Average stratification object over regions using the mask\nmask_means = (strat.dataset*mask_list.mask).mean(dim=\u0026#39;x_dim\u0026#39;).mean(dim=\u0026#39;y_dim\u0026#39;) --------------------------------------------------------------------------- NameError Traceback (most recent call last) Cell In[14], line 1 ----\u0026gt; 1 mask_means = (strat.dataset*mask_list.mask).mean(dim='x_dim').mean(dim='y_dim') NameError: name 'mask_list' is not defined  mask_means --------------------------------------------------------------------------- NameError Traceback (most recent call last) Cell In[15], line 1 ----\u0026gt; 1 mask_means NameError: name 'mask_means' is not defined  # Plot timeseries per region for count_region in range(mask_means.dims[\u0026#39;dim_mask\u0026#39;]): plt.plot( mask_means.PEA.isel(dim_mask=count_region), label=mask_means.region_names[count_region].values, marker=\u0026#34;.\u0026#34;, linestyle=\u0026#39;none\u0026#39;) plt.xlabel(\u0026#39;time\u0026#39;); plt.ylabel(\u0026#39;PEA\u0026#39;) plt.legend() --------------------------------------------------------------------------- NameError Traceback (most recent call last) Cell In[16], line 3 1 # Plot timeseries per region ----\u0026gt; 3 for count_region in range(mask_means.dims['dim_mask']): 5 plt.plot( 6 mask_means.PEA.isel(dim_mask=count_region), 7 label=mask_means.region_names[count_region].values, 8 marker=\u0026quot;.\u0026quot;, linestyle='none') 10 plt.xlabel('time'); plt.ylabel('PEA') NameError: name 'mask_means' is not defined  ","excerpt":"A demonstration to calculate the Potential Energy Anomaly and demonstrate regional masking with …","ref":"/COAsT/docs/examples/notebooks/gridded/potential_energy_tutorial/","title":"Potential energy tutorial"},{"body":"","excerpt":"","ref":"/COAsT/docs/examples/notebooks/profile/","title":"Profile"},{"body":"A demonstration of pycnocline depth and thickness diagnostics. The first and second depth moments of stratification are computed as proxies for pycnocline depth and thickness, suitable for a nearly two-layer fluid.\nNote that in the AMM7 example data the plots are not particularly spectacular as the internal tide is poorly resolved at 7km.\nRelevant imports and filepath configuration import coast import numpy as np import os import matplotlib.pyplot as plt import matplotlib.colors as colors # colormap fiddling # set some paths root = \u0026#34;./\u0026#34; dn_files = root + \u0026#34;./example_files/\u0026#34; fn_nemo_grid_t_dat = dn_files + \u0026#34;nemo_data_T_grid_Aug2015.nc\u0026#34; fn_nemo_dom = dn_files + \u0026#34;coast_example_nemo_domain.nc\u0026#34; config_t = root + \u0026#34;./config/example_nemo_grid_t.json\u0026#34; config_w = root + \u0026#34;./config/example_nemo_grid_w.json\u0026#34; Loading data # Create a Gridded object and load in the data: nemo_t = coast.Gridded(fn_nemo_grid_t_dat, fn_nemo_dom, config=config_t) #nemo_t.dataset # uncomment to print data object summary # The stratification variables are computed as centred differences of the t-grid variables.  # These will become w-grid variables. So, create an empty w-grid object, to store stratification.  # Note how we do not pass a NEMO data file for this load. nemo_w = coast.Gridded(fn_domain=fn_nemo_dom, config=config_w) Subset the domain We are not interested in the whole doman so it is computationally efficient to subset the data for the region of interest. Here we will look at the North Sea between (51N: 62N) and (-4E:15E). We will great subset objects for both the t- and w-grids:\nind_2d = nemo_t.subset_indices(start=[51,-4], end=[62,15]) nemo_nwes_t = nemo_t.isel(y_dim=ind_2d[0], x_dim=ind_2d[1]) #nwes = northwest european shelf ind_2d = nemo_w.subset_indices(start=[51,-4], end=[62,15]) nemo_nwes_w = nemo_w.isel(y_dim=ind_2d[0], x_dim=ind_2d[1]) #nwes = northwest european shelf #nemo_nwes_t.dataset # uncomment to print data object summary Diagnostic calculations and plotting We can use a COAsT method to construct the in-situ density:\nnemo_nwes_t.construct_density( eos=\u0026#39;EOS10\u0026#39; ) Then we construct stratification using a COAsT method to take the vertical derivative. Noting that the inputs are on t-pts and the outputs are on w-pt\nnemo_nwes_w = nemo_nwes_t.differentiate( \u0026#39;density\u0026#39;, dim=\u0026#39;z_dim\u0026#39;, out_var_str=\u0026#39;rho_dz\u0026#39;, out_obj=nemo_nwes_w ) # --\u0026gt; sci_nwes_w.rho_dz This has created a variable called nemo_nwes_w.rho_dz.\nCreate internal tide diagnostics We can now use the GriddedStratification class to construct the first and second moments (over depth) of density. In the limit of an idealised two-layer fluid these converge to the depth and thickness of the interface. I.e. the pycnocline depth and thickness respectively.\nstrat = coast.GriddedStratification(nemo_nwes_t) #%% Construct pycnocline variables: depth and thickness strat.construct_pycnocline_vars( nemo_nwes_t, nemo_nwes_w ) /usr/share/miniconda/envs/coast/lib/python3.8/site-packages/xarray/core/computation.py:771: RuntimeWarning: invalid value encountered in sqrt result_data = func(*input_data)  Plotting data Finally we plot pycnocline variables (depth and thickness) using an GriddedStratification method:\nstrat.quick_plot() (\u0026lt;Figure size 1000x1000 with 2 Axes\u0026gt;, \u0026lt;Axes: title={'center': '01 Aug 2015: masked pycnocline thickness (m)'}, xlabel='longitude', ylabel='latitude'\u0026gt;)  ","excerpt":"A demonstration of pycnocline depth and thickness diagnostics. The first and second depth moments of …","ref":"/COAsT/docs/examples/notebooks/gridded/pycnocline_tutorial/","title":"Pycnocline tutorial"},{"body":"This is a demonstration on regridding in COAsT. To do this, the COAsT package uses the already capable xesmf package, which will need to be installed independently (is not natively part of the COAsT package).\nIntroduction COAsT uses XESMF by providing a data class xesmf_convert which provides functions to prepare COAsT.Gridded objects, so they can be passed to XESMF for regridding to either a curvilinear or rectilienar grid.\nAll you need to do if provide a Gridded object and a grid type when creating a new instance of this class. It will then contain an appropriate input dataset. You may also provide a second COAsT gridded object if regridding between two objects.\nInstall XESMF See the package\u0026rsquo;s documentation website here:\nhttps://xesmf.readthedocs.io/en/latest/index.html  You can install XESMF using:\n conda install -c conda-forge xesmf.  The setup used by this class has been tested for xesmf v0.6.2 alongside esmpy v8.0.0. It was installed using:\n conda install -c conda-forge xesmf esmpy=8.0.0  Example useage If regridding a Gridded object to an arbitrarily defined rectilinear or curvilinear grid, you just need to do the following:\nimport xesmf as xe # Create your gridded object gridded = coast.Gridded(*args, **kwargs) # Pass the gridded object over to xesmf_convert xesmf_ready = coast.xesmf_convert(gridded, input_grid_type = 'curvilinear') # Now this object will contain a dataset called xesmf_input, which can # be passed over to xesmf. E.G: destination_grid = xesmf.util.grid_2d(-15, 15, 1, 45, 65, 1) regridder = xe.Regridder(xesmf_ready.input_grid, destination_grid, \u0026quot;bilinear\u0026quot;) regridded_dataset = regridder(xesmf_ready.input_data) XESMF contains a couple of difference functions for quickly creating output grids, such as xesmf.util.grid_2d and xesmf.util.grid_global(). See their website for more info.\nThe process is almost the same if regridding from one COAsT.Gridded object to another (gridded0 -\u0026gt; gridded1):\nxesmf_ready = coast.xesmf_convert(gridded0, gridded1, input_grid_type = \u0026quot;curvilinear\u0026quot;, output_grid_type = \u0026quot;curvilinear\u0026quot;) regridder = xe.Regridder(xesmf_ready.input_grid, xesmf_ready.output_grid, \u0026quot;bilinear\u0026quot;) regridded_dataset = regridder(xesmf_ready.input_data) Note that you can select which variables you want to regrid, either prior to using this tool or by indexing the input_data dataset. e.g.:\nregridded_dataset = regridder(xesmf_ready.input_data['temperature']) If your input datasets were lazy loaded, then so will the regridded dataset. At this point you can either load the data or (recomended) save the regridded data to file:\nregridded_dataset.to_netcdf(\u0026lt;filename_to_save\u0026gt;) Before saving back to file, call xesmf_ready.to_gridded() to convert the regridded xesmf object back to a gridded object\nCompatability Note (written 8 Sept 2022) xesmf is not included natively within COAsT as satisfying all the dependencies within COAsT gets increasingly challenging with more components in COAsT. So whilst valuable, xesmf is currently deemed not core. Here are some notes from a user on its installation with conda:\nA conda environemt with `esmpy=8.0.0` specified and `xesmf` version unspecified works suggests a downgrade of: netCDF4 1.5.6 scipy 1.5.3 lxml 4.8 A solution to avoid the downgrade maybe found in https://github.com/pangeo-data/pangeo-docker-images/issues/101 conda create … \u0026quot;mpi==openmpi\u0026quot; \u0026quot;esmpy==mpi_openmpi*\u0026quot; xesmf ","excerpt":"This is a demonstration on regridding in COAsT. To do this, the COAsT package uses the already …","ref":"/COAsT/docs/examples/notebooks/gridded/regridding_with_xesmf_tutorial/","title":"Regridding with xesmf tutorial"},{"body":"Tutorial to make a simple SEAsia 1/12 deg DIC plot.\nImport the relevant packages import coast import matplotlib.pyplot as plt Define file paths for data root = \u0026#34;./\u0026#34; dn_files = root + \u0026#34;./example_files/\u0026#34; path_config = root + \u0026#34;./config/\u0026#34; fn_seasia_domain = dn_files + \u0026#34;coast_example_domain_SEAsia.nc\u0026#34; fn_seasia_var = dn_files + \u0026#34;coast_example_SEAsia_BGC_1990.nc\u0026#34; fn_seasia_config_bgc = path_config + \u0026#34;example_nemo_bgc.json\u0026#34; Create a Gridded object seasia_bgc = coast.Gridded(fn_data=fn_seasia_var, fn_domain=fn_seasia_domain, config=fn_seasia_config_bgc) Plot DIC fig = plt.figure() plt.pcolormesh( seasia_bgc.dataset.longitude, seasia_bgc.dataset.latitude, seasia_bgc.dataset.dic.isel(t_dim=0).isel(z_dim=0), cmap=\u0026#34;RdYlBu_r\u0026#34;, vmin=1600, vmax=2080, ) plt.colorbar() plt.title(\u0026#34;DIC, mmol/m^3\u0026#34;) plt.xlabel(\u0026#34;longitude\u0026#34;) plt.ylabel(\u0026#34;latitude\u0026#34;) plt.show() /tmp/ipykernel_3997/1161426776.py:2: UserWarning: The input coordinates to pcolormesh are interpreted as cell centers, but are not monotonically increasing or decreasing. This may lead to incorrectly calculated cell edges, in which case, please supply explicit cell edges to pcolormesh. plt.pcolormesh(  ","excerpt":"Tutorial to make a simple SEAsia 1/12 deg DIC plot.\nImport the relevant packages import coast import …","ref":"/COAsT/docs/examples/notebooks/gridded/seasia_dic_example_plot_tutorial/","title":"Seasia dic example plot tutorial"},{"body":"Overview A function within the Process_data class that will decompose time series into trend, seasonal and residual components. The function is a wrapper that adds functionality to the seasonal_decompose function contained in the statsmodels package to make it more convenient for large geospatial datasets.\nSpecifically:\n Multiple time series spread across multiple dimensions, e.g. a gridded dataset, can be processed. The user simply passes in an xarray DataArray that has a \u0026ldquo;t_dim\u0026rdquo; dimension and 1 or more additional dimensions, for example gridded spatial dimensions Masked locations, such as land points, are handled A dask wrapper is applied to the function that a) supports lazy evaluation b) allows the dataset to be easily seperated into chunks so that processing can be carried out in parallel (rather than processing every time series sequentially) The decomposed time series are returned as xarray DataArrays within a single coast.Gridded object  An example Below is an example using the coast.Process_data.seasonal_decomposition function with the example data. Note that we will artifically extend the length of the example data time series for demonstrative purposes.\nBegin by importing coast, defining paths to the data, and loading the example data into a gridded object:\nimport coast import numpy as np import xarray as xr # Path to a data file root = \u0026#34;./\u0026#34; dn_files = root + \u0026#34;./example_files/\u0026#34; fn_nemo_dat = dn_files + \u0026#34;coast_example_nemo_data.nc\u0026#34; # Set path for domain file if required. fn_nemo_dom = dn_files + \u0026#34;coast_example_nemo_domain.nc\u0026#34; # Set path for model configuration file config = root + \u0026#34;./config/example_nemo_grid_t.json\u0026#34; # Read in data (This example uses NEMO data.) grd = coast.Gridded(fn_nemo_dat, fn_nemo_dom, config=config) The loaded example data only has 7 time stamps, the code below creates a new (fake) extended temperature variable with 48 monthly records. This code is not required to use the function, it is only included here to make a set of time series that are long enough to be interesting.\n# create a 4 yr monthly time coordinate array time_array = np.arange( np.datetime64(\u0026#34;2010-01-01\u0026#34;), np.datetime64(\u0026#34;2014-01-01\u0026#34;), np.timedelta64(1, \u0026#34;M\u0026#34;), dtype=\u0026#34;datetime64[M]\u0026#34; ).astype(\u0026#34;datetime64[s]\u0026#34;) # create 4 years of monthly temperature data based on the loaded data temperature_array = ( (np.arange(0, 48) * 0.05)[:, np.newaxis, np.newaxis, np.newaxis] + np.random.normal(0, 0.1, 48)[:, np.newaxis, np.newaxis, np.newaxis] + np.tile(grd.dataset.temperature[:-1, :2, :, :], (8, 1, 1, 1)) ) # create a new temperature DataArray temperature = xr.DataArray( temperature_array, coords={ \u0026#34;t_dim\u0026#34;: time_array, \u0026#34;depth_0\u0026#34;: grd.dataset.depth_0[:2, :, :], \u0026#34;longitude\u0026#34;: grd.dataset.longitude, \u0026#34;latitude\u0026#34;: grd.dataset.latitude, }, dims=[\u0026#34;t_dim\u0026#34;, \u0026#34;z_dim\u0026#34;, \u0026#34;y_dim\u0026#34;, \u0026#34;x_dim\u0026#34;], ) Check out the new data\n#temperature # uncomment to print data object summary temperature[0,0,:,:].plot() \u0026lt;matplotlib.collections.QuadMesh at 0x7f567dfa8040\u0026gt;  Check out time series at 2 different grid points\ntemperature[:,0,50,50].plot() temperature[:,0,200,200].plot() [\u0026lt;matplotlib.lines.Line2D at 0x7f567cbdb7c0\u0026gt;]  Create a coast.Process_data object, and call the seasonal_decomposition function, passing in the required arguments. The first two arguments are:\n The input data, here the temperature data as an xarray DataArray The number of chuncks to split the data into. Here we split the data into 2 chunks so that the dask scheduler will try to run 4 processes in parallel  The remaining arguments are keyword arguments for the underlying statsmodels.tsa.seasonal.seasonal_decompose function, which are documented on the statsmodels documentation pages. Here we specify:\nthree The type of model, i.e. an additive model The period of the seasonal cycle, here it is 6 months Extrapolate the trend component to cover the entire range of the time series (this is required because the trend is calculated using a convolution filter)  proc_data = coast.Process_data() grd = proc_data.seasonal_decomposition(temperature, 2, model=\u0026#34;additive\u0026#34;, period=6, extrapolate_trend=\u0026#34;freq\u0026#34;) The returned xarray Dataset contains the decomposed time series (trend, seasonal, residual) as dask arrays\n#grd.dataset # uncomment to print data object summary Execute the computation\ngrd.dataset.compute()             /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { \u0026ndash;xr-font-color0: var(\u0026ndash;jp-content-font-color0, rgba(0, 0, 0, 1)); \u0026ndash;xr-font-color2: var(\u0026ndash;jp-content-font-color2, rgba(0, 0, 0, 0.54)); \u0026ndash;xr-font-color3: var(\u0026ndash;jp-content-font-color3, rgba(0, 0, 0, 0.38)); \u0026ndash;xr-border-color: var(\u0026ndash;jp-border-color2, #e0e0e0); \u0026ndash;xr-disabled-color: var(\u0026ndash;jp-layout-color3, #bdbdbd); \u0026ndash;xr-background-color: var(\u0026ndash;jp-layout-color0, white); \u0026ndash;xr-background-color-row-even: var(\u0026ndash;jp-layout-color1, white); \u0026ndash;xr-background-color-row-odd: var(\u0026ndash;jp-layout-color2, #eeeeee); }\nhtml[theme=dark], body[data-theme=dark], body.vscode-dark { \u0026ndash;xr-font-color0: rgba(255, 255, 255, 1); \u0026ndash;xr-font-color2: rgba(255, 255, 255, 0.54); \u0026ndash;xr-font-color3: rgba(255, 255, 255, 0.38); \u0026ndash;xr-border-color: #1F1F1F; \u0026ndash;xr-disabled-color: #515151; \u0026ndash;xr-background-color: #111111; \u0026ndash;xr-background-color-row-even: #111111; \u0026ndash;xr-background-color-row-odd: #313131; }\n.xr-wrap { display: block !important; min-width: 300px; max-width: 700px; }\n.xr-text-repr-fallback { /* fallback to plain text repr when CSS is not injected (untrusted notebook) */ display: none; }\n.xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(\u0026ndash;xr-border-color); }\n.xr-header \u0026gt; div, .xr-header \u0026gt; ul { display: inline; margin-top: 0; margin-bottom: 0; }\n.xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; }\n.xr-obj-type { color: var(\u0026ndash;xr-font-color2); }\n.xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; }\n.xr-section-item { display: contents; }\n.xr-section-item input { display: none; }\n.xr-section-item input + label { color: var(\u0026ndash;xr-disabled-color); }\n.xr-section-item input:enabled + label { cursor: pointer; color: var(\u0026ndash;xr-font-color2); }\n.xr-section-item input:enabled + label:hover { color: var(\u0026ndash;xr-font-color0); }\n.xr-section-summary { grid-column: 1; color: var(\u0026ndash;xr-font-color2); font-weight: 500; }\n.xr-section-summary \u0026gt; span { display: inline-block; padding-left: 0.5em; }\n.xr-section-summary-in:disabled + label { color: var(\u0026ndash;xr-font-color2); }\n.xr-section-summary-in + label:before { display: inline-block; content: \u0026lsquo;►\u0026rsquo;; font-size: 11px; width: 15px; text-align: center; }\n.xr-section-summary-in:disabled + label:before { color: var(\u0026ndash;xr-disabled-color); }\n.xr-section-summary-in:checked + label:before { content: \u0026lsquo;▼\u0026rsquo;; }\n.xr-section-summary-in:checked + label \u0026gt; span { display: none; }\n.xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; }\n.xr-section-inline-details { grid-column: 2 / -1; }\n.xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; }\n.xr-section-summary-in:checked ~ .xr-section-details { display: contents; }\n.xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; }\n.xr-array-wrap \u0026gt; label { grid-column: 1; vertical-align: top; }\n.xr-preview { color: var(\u0026ndash;xr-font-color3); }\n.xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; }\n.xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; }\n.xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; }\n.xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; }\n.xr-dim-list li { display: inline-block; padding: 0; margin: 0; }\n.xr-dim-list:before { content: \u0026lsquo;('; }\n.xr-dim-list:after { content: \u0026lsquo;)'; }\n.xr-dim-list li:not(:last-child):after { content: \u0026lsquo;,'; padding-right: 5px; }\n.xr-has-index { font-weight: bold; }\n.xr-var-list, .xr-var-item { display: contents; }\n.xr-var-item \u0026gt; div, .xr-var-item label, .xr-var-item \u0026gt; .xr-var-name span { background-color: var(\u0026ndash;xr-background-color-row-even); margin-bottom: 0; }\n.xr-var-item \u0026gt; .xr-var-name:hover span { padding-right: 5px; }\n.xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; div, .xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; label, .xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; .xr-var-name span { background-color: var(\u0026ndash;xr-background-color-row-odd); }\n.xr-var-name { grid-column: 1; }\n.xr-var-dims { grid-column: 2; }\n.xr-var-dtype { grid-column: 3; text-align: right; color: var(\u0026ndash;xr-font-color2); }\n.xr-var-preview { grid-column: 4; }\n.xr-index-preview { grid-column: 2 / 5; color: var(\u0026ndash;xr-font-color2); }\n.xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; }\n.xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; }\n.xr-var-attrs, .xr-var-data, .xr-index-data { display: none; background-color: var(\u0026ndash;xr-background-color) !important; padding-bottom: 5px !important; }\n.xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data, .xr-index-data-in:checked ~ .xr-index-data { display: block; }\n.xr-var-data \u0026gt; table { float: right; }\n.xr-var-name span, .xr-var-data, .xr-index-name div, .xr-index-data, .xr-attrs { padding-left: 25px !important; }\n.xr-attrs, .xr-var-attrs, .xr-var-data, .xr-index-data { grid-column: 1 / -1; }\ndl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; }\n.xr-attrs dt, .xr-attrs dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; }\n.xr-attrs dt { font-weight: normal; grid-column: 1; }\n.xr-attrs dt:hover span { display: inline-block; background: var(\u0026ndash;xr-background-color); padding-right: 10px; }\n.xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; }\n.xr-icon-database, .xr-icon-file-text2, .xr-no-icon { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } \u0026lt;xarray.Dataset\u0026gt; Dimensions: (t_dim: 48, z_dim: 2, y_dim: 375, x_dim: 297) Coordinates:\n  t_dim (t_dim) datetime64[ns] 2010-01-01 2010-02-01 \u0026hellip; 2013-12-01 depth_0 (z_dim, y_dim, x_dim) float32 0.5 0.5 0.5 0.5 \u0026hellip; 1.5 1.5 1.5 1.5 longitude (y_dim, x_dim) float32 -19.89 -19.78 -19.67 \u0026hellip; 12.78 12.89 13.0 latitude (y_dim, x_dim) float32 40.07 40.07 40.07 40.07 \u0026hellip; 65.0 65.0 65.0 Dimensions without coordinates: z_dim, y_dim, x_dim Data variables: trend (t_dim, z_dim, y_dim, x_dim) float64 nan nan nan \u0026hellip; nan nan nan seasonal (t_dim, z_dim, y_dim, x_dim) float64 nan nan nan \u0026hellip; nan nan nan residual (t_dim, z_dim, y_dim, x_dim) float64 nan nan nan \u0026hellip; nan nan nanxarray.DatasetDimensions:t_dim: 48z_dim: 2y_dim: 375x_dim: 297Coordinates: (4)t_dim(t_dim)datetime64[ns]2010-01-01 \u0026hellip; 2013-12-01array(['2010-01-01T00:00:00.000000000', '2010-02-01T00:00:00.000000000', '2010-03-01T00:00:00.000000000', '2010-04-01T00:00:00.000000000', '2010-05-01T00:00:00.000000000', '2010-06-01T00:00:00.000000000', '2010-07-01T00:00:00.000000000', '2010-08-01T00:00:00.000000000', '2010-09-01T00:00:00.000000000', '2010-10-01T00:00:00.000000000', '2010-11-01T00:00:00.000000000', '2010-12-01T00:00:00.000000000', '2011-01-01T00:00:00.000000000', '2011-02-01T00:00:00.000000000', '2011-03-01T00:00:00.000000000', '2011-04-01T00:00:00.000000000', '2011-05-01T00:00:00.000000000', '2011-06-01T00:00:00.000000000', '2011-07-01T00:00:00.000000000', '2011-08-01T00:00:00.000000000', '2011-09-01T00:00:00.000000000', '2011-10-01T00:00:00.000000000', '2011-11-01T00:00:00.000000000', '2011-12-01T00:00:00.000000000', '2012-01-01T00:00:00.000000000', '2012-02-01T00:00:00.000000000', '2012-03-01T00:00:00.000000000', '2012-04-01T00:00:00.000000000', '2012-05-01T00:00:00.000000000', '2012-06-01T00:00:00.000000000', '2012-07-01T00:00:00.000000000', '2012-08-01T00:00:00.000000000', '2012-09-01T00:00:00.000000000', '2012-10-01T00:00:00.000000000', '2012-11-01T00:00:00.000000000', '2012-12-01T00:00:00.000000000', '2013-01-01T00:00:00.000000000', '2013-02-01T00:00:00.000000000', '2013-03-01T00:00:00.000000000', '2013-04-01T00:00:00.000000000', '2013-05-01T00:00:00.000000000', '2013-06-01T00:00:00.000000000', '2013-07-01T00:00:00.000000000', '2013-08-01T00:00:00.000000000', '2013-09-01T00:00:00.000000000', '2013-10-01T00:00:00.000000000', '2013-11-01T00:00:00.000000000', '2013-12-01T00:00:00.000000000'], dtype='datetime64[ns]')depth_0(z_dim, y_dim, x_dim)float320.5 0.5 0.5 0.5 \u0026hellip; 1.5 1.5 1.5 1.5units :mstandard_name :Depth at time zero on the t-gridarray([[[0.5 , 0.5 , 0.5 , \u0026hellip;, 0.5 , 0.5 , 0.5 ], [0.5 , 0.4975586 , 0.4975586 , \u0026hellip;, 0.10009766, 0.10009766, 0.5 ], [0.5 , 0.4975586 , 0.4975586 , \u0026hellip;, 0.10009766, 0.10009766, 0.5 ], \u0026hellip;, [0.5 , 0.10009766, 0.10009766, \u0026hellip;, 0.10009766, 0.10009766, 0.5 ], [0.5 , 0.10009766, 0.10009766, \u0026hellip;, 0.10009766, 0.10009766, 0.5 ], [0.5 , 0.5 , 0.5 , \u0026hellip;, 0.5 , 0.5 , 0.5 ]],\n[[1.5 , 1.5 , 1.5 , \u0026hellip;, 1.5 , 1.5 , 1.5 ], [1.5 , 1.5170898 , 1.5170898 , \u0026hellip;, 0.30029297, 0.30029297, 1.5 ], [1.5 , 1.5170898 , 1.5170898 , \u0026hellip;, 0.30029297, 0.30029297, 1.5 ], \u0026hellip;, [1.5 , 0.30029297, 0.30029297, \u0026hellip;, 0.30029297, 0.30029297, 1.5 ], [1.5 , 0.30029297, 0.30029297, \u0026hellip;, 0.30029297, 0.30029297, 1.5 ], [1.5 , 1.5 , 1.5 , \u0026hellip;, 1.5 , 1.5 , 1.5 ]]], dtype=float32)longitude(y_dim, x_dim)float32-19.89 -19.78 -19.67 \u0026hellip; 12.89 13.0array([[-19.888672, -19.777344, -19.666992, \u0026hellip;, 12.777344, 12.888672, 13. ], [-19.888672, -19.777344, -19.666992, \u0026hellip;, 12.777344, 12.888672, 13. ], [-19.888672, -19.777344, -19.666992, \u0026hellip;, 12.777344, 12.888672, 13. ], \u0026hellip;, [-19.888672, -19.777344, -19.666992, \u0026hellip;, 12.777344, 12.888672, 13. ], [-19.888672, -19.777344, -19.666992, \u0026hellip;, 12.777344, 12.888672, 13. ], [-19.888672, -19.777344, -19.666992, \u0026hellip;, 12.777344, 12.888672, 13. ]], dtype=float32)latitude(y_dim, x_dim)float3240.07 40.07 40.07 \u0026hellip; 65.0 65.0array([[40.066406, 40.066406, 40.066406, \u0026hellip;, 40.066406, 40.066406, 40.066406], [40.13379 , 40.13379 , 40.13379 , \u0026hellip;, 40.13379 , 40.13379 , 40.13379 ], [40.200195, 40.200195, 40.200195, \u0026hellip;, 40.200195, 40.200195, 40.200195], \u0026hellip;, [64.868164, 64.868164, 64.868164, \u0026hellip;, 64.868164, 64.868164, 64.868164], [64.93457 , 64.93457 , 64.93457 , \u0026hellip;, 64.93457 , 64.93457 , 64.93457 ], [65.00098 , 65.00098 , 65.00098 , \u0026hellip;, 65.00098 , 65.00098 , 65.00098 ]], dtype=float32)Data variables: (3)trend(t_dim, z_dim, y_dim, x_dim)float64nan nan nan nan \u0026hellip; nan nan nan nanarray([[[[ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, 15.22220794, 15.21895274, \u0026hellip;, nan, nan, nan], [ nan, 15.21114024, 15.37894623, \u0026hellip;, nan, nan, nan], \u0026hellip;, [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan]],\n[[ nan, nan, nan, ..., nan, nan, nan], [ nan, 15.22123138, 15.21813893, ..., nan, nan, nan], [ nan, 15.21000091, 15.38285248, ..., nan, nan, nan],    \u0026hellip; [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan]],\n [[ nan, nan, nan, ..., nan, nan, nan], [ nan, 17.62710025, 17.6240078 , ..., nan, nan, nan], [ nan, 17.61586978, 17.78872134, ..., nan, nan, nan], ..., [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan]]]])\u0026lt;/pre\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li class='xr-var-item'\u0026gt;\u0026lt;div class='xr-var-name'\u0026gt;\u0026lt;span\u0026gt;seasonal\u0026lt;/span\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;div class='xr-var-dims'\u0026gt;(t_dim, z_dim, y_dim, x_dim)\u0026lt;/div\u0026gt;\u0026lt;div class='xr-var-dtype'\u0026gt;float64\u0026lt;/div\u0026gt;\u0026lt;div class='xr-var-preview xr-preview'\u0026gt;nan nan nan nan ... nan nan nan nan\u0026lt;/div\u0026gt;\u0026lt;input id='attrs-c9911e4e-7848-44af-90d4-d1667c9023ed' class='xr-var-attrs-in' type='checkbox' disabled\u0026gt;\u0026lt;label for='attrs-c9911e4e-7848-44af-90d4-d1667c9023ed' title='Show/Hide attributes'\u0026gt;\u0026lt;svg class='icon xr-icon-file-text2'\u0026gt;\u0026lt;use xlink:href='#icon-file-text2'\u0026gt;\u0026lt;/use\u0026gt;\u0026lt;/svg\u0026gt;\u0026lt;/label\u0026gt;\u0026lt;input id='data-e199cbdb-161d-4a38-8edd-64d801f2379e' class='xr-var-data-in' type='checkbox'\u0026gt;\u0026lt;label for='data-e199cbdb-161d-4a38-8edd-64d801f2379e' title='Show/Hide data repr'\u0026gt;\u0026lt;svg class='icon xr-icon-database'\u0026gt;\u0026lt;use xlink:href='#icon-database'\u0026gt;\u0026lt;/use\u0026gt;\u0026lt;/svg\u0026gt;\u0026lt;/label\u0026gt;\u0026lt;div class='xr-var-attrs'\u0026gt;\u0026lt;dl class='xr-attrs'\u0026gt;\u0026lt;/dl\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;div class='xr-var-data'\u0026gt;\u0026lt;pre\u0026gt;array([[[[ nan, nan, nan, ..., nan, nan, nan], [ nan, 0.08707137, 0.00341252, ..., nan, nan, nan], [ nan, 0.11278752, 0.34244247, ..., nan, nan, nan], ..., [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan]], [[ nan, nan, nan, ..., nan, nan, nan], [ nan, 0.07632919, -0.00749243, ..., nan, nan, nan], [ nan, 0.10220809, 0.33853622, ..., nan, nan, nan],  \u0026hellip; [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan]],\n [[ nan, nan, nan, ..., nan, nan, nan], [ nan, -0.34491586, -0.32815154, ..., nan, nan, nan], [ nan, -0.35516977, -0.70380258, ..., nan, nan, nan], ..., [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan]]]])\u0026lt;/pre\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li class='xr-var-item'\u0026gt;\u0026lt;div class='xr-var-name'\u0026gt;\u0026lt;span\u0026gt;residual\u0026lt;/span\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;div class='xr-var-dims'\u0026gt;(t_dim, z_dim, y_dim, x_dim)\u0026lt;/div\u0026gt;\u0026lt;div class='xr-var-dtype'\u0026gt;float64\u0026lt;/div\u0026gt;\u0026lt;div class='xr-var-preview xr-preview'\u0026gt;nan nan nan nan ... nan nan nan nan\u0026lt;/div\u0026gt;\u0026lt;input id='attrs-c7ea4039-7cd4-4d32-8a70-b4ca4023d93a' class='xr-var-attrs-in' type='checkbox' disabled\u0026gt;\u0026lt;label for='attrs-c7ea4039-7cd4-4d32-8a70-b4ca4023d93a' title='Show/Hide attributes'\u0026gt;\u0026lt;svg class='icon xr-icon-file-text2'\u0026gt;\u0026lt;use xlink:href='#icon-file-text2'\u0026gt;\u0026lt;/use\u0026gt;\u0026lt;/svg\u0026gt;\u0026lt;/label\u0026gt;\u0026lt;input id='data-062986f2-6583-4d8f-a176-7bd1cbaf9c74' class='xr-var-data-in' type='checkbox'\u0026gt;\u0026lt;label for='data-062986f2-6583-4d8f-a176-7bd1cbaf9c74' title='Show/Hide data repr'\u0026gt;\u0026lt;svg class='icon xr-icon-database'\u0026gt;\u0026lt;use xlink:href='#icon-database'\u0026gt;\u0026lt;/use\u0026gt;\u0026lt;/svg\u0026gt;\u0026lt;/label\u0026gt;\u0026lt;div class='xr-var-attrs'\u0026gt;\u0026lt;dl class='xr-attrs'\u0026gt;\u0026lt;/dl\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;div class='xr-var-data'\u0026gt;\u0026lt;pre\u0026gt;array([[[[ nan, nan, nan, ..., nan, nan, nan], [ nan, 0.05458194, 0.05458194, ..., nan, nan, nan], [ nan, 0.05458194, 0.05458194, ..., nan, nan, nan], ..., [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan]], [[ nan, nan, nan, ..., nan, nan, nan], [ nan, 0.05458194, 0.05458194, ..., nan, nan, nan], [ nan, 0.05458194, 0.05458194, ..., nan, nan, nan],  \u0026hellip; [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan]],\n [[ nan, nan, nan, ..., nan, nan, nan], [ nan, -0.01408253, -0.01408253, ..., nan, nan, nan], [ nan, -0.01408253, -0.01408253, ..., nan, nan, nan], ..., [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan]]]])\u0026lt;/pre\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li class='xr-section-item'\u0026gt;\u0026lt;input id='section-9661b5e4-eee2-4dea-97ed-27736d778672' class='xr-section-summary-in' type='checkbox' \u0026gt;\u0026lt;label for='section-9661b5e4-eee2-4dea-97ed-27736d778672' class='xr-section-summary' \u0026gt;Indexes: \u0026lt;span\u0026gt;(1)\u0026lt;/span\u0026gt;\u0026lt;/label\u0026gt;\u0026lt;div class='xr-section-inline-details'\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;div class='xr-section-details'\u0026gt;\u0026lt;ul class='xr-var-list'\u0026gt;\u0026lt;li class='xr-var-item'\u0026gt;\u0026lt;div class='xr-index-name'\u0026gt;\u0026lt;div\u0026gt;t_dim\u0026lt;/div\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;div class='xr-index-preview'\u0026gt;PandasIndex\u0026lt;/div\u0026gt;\u0026lt;div\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;input id='index-38d0f3d7-8a45-4080-84d9-a21f0abf7bcd' class='xr-index-data-in' type='checkbox'/\u0026gt;\u0026lt;label for='index-38d0f3d7-8a45-4080-84d9-a21f0abf7bcd' title='Show/Hide index repr'\u0026gt;\u0026lt;svg class='icon xr-icon-database'\u0026gt;\u0026lt;use xlink:href='#icon-database'\u0026gt;\u0026lt;/use\u0026gt;\u0026lt;/svg\u0026gt;\u0026lt;/label\u0026gt;\u0026lt;div class='xr-index-data'\u0026gt;\u0026lt;pre\u0026gt;PandasIndex(DatetimeIndex([\u0026amp;#x27;2010-01-01\u0026amp;#x27;, \u0026amp;#x27;2010-02-01\u0026amp;#x27;, \u0026amp;#x27;2010-03-01\u0026amp;#x27;, \u0026amp;#x27;2010-04-01\u0026amp;#x27;, \u0026amp;#x27;2010-05-01\u0026amp;#x27;, \u0026amp;#x27;2010-06-01\u0026amp;#x27;, \u0026amp;#x27;2010-07-01\u0026amp;#x27;, \u0026amp;#x27;2010-08-01\u0026amp;#x27;, \u0026amp;#x27;2010-09-01\u0026amp;#x27;, \u0026amp;#x27;2010-10-01\u0026amp;#x27;, \u0026amp;#x27;2010-11-01\u0026amp;#x27;, \u0026amp;#x27;2010-12-01\u0026amp;#x27;, \u0026amp;#x27;2011-01-01\u0026amp;#x27;, \u0026amp;#x27;2011-02-01\u0026amp;#x27;, \u0026amp;#x27;2011-03-01\u0026amp;#x27;, \u0026amp;#x27;2011-04-01\u0026amp;#x27;, \u0026amp;#x27;2011-05-01\u0026amp;#x27;, \u0026amp;#x27;2011-06-01\u0026amp;#x27;, \u0026amp;#x27;2011-07-01\u0026amp;#x27;, \u0026amp;#x27;2011-08-01\u0026amp;#x27;, \u0026amp;#x27;2011-09-01\u0026amp;#x27;, \u0026amp;#x27;2011-10-01\u0026amp;#x27;, \u0026amp;#x27;2011-11-01\u0026amp;#x27;, \u0026amp;#x27;2011-12-01\u0026amp;#x27;, \u0026amp;#x27;2012-01-01\u0026amp;#x27;, \u0026amp;#x27;2012-02-01\u0026amp;#x27;, \u0026amp;#x27;2012-03-01\u0026amp;#x27;, \u0026amp;#x27;2012-04-01\u0026amp;#x27;, \u0026amp;#x27;2012-05-01\u0026amp;#x27;, \u0026amp;#x27;2012-06-01\u0026amp;#x27;, \u0026amp;#x27;2012-07-01\u0026amp;#x27;, \u0026amp;#x27;2012-08-01\u0026amp;#x27;, \u0026amp;#x27;2012-09-01\u0026amp;#x27;, \u0026amp;#x27;2012-10-01\u0026amp;#x27;, \u0026amp;#x27;2012-11-01\u0026amp;#x27;, \u0026amp;#x27;2012-12-01\u0026amp;#x27;, \u0026amp;#x27;2013-01-01\u0026amp;#x27;, \u0026amp;#x27;2013-02-01\u0026amp;#x27;, \u0026amp;#x27;2013-03-01\u0026amp;#x27;, \u0026amp;#x27;2013-04-01\u0026amp;#x27;, \u0026amp;#x27;2013-05-01\u0026amp;#x27;, \u0026amp;#x27;2013-06-01\u0026amp;#x27;, \u0026amp;#x27;2013-07-01\u0026amp;#x27;, \u0026amp;#x27;2013-08-01\u0026amp;#x27;, \u0026amp;#x27;2013-09-01\u0026amp;#x27;, \u0026amp;#x27;2013-10-01\u0026amp;#x27;, \u0026amp;#x27;2013-11-01\u0026amp;#x27;, \u0026amp;#x27;2013-12-01\u0026amp;#x27;], dtype=\u0026amp;#x27;datetime64[ns]\u0026amp;#x27;, name=\u0026amp;#x27;t_dim\u0026amp;#x27;, freq=None))\u0026lt;/pre\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li class='xr-section-item'\u0026gt;\u0026lt;input id='section-ef51b210-dd89-4548-b169-0da7d1d04474' class='xr-section-summary-in' type='checkbox' disabled \u0026gt;\u0026lt;label for='section-ef51b210-dd89-4548-b169-0da7d1d04474' class='xr-section-summary' title='Expand/collapse section'\u0026gt;Attributes: \u0026lt;span\u0026gt;(0)\u0026lt;/span\u0026gt;\u0026lt;/label\u0026gt;\u0026lt;div class='xr-section-inline-details'\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;div class='xr-section-details'\u0026gt;\u0026lt;dl class='xr-attrs'\u0026gt;\u0026lt;/dl\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/div\u0026gt;  Plot the decomposed time series\ncomponent = xr.DataArray( [\u0026#34;trend\u0026#34;,\u0026#34;seasonal\u0026#34;,\u0026#34;residual\u0026#34;], dims=\u0026#34;component\u0026#34;, name=\u0026#34;component\u0026#34; ) temp_decomp = xr.concat( [grd.dataset.trend, grd.dataset.seasonal,grd.dataset.residual], dim=component ) temp_decomp.name = \u0026#34;temperature\u0026#34; temp_decomp[:,:,0,200,200].plot(hue=\u0026#34;component\u0026#34;) [\u0026lt;matplotlib.lines.Line2D at 0x7f567caaddc0\u0026gt;, \u0026lt;matplotlib.lines.Line2D at 0x7f567ca84340\u0026gt;, \u0026lt;matplotlib.lines.Line2D at 0x7f567ca3a040\u0026gt;]  ","excerpt":"Overview A function within the Process_data class that will decompose time series into trend, …","ref":"/COAsT/docs/examples/notebooks/general/seasonal_decomp_example/","title":"Seasonal decomp example"},{"body":"","excerpt":"","ref":"/COAsT/docs/examples/notebooks/tidegauge/","title":"Tidegauge"},{"body":"This tutorial gives an overview of some of validation tools available when using the Tidegauge objects in COAsT.\nThis includes:\n creating tidegauge objects  reading in tidegauge data creating tidegauge object from gridded simulation data   basic plotting  on maps and timeseries   analysis  harmonic analysis and calculation of non-tidal residual doodsonX0 tidal filtering threshold statistics error calculation: mean errors, mean absolute error (MAE), mean square error (MSE)    Import necessary libraries import xarray as xr import numpy as np import coast import datetime import matplotlib.pyplot as plt Define paths fn_dom = \u0026#34;\u0026lt;PATH_TO_NEMO_DOMAIN\u0026gt;\u0026#34; fn_dat = \u0026#34;\u0026lt;PATH_TO_NEMO_DATA\u0026gt;\u0026#34; fn_config = \u0026#34;\u0026lt;PATH_TO_CONFIG.json\u0026gt;\u0026#34; fn_tg = \u0026#34;\u0026lt;PATH_TO_TIDEGAUGE_NETCDF\u0026gt;\u0026#34; # This should already be processed, on the same time dimension # Change this to 0 to not use default files. if 1: #print(f\u0026#34;Use default files\u0026#34;) dir = \u0026#34;./example_files/\u0026#34; fn_dom = dir + \u0026#34;coast_example_nemo_domain.nc\u0026#34; fn_dat = dir + \u0026#34;coast_example_nemo_data.nc\u0026#34; fn_config = \u0026#34;./config/example_nemo_grid_t.json\u0026#34; fn_tidegauge = dir + \u0026#34;tide_gauges/lowestoft-p024-uk-bodc\u0026#34; fn_tg = dir + \u0026#34;tide_gauges/coast_example_tidegauges.nc\u0026#34; # These are a collection (xr.DataSet) of tidegauge observations. Created for this demonstration, they are synthetic. Reading data We can create our empty tidegauge object:\ntidegauge = coast.Tidegauge() Tidegauge object at 0x561eabdbafc0 initialised  The Tidegauge class contains multiple methods for reading different typical tidegauge formats. This includes reading from the GESLA and BODC databases. To read a gesla file between two dates, we can use:\ndate0 = datetime.datetime(2007,1,10) date1 = datetime.datetime(2007,1,12) tidegauge.read_gesla_v3(fn_tidegauge, date_start = date0, date_end = date1) A Tidegauge object is a type of Timeseries object, so it has the form:\ntidegauge.dataset             /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { \u0026ndash;xr-font-color0: var(\u0026ndash;jp-content-font-color0, rgba(0, 0, 0, 1)); \u0026ndash;xr-font-color2: var(\u0026ndash;jp-content-font-color2, rgba(0, 0, 0, 0.54)); \u0026ndash;xr-font-color3: var(\u0026ndash;jp-content-font-color3, rgba(0, 0, 0, 0.38)); \u0026ndash;xr-border-color: var(\u0026ndash;jp-border-color2, #e0e0e0); \u0026ndash;xr-disabled-color: var(\u0026ndash;jp-layout-color3, #bdbdbd); \u0026ndash;xr-background-color: var(\u0026ndash;jp-layout-color0, white); \u0026ndash;xr-background-color-row-even: var(\u0026ndash;jp-layout-color1, white); \u0026ndash;xr-background-color-row-odd: var(\u0026ndash;jp-layout-color2, #eeeeee); }\nhtml[theme=dark], body[data-theme=dark], body.vscode-dark { \u0026ndash;xr-font-color0: rgba(255, 255, 255, 1); \u0026ndash;xr-font-color2: rgba(255, 255, 255, 0.54); \u0026ndash;xr-font-color3: rgba(255, 255, 255, 0.38); \u0026ndash;xr-border-color: #1F1F1F; \u0026ndash;xr-disabled-color: #515151; \u0026ndash;xr-background-color: #111111; \u0026ndash;xr-background-color-row-even: #111111; \u0026ndash;xr-background-color-row-odd: #313131; }\n.xr-wrap { display: block !important; min-width: 300px; max-width: 700px; }\n.xr-text-repr-fallback { /* fallback to plain text repr when CSS is not injected (untrusted notebook) */ display: none; }\n.xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(\u0026ndash;xr-border-color); }\n.xr-header \u0026gt; div, .xr-header \u0026gt; ul { display: inline; margin-top: 0; margin-bottom: 0; }\n.xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; }\n.xr-obj-type { color: var(\u0026ndash;xr-font-color2); }\n.xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; }\n.xr-section-item { display: contents; }\n.xr-section-item input { display: none; }\n.xr-section-item input + label { color: var(\u0026ndash;xr-disabled-color); }\n.xr-section-item input:enabled + label { cursor: pointer; color: var(\u0026ndash;xr-font-color2); }\n.xr-section-item input:enabled + label:hover { color: var(\u0026ndash;xr-font-color0); }\n.xr-section-summary { grid-column: 1; color: var(\u0026ndash;xr-font-color2); font-weight: 500; }\n.xr-section-summary \u0026gt; span { display: inline-block; padding-left: 0.5em; }\n.xr-section-summary-in:disabled + label { color: var(\u0026ndash;xr-font-color2); }\n.xr-section-summary-in + label:before { display: inline-block; content: \u0026lsquo;►\u0026rsquo;; font-size: 11px; width: 15px; text-align: center; }\n.xr-section-summary-in:disabled + label:before { color: var(\u0026ndash;xr-disabled-color); }\n.xr-section-summary-in:checked + label:before { content: \u0026lsquo;▼\u0026rsquo;; }\n.xr-section-summary-in:checked + label \u0026gt; span { display: none; }\n.xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; }\n.xr-section-inline-details { grid-column: 2 / -1; }\n.xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; }\n.xr-section-summary-in:checked ~ .xr-section-details { display: contents; }\n.xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; }\n.xr-array-wrap \u0026gt; label { grid-column: 1; vertical-align: top; }\n.xr-preview { color: var(\u0026ndash;xr-font-color3); }\n.xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; }\n.xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; }\n.xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; }\n.xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; }\n.xr-dim-list li { display: inline-block; padding: 0; margin: 0; }\n.xr-dim-list:before { content: \u0026lsquo;('; }\n.xr-dim-list:after { content: \u0026lsquo;)'; }\n.xr-dim-list li:not(:last-child):after { content: \u0026lsquo;,'; padding-right: 5px; }\n.xr-has-index { font-weight: bold; }\n.xr-var-list, .xr-var-item { display: contents; }\n.xr-var-item \u0026gt; div, .xr-var-item label, .xr-var-item \u0026gt; .xr-var-name span { background-color: var(\u0026ndash;xr-background-color-row-even); margin-bottom: 0; }\n.xr-var-item \u0026gt; .xr-var-name:hover span { padding-right: 5px; }\n.xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; div, .xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; label, .xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; .xr-var-name span { background-color: var(\u0026ndash;xr-background-color-row-odd); }\n.xr-var-name { grid-column: 1; }\n.xr-var-dims { grid-column: 2; }\n.xr-var-dtype { grid-column: 3; text-align: right; color: var(\u0026ndash;xr-font-color2); }\n.xr-var-preview { grid-column: 4; }\n.xr-index-preview { grid-column: 2 / 5; color: var(\u0026ndash;xr-font-color2); }\n.xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; }\n.xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; }\n.xr-var-attrs, .xr-var-data, .xr-index-data { display: none; background-color: var(\u0026ndash;xr-background-color) !important; padding-bottom: 5px !important; }\n.xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data, .xr-index-data-in:checked ~ .xr-index-data { display: block; }\n.xr-var-data \u0026gt; table { float: right; }\n.xr-var-name span, .xr-var-data, .xr-index-name div, .xr-index-data, .xr-attrs { padding-left: 25px !important; }\n.xr-attrs, .xr-var-attrs, .xr-var-data, .xr-index-data { grid-column: 1 / -1; }\ndl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; }\n.xr-attrs dt, .xr-attrs dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; }\n.xr-attrs dt { font-weight: normal; grid-column: 1; }\n.xr-attrs dt:hover span { display: inline-block; background: var(\u0026ndash;xr-background-color); padding-right: 10px; }\n.xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; }\n.xr-icon-database, .xr-icon-file-text2, .xr-no-icon { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } \u0026lt;xarray.Dataset\u0026gt; Dimensions: (id_dim: 1, t_dim: 193) Coordinates: time (t_dim) datetime64[ns] 2007-01-10 \u0026hellip; 2007-01-12 longitude (id_dim) float64 1.751 latitude (id_dim) float64 52.47 id_name (id_dim) \u0026lt;U9 'Lowestoft' Dimensions without coordinates: id_dim, t_dim Data variables: ssh (id_dim, t_dim) float64 2.818 2.823 2.871 \u0026hellip; 3.214 3.257 3.371 qc_flags (id_dim, t_dim) int64 1 1 1 1 1 1 1 1 1 1 \u0026hellip; 1 1 1 1 1 1 1 1 1 1xarray.DatasetDimensions:id_dim: 1t_dim: 193Coordinates: (4)time(t_dim)datetime64[ns]2007-01-10 \u0026hellip; 2007-01-12array(['2007-01-10T00:00:00.000000000', '2007-01-10T00:15:00.000000000', '2007-01-10T00:30:00.000000000', '2007-01-10T00:45:00.000000000', '2007-01-10T01:00:00.000000000', '2007-01-10T01:15:00.000000000', '2007-01-10T01:30:00.000000000', '2007-01-10T01:45:00.000000000', '2007-01-10T02:00:00.000000000', '2007-01-10T02:15:00.000000000', '2007-01-10T02:30:00.000000000', '2007-01-10T02:45:00.000000000', '2007-01-10T03:00:00.000000000', '2007-01-10T03:15:00.000000000', '2007-01-10T03:30:00.000000000', '2007-01-10T03:45:00.000000000', '2007-01-10T04:00:00.000000000', '2007-01-10T04:15:00.000000000', '2007-01-10T04:30:00.000000000', '2007-01-10T04:45:00.000000000', '2007-01-10T05:00:00.000000000', '2007-01-10T05:15:00.000000000', '2007-01-10T05:30:00.000000000', '2007-01-10T05:45:00.000000000', '2007-01-10T06:00:00.000000000', '2007-01-10T06:15:00.000000000', '2007-01-10T06:30:00.000000000', '2007-01-10T06:45:00.000000000', '2007-01-10T07:00:00.000000000', '2007-01-10T07:15:00.000000000', '2007-01-10T07:30:00.000000000', '2007-01-10T07:45:00.000000000', '2007-01-10T08:00:00.000000000', '2007-01-10T08:15:00.000000000', '2007-01-10T08:30:00.000000000', '2007-01-10T08:45:00.000000000', '2007-01-10T09:00:00.000000000', '2007-01-10T09:15:00.000000000', '2007-01-10T09:30:00.000000000', '2007-01-10T09:45:00.000000000', \u0026hellip; '2007-01-11T14:30:00.000000000', '2007-01-11T14:45:00.000000000', '2007-01-11T15:00:00.000000000', '2007-01-11T15:15:00.000000000', '2007-01-11T15:30:00.000000000', '2007-01-11T15:45:00.000000000', '2007-01-11T16:00:00.000000000', '2007-01-11T16:15:00.000000000', '2007-01-11T16:30:00.000000000', '2007-01-11T16:45:00.000000000', '2007-01-11T17:00:00.000000000', '2007-01-11T17:15:00.000000000', '2007-01-11T17:30:00.000000000', '2007-01-11T17:45:00.000000000', '2007-01-11T18:00:00.000000000', '2007-01-11T18:15:00.000000000', '2007-01-11T18:30:00.000000000', '2007-01-11T18:45:00.000000000', '2007-01-11T19:00:00.000000000', '2007-01-11T19:15:00.000000000', '2007-01-11T19:30:00.000000000', '2007-01-11T19:45:00.000000000', '2007-01-11T20:00:00.000000000', '2007-01-11T20:15:00.000000000', '2007-01-11T20:30:00.000000000', '2007-01-11T20:45:00.000000000', '2007-01-11T21:00:00.000000000', '2007-01-11T21:15:00.000000000', '2007-01-11T21:30:00.000000000', '2007-01-11T21:45:00.000000000', '2007-01-11T22:00:00.000000000', '2007-01-11T22:15:00.000000000', '2007-01-11T22:30:00.000000000', '2007-01-11T22:45:00.000000000', '2007-01-11T23:00:00.000000000', '2007-01-11T23:15:00.000000000', '2007-01-11T23:30:00.000000000', '2007-01-11T23:45:00.000000000', '2007-01-12T00:00:00.000000000'], dtype='datetime64[ns]')longitude(id_dim)float641.751array([1.75083])latitude(id_dim)float6452.47array([52.473])id_name(id_dim)\u0026lt;U9'Lowestoft'array(['Lowestoft'], dtype='\u0026lt;U9')Data variables: (2)ssh(id_dim, t_dim)float642.818 2.823 2.871 \u0026hellip; 3.257 3.371array([[ 2.818, 2.823, 2.871, 2.931, 2.961, 2.979, 2.953, 2.913, 2.864, 2.806, 2.723, 2.664, 2.606, 2.511, 2.43 , 2.379, 2.296, 2.201, 2.105, 2.006, 1.908, 1.801, 1.684, 1.579, 1.494, 1.402, 1.306, 1.233, 1.171, 1.102, 1.054, 1.028, 0.989, 0.97 , 0.983, 1.004, 1.026, 1.068, 1.153, 1.225, 1.296, 1.362, 1.436, 1.481, 1.536, 1.615, 1.695, 1.726, 1.802, 1.842, 1.86 , 1.872, 1.897, 1.912, 1.946, 1.994, 2.006, 2.028, 2.067, 2.081, 2.098, 2.137, 2.113, 2.068, 2.053, 1.985, 1.917, 1.869, 1.803, 1.695, 1.642, 1.545, 1.463, 1.463, 1.466, 1.462, 1.476, 1.524, 1.574, 1.633, 1.661, 1.717, 1.818, 1.918, 2.018, 2.093, 2.14 , 2.223, 2.278, 2.303, 2.372, 2.375, 2.395, 2.468, 2.481, 2.487, 2.535, 2.543, 2.578, 2.621, 2.627, 2.626, 2.585, 2.563, 2.529, 2.451, 2.335, 2.207, 2.086, 1.982, 1.855, 1.741, 1.618, 1.531, 1.429, 1.342, 1.246, 1.141, 1.031, 0.902, 0.784, 0.673, 0.571, 0.457, 0.323, 0.203, 0.13 , 0.056, -0.028, -0.077, -0.093, -0.143, -0.181, -0.211, -0.217, -0.182, -0.1 , -0.046, 0.02 , 0.121, 0.247, 0.358, 0.468, 0.65 , 0.845, 0.987, 1.059, 1.199, 1.322, 1.38 , 1.465, 1.519, 1.559, 1.691, 1.775, 1.844, 2.019, 2.113, 2.159, 2.266, 2.311, 2.406, 2.512, 2.533, 2.43 , 2.309, 2.185, 2.136, 2.086, 2.066, 2.114, 2.114, 2.051, 2.033, 2.055, 2.1 , 2.192, 2.278, 2.334, 2.421, 2.497, 2.548, 2.603, 2.679, 2.803, 2.859, 2.875, 3.001, 3.075, 3.135, 3.214, 3.257, 3.371]])qc_flags(id_dim, t_dim)int641 1 1 1 1 1 1 1 \u0026hellip; 1 1 1 1 1 1 1 1array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])Indexes: (0)Attributes: (0)\nAn example data variable could be ssh, or ntr (non-tidal residual). This object can also be used for other instrument types, not just tide gauges. For example moorings.\nEvery id index for this object should use the same time coordinates. Therefore, timeseries need to be aligned before being placed into the object. If there is any padding needed, then NaNs should be used. NaNs should also be used for quality control/data rejection.\nFor the rest of our examples, we will use data from multiple tide gauges on the same time dimension. It will be read in from a simple netCDF file:\ntt = xr.open_dataset(fn_tg) obs = coast.Tidegauge(dataset=tt) Tidegauge object at 0x561eabdbafc0 initialised  # Create the object and then inset the netcdf dataset tt = xr.open_dataset(fn_tg) obs = coast.Tidegauge(dataset=tt) obs.dataset = obs.dataset.set_coords(\u0026#34;time\u0026#34;) Tidegauge object at 0x561eabdbafc0 initialised  Quick plotting to visualise Tidegauge data Tidegauge has ready made quick plotting routines for viewing time series and tide gauge location. To look at the tide gauge location:\nfig, ax = obs.plot_on_map() /usr/share/miniconda/envs/coast/lib/python3.8/site-packages/cartopy/io/__init__.py:241: DownloadWarning: Downloading: https://naturalearth.s3.amazonaws.com/50m_physical/ne_50m_coastline.zip warnings.warn(f'Downloading: {url}', DownloadWarning)  There is also a slightly expanded version where you can plot multiple tidegauge objects, included as a list, and also the colour (if it is a dataarray with one value per location).\n# plot a list tidegauge datasets (here repeating obs for the point of demonstration) and colour fig, ax = obs.plot_on_map_multiple([obs,obs], color_var_str=\u0026#34;latitude\u0026#34;) Time series can be plotted using matplotlib.pyplot methods. However xarray has its own plotting wrappers that can be used, which offers some conveniences with labelling\nstn_id=26 # pick a gauge station plt.subplot(2,1,1) obs.dataset.ssh[stn_id].plot() # rename time dimension to enable automatic x-axis labelling plt.subplot(2,1,2) obs.dataset.ssh[stn_id].swap_dims({\u0026#39;t_dim\u0026#39;:\u0026#39;time\u0026#39;}).plot() # rename time dimension to enable automatic x-axis labelling plt.tight_layout() plt.show() Or you can use the Tidegauge.plot_timeseries() method, in which start and end dates can also be specified.\nobs.isel(id_dim=stn_id).plot_timeseries(date_start=np.datetime64(\u0026#39;2007-01-01\u0026#39;), date_end = np.datetime64(\u0026#39;2007-01-31\u0026#39;) ) (\u0026lt;Figure size 1000x1000 with 1 Axes\u0026gt;, \u0026lt;matplotlib.collections.PathCollection at 0x7fcae0534c70\u0026gt;)  Basic manipulation: subsetting + plotting We can do some simple spatial and temporal manipulations of this data:\n# Cut out a time window obs_cut = obs.time_slice( date0 = datetime.datetime(2007, 1, 1), date1 = datetime.datetime(2007,1,31)) # Cut out geographical boxes obs_upper = obs_cut.subset_indices_lonlat_box(lonbounds = [0, 3], latbounds = [50, 55]) obs_lower = obs_cut.subset_indices_lonlat_box(lonbounds = [-9, -3], latbounds = [55.5, 59]) #fig, ax = obs_cut.plot_on_map() fig, ax = obs_upper.plot_on_map_multiple([obs_upper, obs_lower], color_var_str=\u0026#34;latitude\u0026#34;) Tidegauge object at 0x561eabdbafc0 initialised Tidegauge object at 0x561eabdbafc0 initialised Tidegauge object at 0x561eabdbafc0 initialised  Gridded model comparison To compare our observations to the model, we will interpolate a model variable to the same time and geographical space as the tidegauge. This is done using the obs_operator() method.\nBut first load some gridded data and manipulate some variable names for convenience\nnemo = coast.Gridded(fn_dat, fn_dom, multiple=True, config=fn_config) # Rename depth_0 to be depth nemo.dataset = nemo.dataset.rename({\u0026#34;depth_0\u0026#34;: \u0026#34;depth\u0026#34;}) #nemo.dataset = nemo.dataset[[\u0026#34;ssh\u0026#34;, \u0026#34;landmask\u0026#34;]] interpolate model onto obs locations\ntidegauge_from_model = obs.obs_operator(nemo, time_interp='nearest') Doing this would create a new interpolated tidegauge called tidegauge_from_model Take a look at tidegauge_from_model.dataset to see for yourself. If a landmask variable is present in the Gridded dataset then the nearest wet points will be taken. Otherwise, just the nearest point is taken. If landmask is required but not present you will need to insert it into the dataset yourself. For nemo data, you could use the bottom_level or mbathy variables to do this. E.g:\n# Create a landmask array and put it into the nemo object. # Here, using the bottom_level == 0 variable from the domain file is enough. nemo.dataset[\u0026#34;landmask\u0026#34;] = nemo.dataset.bottom_level == 0 # Then do the interpolation tidegauge_from_model = obs.obs_operator(nemo, time_interp=\u0026#39;nearest\u0026#39;) Calculating spatial indices. Calculating time indices. Indexing model data at tide gauge locations.. Calculating interpolation distances. Interpolating in time... Tidegauge object at 0x561eabdbafc0 initialised  However, the new tidegauge_from_model will the same number of time entries as the obs data, rather than the model data (so this will include lots of empty values). So, for a more useful demonstration we trim the observed gauge data so it better matches the model data.\n# Cut down data to be only in 2007 to match model data. start_date = datetime.datetime(2007, 1, 1) end_date = datetime.datetime(2007, 1, 31) obs = obs.time_slice(start_date, end_date) Tidegauge object at 0x561eabdbafc0 initialised  Interpolate model data onto obs locations model_timeseries = obs.obs_operator(nemo) # Take a look model_timeseries.dataset Calculating spatial indices. Calculating time indices. Indexing model data at tide gauge locations.. Calculating interpolation distances. Interpolating in time... Tidegauge object at 0x561eabdbafc0 initialised              /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { \u0026ndash;xr-font-color0: var(\u0026ndash;jp-content-font-color0, rgba(0, 0, 0, 1)); \u0026ndash;xr-font-color2: var(\u0026ndash;jp-content-font-color2, rgba(0, 0, 0, 0.54)); \u0026ndash;xr-font-color3: var(\u0026ndash;jp-content-font-color3, rgba(0, 0, 0, 0.38)); \u0026ndash;xr-border-color: var(\u0026ndash;jp-border-color2, #e0e0e0); \u0026ndash;xr-disabled-color: var(\u0026ndash;jp-layout-color3, #bdbdbd); \u0026ndash;xr-background-color: var(\u0026ndash;jp-layout-color0, white); \u0026ndash;xr-background-color-row-even: var(\u0026ndash;jp-layout-color1, white); \u0026ndash;xr-background-color-row-odd: var(\u0026ndash;jp-layout-color2, #eeeeee); }\nhtml[theme=dark], body[data-theme=dark], body.vscode-dark { \u0026ndash;xr-font-color0: rgba(255, 255, 255, 1); \u0026ndash;xr-font-color2: rgba(255, 255, 255, 0.54); \u0026ndash;xr-font-color3: rgba(255, 255, 255, 0.38); \u0026ndash;xr-border-color: #1F1F1F; \u0026ndash;xr-disabled-color: #515151; \u0026ndash;xr-background-color: #111111; \u0026ndash;xr-background-color-row-even: #111111; \u0026ndash;xr-background-color-row-odd: #313131; }\n.xr-wrap { display: block !important; min-width: 300px; max-width: 700px; }\n.xr-text-repr-fallback { /* fallback to plain text repr when CSS is not injected (untrusted notebook) */ display: none; }\n.xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(\u0026ndash;xr-border-color); }\n.xr-header \u0026gt; div, .xr-header \u0026gt; ul { display: inline; margin-top: 0; margin-bottom: 0; }\n.xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; }\n.xr-obj-type { color: var(\u0026ndash;xr-font-color2); }\n.xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; }\n.xr-section-item { display: contents; }\n.xr-section-item input { display: none; }\n.xr-section-item input + label { color: var(\u0026ndash;xr-disabled-color); }\n.xr-section-item input:enabled + label { cursor: pointer; color: var(\u0026ndash;xr-font-color2); }\n.xr-section-item input:enabled + label:hover { color: var(\u0026ndash;xr-font-color0); }\n.xr-section-summary { grid-column: 1; color: var(\u0026ndash;xr-font-color2); font-weight: 500; }\n.xr-section-summary \u0026gt; span { display: inline-block; padding-left: 0.5em; }\n.xr-section-summary-in:disabled + label { color: var(\u0026ndash;xr-font-color2); }\n.xr-section-summary-in + label:before { display: inline-block; content: \u0026lsquo;►\u0026rsquo;; font-size: 11px; width: 15px; text-align: center; }\n.xr-section-summary-in:disabled + label:before { color: var(\u0026ndash;xr-disabled-color); }\n.xr-section-summary-in:checked + label:before { content: \u0026lsquo;▼\u0026rsquo;; }\n.xr-section-summary-in:checked + label \u0026gt; span { display: none; }\n.xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; }\n.xr-section-inline-details { grid-column: 2 / -1; }\n.xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; }\n.xr-section-summary-in:checked ~ .xr-section-details { display: contents; }\n.xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; }\n.xr-array-wrap \u0026gt; label { grid-column: 1; vertical-align: top; }\n.xr-preview { color: var(\u0026ndash;xr-font-color3); }\n.xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; }\n.xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; }\n.xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; }\n.xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; }\n.xr-dim-list li { display: inline-block; padding: 0; margin: 0; }\n.xr-dim-list:before { content: \u0026lsquo;('; }\n.xr-dim-list:after { content: \u0026lsquo;)'; }\n.xr-dim-list li:not(:last-child):after { content: \u0026lsquo;,'; padding-right: 5px; }\n.xr-has-index { font-weight: bold; }\n.xr-var-list, .xr-var-item { display: contents; }\n.xr-var-item \u0026gt; div, .xr-var-item label, .xr-var-item \u0026gt; .xr-var-name span { background-color: var(\u0026ndash;xr-background-color-row-even); margin-bottom: 0; }\n.xr-var-item \u0026gt; .xr-var-name:hover span { padding-right: 5px; }\n.xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; div, .xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; label, .xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; .xr-var-name span { background-color: var(\u0026ndash;xr-background-color-row-odd); }\n.xr-var-name { grid-column: 1; }\n.xr-var-dims { grid-column: 2; }\n.xr-var-dtype { grid-column: 3; text-align: right; color: var(\u0026ndash;xr-font-color2); }\n.xr-var-preview { grid-column: 4; }\n.xr-index-preview { grid-column: 2 / 5; color: var(\u0026ndash;xr-font-color2); }\n.xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; }\n.xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; }\n.xr-var-attrs, .xr-var-data, .xr-index-data { display: none; background-color: var(\u0026ndash;xr-background-color) !important; padding-bottom: 5px !important; }\n.xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data, .xr-index-data-in:checked ~ .xr-index-data { display: block; }\n.xr-var-data \u0026gt; table { float: right; }\n.xr-var-name span, .xr-var-data, .xr-index-name div, .xr-index-data, .xr-attrs { padding-left: 25px !important; }\n.xr-attrs, .xr-var-attrs, .xr-var-data, .xr-index-data { grid-column: 1 / -1; }\ndl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; }\n.xr-attrs dt, .xr-attrs dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; }\n.xr-attrs dt { font-weight: normal; grid-column: 1; }\n.xr-attrs dt:hover span { display: inline-block; background: var(\u0026ndash;xr-background-color); padding-right: 10px; }\n.xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; }\n.xr-icon-database, .xr-icon-file-text2, .xr-no-icon { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } \u0026lt;xarray.Dataset\u0026gt; Dimensions: (z_dim: 51, axis_nbounds: 2, t_dim: 720, id_dim: 61) Coordinates: longitude (id_dim) float32 1.444 -4.0 -5.333 \u0026hellip; 7.666 -9.111 8.777 latitude (id_dim) float32 51.93 51.53 58.0 51.67 \u0026hellip; 58.0 51.53 53.87 depth (z_dim, id_dim) float32 0.1001 0.2183 0.2529 \u0026hellip; 27.32 10.11\n  time (t_dim) datetime64[ns] 2007-01-01 \u0026hellip; 2007-01-30T23:00:00 Dimensions without coordinates: z_dim, axis_nbounds, t_dim, id_dim Data variables: deptht_bounds (z_dim, axis_nbounds) float32 0.0 6.157 \u0026hellip; 5.924e+03 ssh (t_dim, id_dim) float32 nan nan nan \u0026hellip; 0.3721 -0.0752 0.7412 temperature (t_dim, z_dim, id_dim) float32 nan nan nan \u0026hellip; nan nan nan bathymetry (id_dim) float32 10.0 21.81 6.075 15.56 \u0026hellip; 17.8 14.06 10.0 e1 (id_dim) float32 7.618e+03 7.686e+03 \u0026hellip; 7.686e+03 7.285e+03 e2 (id_dim) float32 7.414e+03 7.414e+03 \u0026hellip; 7.414e+03 7.414e+03 e3_0 (z_dim, id_dim) float32 0.2002 0.4365 0.5059 \u0026hellip; 0.541 0.2002 bottom_level (id_dim) float32 50.0 50.0 12.0 32.0 \u0026hellip; 44.0 17.0 26.0 50.0 landmask (id_dim) bool False False False False \u0026hellip; False False False interp_dist (id_dim) float64 10.56 4.33 15.65 6.018 \u0026hellip; 5.835 4.96 3.957 Attributes: name: AMM7_1d_20070101_20070131_25hourm_grid_T description: ocean T grid variables, 25h meaned title: ocean T grid variables, 25h meaned Conventions: CF-1.6 timeStamp: 2019-Dec-26 04:35:28 GMT uuid: 96cae459-d3a1-4f4f-b82b-9259179f95f7 history: Tue May 19 12:07:51 2020: ncks -v votemper,sossheig -d time\u0026hellip; NCO: 4.4.7xarray.DatasetDimensions:z_dim: 51axis_nbounds: 2t_dim: 720id_dim: 61Coordinates: (4)longitude(id_dim)float321.444 -4.0 -5.333 \u0026hellip; -9.111 8.777array([ 1.4443359 , -4. , -5.333008 , -5.111328 , -3. , -3.1113281 , -5.2226562 , -3. , -1.4443359 , 1.4443359 , -4.666992 , 1.3330078 , -6.2226562 , -2.555664 , -3.7783203 , -4.333008 , 1.1113281 , -0.55566406, -5.555664 , -5.111328 , -1.4443359 , -2. , -1.1113281 , -2.8886719 , -1.8886719 , -6.333008 , -3.1113281 , -6.2226562 , -0. , 0.11132812, -4. , 1.3330078 , -2.7783203 , -2.1113281 , -1.4443359 , -3.555664 , -1.4443359 , -2.7783203 , -3.2226562 , 1.7773438 , -5. , -5.555664 , -3.1113281 , -1.1113281 , -6.333008 , -5. , -4.7783203 , -6.666992 , -2.7783203 , -1.4443359 , -4.111328 , -4.111328 , -1.4443359 , 11.22168 , 11.777344 , 4.888672 , 11.22168 , -7.333008 , 7.6660156 , -9.111328 , 8.777344 ], dtype=float32)latitude(id_dim)float3251.93 51.53 58.0 \u0026hellip; 51.53 53.87array([51.933594, 51.53418 , 58.000977, 51.666992, 54.000977, 51.26758 , 58.467773, 58.467773, 55.067383, 51.933594, 53.333984, 52.933594, 56.66797 , 50.600586, 53.333984, 50.333984, 51.53418 , 54.467773, 50.067383, 54.80078 , 55.067383, 57.13379 , 60.20117 , 51.53418 , 50.666992, 49.933594, 53.467773, 55.600586, 53.600586, 50.7334 , 57.600586, 51.067383, 51.53418 , 49.13379 , 55.067383, 54.666992, 55.067383, 51.53418 , 56.000977, 52.467773, 52.067383, 54.666992, 51.26758 , 50.7334 , 58.13379 , 55.734375, 54.067383, 55.26758 , 51.53418 , 55.067383, 52.734375, 51.26758 , 55.067383, 58.333984, 57.66797 , 61.93457 , 64.868164, 55.40039 , 58.000977, 51.53418 , 53.867188], dtype=float32)depth(z_dim, id_dim)float320.1001 0.2183 \u0026hellip; 27.32 10.11units :mstandard_name :Depth at time zero on the t-gridarray([[1.00097656e-01, 2.18261719e-01, 2.52929688e-01, \u0026hellip;, 5.02441406e-01, 2.70507812e-01, 1.00097656e-01], [3.00292969e-01, 6.54785156e-01, 7.58789062e-01, \u0026hellip;, 1.49365234e+00, 8.11523438e-01, 3.00292969e-01], [5.00488281e-01, 1.09130859e+00, 1.26464844e+00, \u0026hellip;, 2.46826172e+00, 1.35253906e+00, 5.00488281e-01], \u0026hellip;, [9.70947266e+00, 2.11713867e+01, 2.45341797e+01, \u0026hellip;, 1.70940918e+02, 2.62392578e+01, 9.70947266e+00], [9.90966797e+00, 2.16079102e+01, 2.50400391e+01, \u0026hellip;, 1.75505371e+02, 2.67802734e+01, 9.90966797e+00], [1.01098633e+01, 2.20444336e+01, 2.55458984e+01, \u0026hellip;, 1.79002441e+02, 2.73212891e+01, 1.01098633e+01]], dtype=float32)time(t_dim)datetime64[ns]2007-01-01 \u0026hellip; 2007-01-30T23:00:00axis :Tstandard_name :timelong_name :Time axistime_origin :1900-01-01 00:00:00bounds :time_counter_boundsarray(['2007-01-01T00:00:00.000000000', '2007-01-01T01:00:00.000000000', '2007-01-01T02:00:00.000000000', \u0026hellip;, '2007-01-30T21:00:00.000000000', '2007-01-30T22:00:00.000000000', '2007-01-30T23:00:00.000000000'], dtype='datetime64[ns]')Data variables: (10)deptht_bounds(z_dim, axis_nbounds)float320.0 6.157 \u0026hellip; 5.72e+03 5.924e+03array([[ 0. , 6.1572266], [ 6.1572266, 12.678711 ], [ 12.678711 , 19.65332 ], [ 19.65332 , 27.19043 ], [ 27.19043 , 35.426758 ], [ 35.426758 , 44.527344 ], [ 44.527344 , 54.69922 ], [ 54.69922 , 66.19141 ], [ 66.19141 , 79.305664 ], [ 79.305664 , 94.41016 ], [ 94.41016 , 111.94238 ], [ 111.94238 , 132.41895 ], [ 132.41895 , 156.44531 ], [ 156.44531 , 184.71582 ], [ 184.71582 , 218.01562 ], [ 218.01562 , 257.20605 ], [ 257.20605 , 303.20508 ], [ 303.20508 , 356.95898 ], [ 356.95898 , 419.39258 ], [ 419.39258 , 491.35645 ], \u0026hellip; [1955.7686 , 2136.3613 ], [2136.3613 , 2321.292 ], [2321.292 , 2509.8564 ], [2509.8564 , 2701.4355 ], [2701.4355 , 2895.504 ], [2895.504 , 3091.6123 ], [3091.6123 , 3289.3867 ], [3289.3867 , 3488.5156 ], [3488.5156 , 3688.7441 ], [3688.7441 , 3889.8613 ], [3889.8613 , 4091.6963 ], [4091.6963 , 4294.1094 ], [4294.1094 , 4496.9893 ], [4496.9893 , 4700.242 ], [4700.242 , 4903.797 ], [4903.797 , 5107.5938 ], [5107.5938 , 5311.584 ], [5311.584 , 5515.7295 ], [5515.7295 , 5720. ], [5720. , 5924.2705 ]], dtype=float32)ssh(t_dim, id_dim)float32nan nan nan \u0026hellip; -0.0752 0.7412units :monline_operation :instantinterval_operation :300 sinterval_write :1 dcell_methods :time: point (interval: 300 s)array([[ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan], \u0026hellip;, [ 0.24902344, -0.046875 , 0.20019531, \u0026hellip;, 0.3720703 , -0.07519531, 0.74121094], [ 0.24902344, -0.046875 , 0.20019531, \u0026hellip;, 0.3720703 , -0.07519531, 0.74121094], [ 0.24902344, -0.046875 , 0.20019531, \u0026hellip;, 0.3720703 , -0.07519531, 0.74121094]], dtype=float32)temperature(t_dim, z_dim, id_dim)float32nan nan nan nan \u0026hellip; nan nan nan nanunits :degConline_operation :instantinterval_operation :300 sinterval_write :1 dcell_methods :time: point (interval: 300 s)array([[[ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan], \u0026hellip;, [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan]],\n[[ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan], \u0026hellip; [7.4384766, 8.22168 , nan, \u0026hellip;, nan, nan, 5.0009766], [7.439453 , 8.22168 , nan, \u0026hellip;, nan, nan, 5.0009766], [ nan, nan, nan, \u0026hellip;, nan, nan, nan]],\n[[7.4335938, 8.217773 , 7.245117 , \u0026hellip;, 5.8652344, 8.924805 , 5.0322266], [7.4345703, 8.219727 , 7.245117 , \u0026hellip;, 5.8671875, 8.926758 , 5.0322266], [7.4345703, 8.219727 , 7.2421875, \u0026hellip;, 5.8691406, 8.926758 , 5.0322266], \u0026hellip;, [7.4384766, 8.22168 , nan, \u0026hellip;, nan, nan, 5.0009766], [7.439453 , 8.22168 , nan, \u0026hellip;, nan, nan, 5.0009766], [ nan, nan, nan, \u0026hellip;, nan, nan, nan]]], dtype=float32)bathymetry(id_dim)float3210.0 21.81 6.075 \u0026hellip; 14.06 10.0units :mstandard_name :bathymetrydescription :depth of last wet w-level on the horizontal t-gridarray([10. , 21.80957 , 6.0751953, 15.558594 , 10.65625 , 12.145508 , 53.125 , 47.216797 , 10.3125 , 10. , 21.898438 , 10.095703 , 11.674805 , 15.611328 , 11.230469 , 20.368164 , 10.25293 , 14.34082 , 23.28418 , 37.7666 , 10.3125 , 34.78711 , 15.394531 , 10. , 12.630859 , 21.493164 , 10. , 18.167969 , 10. , 17.354492 , 10. , 24.625 , 10. , 9.915039 , 10.3125 , 10. , 10.3125 , 10. , 10. , 10.0703125, 37.003906 , 10.40918 , 12.145508 , 10. , 30.885742 , 35.280273 , 17.089844 , 33.78711 , 10. , 10.3125 , 10. , 37.25879 , 10.3125 , 12.712891 , 8.652344 , 14.458984 , 83.96973 , 19.424805 , 17.804688 , 14.060547 , 10. ], dtype=float32)e1(id_dim)float327.618e+03 7.686e+03 \u0026hellip; 7.285e+03array([7617.912 , 7685.6387, 6547.1533, 7663.1045, 7262.1484, 7730.582 , 6461.591 , 6461.591 , 7074.802 , 7617.912 , 7377.966 , 7446.9775, 6789.205 , 7842.205 , 7377.966 , 7886.5586, 7685.6387, 7180.49 , 7930.74 , 7121.871 , 7074.802 , 6704.8984, 6140.08 , 7685.6387, 7831.091 , 7952.7676, 7354.881 , 6980.205 , 7331.758 , 7819.965 , 6620.1465, 7764.1797, 7685.6387, 8084.0186, 7074.802 , 7145.3477, 7074.802 , 7685.6387, 6908.8594, 7527.032 , 7595.254 , 7145.3477, 7730.582 , 7819.965 , 6522.751 , 6956.461 , 7250.5117, 7039.3994, 7685.6387, 7074.802 , 7481.3477, 7730.582 , 7074.802 , 6486.082 , 6608.004 , 5812.949 , 5247.382 , 7015.75 , 6547.1533, 7685.6387, 7285.3906], dtype=float32)e2(id_dim)float327.414e+03 7.414e+03 \u0026hellip; 7.414e+03array([7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633, 7413.633], dtype=float32)e3_0(z_dim, id_dim)float320.2002 0.4365 \u0026hellip; 0.541 0.2002array([[0.20019531, 0.43652344, 0.5058594 , \u0026hellip;, 1. , 0.5410156 , 0.20019531], [0.20019531, 0.43652344, 0.5058594 , \u0026hellip;, 0.9824219 , 0.5410156 , 0.20019531], [0.20019531, 0.43652344, 0.5058594 , \u0026hellip;, 0.96777344, 0.5410156 , 0.20019531], \u0026hellip;, [0.20019531, 0.43652344, 0.5058594 , \u0026hellip;, 5.0214844 , 0.5410156 , 0.20019531], [0.20019531, 0.43652344, 0.5058594 , \u0026hellip;, 4.057617 , 0.5410156 , 0.20019531], [0.20019531, 0.43652344, 0.5058594 , \u0026hellip;, 3.203125 , 0.5410156 , 0.20019531]], dtype=float32)bottom_level(id_dim)float3250.0 50.0 12.0 \u0026hellip; 17.0 26.0 50.0array([50., 50., 12., 32., 50., 50., 50., 50., 21., 50., 29., 42., 32., 42., 40., 33., 50., 25., 37., 32., 21., 50., 20., 50., 44., 24., 50., 24., 50., 43., 50., 50., 50., 34., 21., 50., 21., 50., 50., 33., 50., 11., 50., 50., 33., 45., 29., 39., 50., 21., 50., 50., 21., 19., 25., 25., 50., 44., 17., 26., 50.], dtype=float32)landmask(id_dim)boolFalse False False \u0026hellip; False Falsearray([False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False])interp_dist(id_dim)float6410.56 4.33 15.65 \u0026hellip; 4.96 3.957array([10.55753639, 4.3298596 , 15.65224834, 6.01847521, 6.21944535, 6.03485965, 10.09762811, 5.83889348, 6.6716266 , 7.06250586, 3.81132755, 2.10403027, 10.89520817, 7.65307627, 3.12483581, 11.15992752, 27.30753169, 4.52090493, 4.06485834, 4.67908778, 6.6716266 , 4.96772739, 5.4813996 , 7.053344 , 5.35290231, 2.05865294, 6.49778121, 3.61860367, 12.72904061, 6.59773619, 0.24707792, 5.27676859, 5.09090741, 5.52241217, 6.6716266 , 1.95744877, 6.6716266 , 5.13046117, 2.83309362, 1.88771332, 6.06839329, 7.32299892, 6.03485965, 7.69045765, 8.78690904, 6.13400869, 2.11138548, 6.79115084, 5.13046117, 6.6716266 , 4.76955022, 6.29453137, 6.6716266 , 1.82312538, 2.14643925, 11.94656233, 1.34373275, 3.71293547, 5.83537153, 4.96020927, 3.9566329 ])Indexes: (1)timePandasIndexPandasIndex(DatetimeIndex(['2007-01-01 00:00:00', '2007-01-01 01:00:00', '2007-01-01 02:00:00', '2007-01-01 03:00:00', '2007-01-01 04:00:00', '2007-01-01 05:00:00', '2007-01-01 06:00:00', '2007-01-01 07:00:00', '2007-01-01 08:00:00', '2007-01-01 09:00:00', \u0026hellip; '2007-01-30 14:00:00', '2007-01-30 15:00:00', '2007-01-30 16:00:00', '2007-01-30 17:00:00', '2007-01-30 18:00:00', '2007-01-30 19:00:00', '2007-01-30 20:00:00', '2007-01-30 21:00:00', '2007-01-30 22:00:00', '2007-01-30 23:00:00'], dtype='datetime64[ns]', name='time', length=720, freq=None))Attributes: (8)name :AMM7_1d_20070101_20070131_25hourm_grid_Tdescription :ocean T grid variables, 25h meanedtitle :ocean T grid variables, 25h meanedConventions :CF-1.6timeStamp :2019-Dec-26 04:35:28 GMTuuid :96cae459-d3a1-4f4f-b82b-9259179f95f7history :Tue May 19 12:07:51 2020: ncks -v votemper,sossheig -d time_counter,0,30,5 AMM7_1d_20070101_20070131_25hourm_grid_T.nc example_data.ncNCO :4.4.7\n  stn_id=26 # pick a gauge station plt.subplot(2,1,1) model_timeseries.dataset.ssh.isel(id_dim=stn_id).plot() # rename time dimension to enable automatic x-axis labelling plt.subplot(2,1,2) obs.dataset.ssh.isel(id_dim=stn_id).swap_dims({\u0026#39;t_dim\u0026#39;:\u0026#39;time\u0026#39;}).plot() # rename time dimension to enable automatic x-axis labelling plt.tight_layout() plt.show() We can see that the structure for the new dataset model_timeseries, generated from the gridded model simulation, is that of a tidegauge object. NB in the example simulation data the ssh variable is output as 5-day means. So it is not particulatly useful for high frequency validation but serves as a demonstration of the workflow.\nTidegauge analysis methods For a good comparison, we would like to make sure that both the observed and modelled Tidegauge objects contain the same missing values. TidegaugeAnalysis contains a routine for ensuring this. First create our analysis object:\ntganalysis = coast.TidegaugeAnalysis() # This routine searches for missing values in each dataset and applies them # equally to each corresponding dataset obs_new, model_new = tganalysis.match_missing_values(obs.dataset.ssh, model_timeseries.dataset.ssh) Tidegauge object at 0x561eabdbafc0 initialised Tidegauge object at 0x561eabdbafc0 initialised  Although we input data arrays to the above routine, it returns two new Tidegauge objects. Now you have equivalent and comparable sets of time series that can be easily compared.\nHarmonic Analysis \u0026amp; Non tidal-Residuals The Tidegauge object contains some routines which make harmonic analysis and the calculation/comparison of non-tidal residuals easier. Harmonic analysis is done using the utide package. Please see here for more info.\nFirst we can use the TidegaugeAnalysis class to do a harmonic analysis. Suppose we have two Tidegauge objects called obs and model_timeseries generated from tidegauge observations and model simulations respectively.\nThen subtract means from all the time series\n# Subtract means from all time series obs_new = tganalysis.demean_timeseries(obs_new.dataset) model_new = tganalysis.demean_timeseries(model_new.dataset) # Now you have equivalent and comparable sets of time series that can be # easily compared. Tidegauge object at 0x561eabdbafc0 initialised Tidegauge object at 0x561eabdbafc0 initialised  plt.figure() plt.plot( model_new.dataset.time, model_new.dataset.ssh.isel(id_dim=stn_id), label=\u0026#39;model - demeaned\u0026#39;, color=\u0026#39;orange\u0026#39; ) plt.plot( obs_new.dataset.time, obs_new.dataset.ssh.isel(id_dim=stn_id) , label=\u0026#39;obs - demeaned\u0026#39;, color=\u0026#39;blue\u0026#39; ) plt.title(f\u0026#39;model and observed timeseries : {obs_new.dataset.site_name.isel(id_dim=stn_id).coords}\u0026#39;) plt.legend() plt.xticks(rotation=45) plt.show() Then we can apply the harmonic analysis (though the example data is too short for this example to be that meaningful and the model data is only saved as 5-day means! Nevertheless we proceed):\nCalculate non tidal residuals First, do a harmonic analysis. This routine uses utide\nha_mod = tganalysis.harmonic_analysis_utide(model_new.dataset.ssh, min_datapoints=1) ha_obs = tganalysis.harmonic_analysis_utide(obs_new.dataset.ssh, min_datapoints=1) solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done.  The harmonic_analysis_utide routine returns a list of utide structure object, one for each id_dim in the Tidegauge object. It can be passed any of the arguments that go to utide. It also has an additional argument min_datapoints which determines a minimum number of data points for the harmonics analysis. If a tidegauge id_dim has less than this number, it will not return an analysis.\nNow, create new TidegaugeMultiple objects containing reconstructed tides:\ntide_mod = tganalysis.reconstruct_tide_utide(model_new.dataset.time, ha_mod) tide_obs = tganalysis.reconstruct_tide_utide(obs_new.dataset.time, ha_obs) prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. Tidegauge object at 0x561eabdbafc0 initialised prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. Tidegauge object at 0x561eabdbafc0 initialised  Get new TidegaugeMultiple objects containing non tidal residuals:\nntr_mod = tganalysis.calculate_non_tidal_residuals(model_new.dataset.ssh, tide_mod.dataset.reconstructed, apply_filter=False) ntr_obs = tganalysis.calculate_non_tidal_residuals(obs_new.dataset.ssh, tide_obs.dataset.reconstructed, apply_filter=True, window_length=10, polyorder=2) # Take a look ntr_obs.dataset Tidegauge object at 0x561eabdbafc0 initialised Tidegauge object at 0x561eabdbafc0 initialised              /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { \u0026ndash;xr-font-color0: var(\u0026ndash;jp-content-font-color0, rgba(0, 0, 0, 1)); \u0026ndash;xr-font-color2: var(\u0026ndash;jp-content-font-color2, rgba(0, 0, 0, 0.54)); \u0026ndash;xr-font-color3: var(\u0026ndash;jp-content-font-color3, rgba(0, 0, 0, 0.38)); \u0026ndash;xr-border-color: var(\u0026ndash;jp-border-color2, #e0e0e0); \u0026ndash;xr-disabled-color: var(\u0026ndash;jp-layout-color3, #bdbdbd); \u0026ndash;xr-background-color: var(\u0026ndash;jp-layout-color0, white); \u0026ndash;xr-background-color-row-even: var(\u0026ndash;jp-layout-color1, white); \u0026ndash;xr-background-color-row-odd: var(\u0026ndash;jp-layout-color2, #eeeeee); }\nhtml[theme=dark], body[data-theme=dark], body.vscode-dark { \u0026ndash;xr-font-color0: rgba(255, 255, 255, 1); \u0026ndash;xr-font-color2: rgba(255, 255, 255, 0.54); \u0026ndash;xr-font-color3: rgba(255, 255, 255, 0.38); \u0026ndash;xr-border-color: #1F1F1F; \u0026ndash;xr-disabled-color: #515151; \u0026ndash;xr-background-color: #111111; \u0026ndash;xr-background-color-row-even: #111111; \u0026ndash;xr-background-color-row-odd: #313131; }\n.xr-wrap { display: block !important; min-width: 300px; max-width: 700px; }\n.xr-text-repr-fallback { /* fallback to plain text repr when CSS is not injected (untrusted notebook) */ display: none; }\n.xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(\u0026ndash;xr-border-color); }\n.xr-header \u0026gt; div, .xr-header \u0026gt; ul { display: inline; margin-top: 0; margin-bottom: 0; }\n.xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; }\n.xr-obj-type { color: var(\u0026ndash;xr-font-color2); }\n.xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; }\n.xr-section-item { display: contents; }\n.xr-section-item input { display: none; }\n.xr-section-item input + label { color: var(\u0026ndash;xr-disabled-color); }\n.xr-section-item input:enabled + label { cursor: pointer; color: var(\u0026ndash;xr-font-color2); }\n.xr-section-item input:enabled + label:hover { color: var(\u0026ndash;xr-font-color0); }\n.xr-section-summary { grid-column: 1; color: var(\u0026ndash;xr-font-color2); font-weight: 500; }\n.xr-section-summary \u0026gt; span { display: inline-block; padding-left: 0.5em; }\n.xr-section-summary-in:disabled + label { color: var(\u0026ndash;xr-font-color2); }\n.xr-section-summary-in + label:before { display: inline-block; content: \u0026lsquo;►\u0026rsquo;; font-size: 11px; width: 15px; text-align: center; }\n.xr-section-summary-in:disabled + label:before { color: var(\u0026ndash;xr-disabled-color); }\n.xr-section-summary-in:checked + label:before { content: \u0026lsquo;▼\u0026rsquo;; }\n.xr-section-summary-in:checked + label \u0026gt; span { display: none; }\n.xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; }\n.xr-section-inline-details { grid-column: 2 / -1; }\n.xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; }\n.xr-section-summary-in:checked ~ .xr-section-details { display: contents; }\n.xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; }\n.xr-array-wrap \u0026gt; label { grid-column: 1; vertical-align: top; }\n.xr-preview { color: var(\u0026ndash;xr-font-color3); }\n.xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; }\n.xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; }\n.xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; }\n.xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; }\n.xr-dim-list li { display: inline-block; padding: 0; margin: 0; }\n.xr-dim-list:before { content: \u0026lsquo;('; }\n.xr-dim-list:after { content: \u0026lsquo;)'; }\n.xr-dim-list li:not(:last-child):after { content: \u0026lsquo;,'; padding-right: 5px; }\n.xr-has-index { font-weight: bold; }\n.xr-var-list, .xr-var-item { display: contents; }\n.xr-var-item \u0026gt; div, .xr-var-item label, .xr-var-item \u0026gt; .xr-var-name span { background-color: var(\u0026ndash;xr-background-color-row-even); margin-bottom: 0; }\n.xr-var-item \u0026gt; .xr-var-name:hover span { padding-right: 5px; }\n.xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; div, .xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; label, .xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; .xr-var-name span { background-color: var(\u0026ndash;xr-background-color-row-odd); }\n.xr-var-name { grid-column: 1; }\n.xr-var-dims { grid-column: 2; }\n.xr-var-dtype { grid-column: 3; text-align: right; color: var(\u0026ndash;xr-font-color2); }\n.xr-var-preview { grid-column: 4; }\n.xr-index-preview { grid-column: 2 / 5; color: var(\u0026ndash;xr-font-color2); }\n.xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; }\n.xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; }\n.xr-var-attrs, .xr-var-data, .xr-index-data { display: none; background-color: var(\u0026ndash;xr-background-color) !important; padding-bottom: 5px !important; }\n.xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data, .xr-index-data-in:checked ~ .xr-index-data { display: block; }\n.xr-var-data \u0026gt; table { float: right; }\n.xr-var-name span, .xr-var-data, .xr-index-name div, .xr-index-data, .xr-attrs { padding-left: 25px !important; }\n.xr-attrs, .xr-var-attrs, .xr-var-data, .xr-index-data { grid-column: 1 / -1; }\ndl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; }\n.xr-attrs dt, .xr-attrs dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; }\n.xr-attrs dt { font-weight: normal; grid-column: 1; }\n.xr-attrs dt:hover span { display: inline-block; background: var(\u0026ndash;xr-background-color); padding-right: 10px; }\n.xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; }\n.xr-icon-database, .xr-icon-file-text2, .xr-no-icon { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } \u0026lt;xarray.Dataset\u0026gt; Dimensions: (t_dim: 720, id_dim: 61) Coordinates: time (t_dim) datetime64[ns] 2007-01-01 \u0026hellip; 2007-01-30T23:00:00 longitude (id_dim) float64 1.292 -3.975 -5.158 -5.051 \u0026hellip; 7.567 350.8 8.717 latitude (id_dim) float64 51.95 51.57 57.9 51.71 \u0026hellip; 58.0 51.53 53.87 site_name (id_dim) object 'Harwich' 'Mumbles' 'Ullapool' \u0026hellip; 'N/A' 'N/A' Dimensions without coordinates: t_dim, id_dim Data variables: ntr (id_dim, t_dim) float64 0.4182 0.4182 0.4182 \u0026hellip; -0.02699 0.2945xarray.DatasetDimensions:t_dim: 720id_dim: 61Coordinates: (4)time(t_dim)datetime64[ns]2007-01-01 \u0026hellip; 2007-01-30T23:00:00array(['2007-01-01T00:00:00.000000000', '2007-01-01T01:00:00.000000000', '2007-01-01T02:00:00.000000000', '2007-01-01T03:00:00.000000000', '2007-01-01T04:00:00.000000000', '2007-01-01T05:00:00.000000000', '2007-01-01T06:00:00.000000000', '2007-01-01T07:00:00.000000000', '2007-01-01T08:00:00.000000000', '2007-01-01T09:00:00.000000000', '2007-01-01T10:00:00.000000000', '2007-01-01T11:00:00.000000000', '2007-01-01T12:00:00.000000000', '2007-01-01T13:00:00.000000000', '2007-01-01T14:00:00.000000000', '2007-01-01T15:00:00.000000000', '2007-01-01T16:00:00.000000000', '2007-01-01T17:00:00.000000000', '2007-01-01T18:00:00.000000000', '2007-01-01T19:00:00.000000000', '2007-01-01T20:00:00.000000000', '2007-01-01T21:00:00.000000000', '2007-01-01T22:00:00.000000000', '2007-01-01T23:00:00.000000000', '2007-01-02T00:00:00.000000000', '2007-01-02T01:00:00.000000000', '2007-01-02T02:00:00.000000000', '2007-01-02T03:00:00.000000000', '2007-01-02T04:00:00.000000000', '2007-01-02T05:00:00.000000000', '2007-01-02T06:00:00.000000000', '2007-01-02T07:00:00.000000000', '2007-01-02T08:00:00.000000000', '2007-01-02T09:00:00.000000000', '2007-01-02T10:00:00.000000000', '2007-01-02T11:00:00.000000000', '2007-01-02T12:00:00.000000000', '2007-01-02T13:00:00.000000000', '2007-01-02T14:00:00.000000000', '2007-01-02T15:00:00.000000000', \u0026hellip; '2007-01-29T10:00:00.000000000', '2007-01-29T11:00:00.000000000', '2007-01-29T12:00:00.000000000', '2007-01-29T13:00:00.000000000', '2007-01-29T14:00:00.000000000', '2007-01-29T15:00:00.000000000', '2007-01-29T16:00:00.000000000', '2007-01-29T17:00:00.000000000', '2007-01-29T18:00:00.000000000', '2007-01-29T19:00:00.000000000', '2007-01-29T20:00:00.000000000', '2007-01-29T21:00:00.000000000', '2007-01-29T22:00:00.000000000', '2007-01-29T23:00:00.000000000', '2007-01-30T00:00:00.000000000', '2007-01-30T01:00:00.000000000', '2007-01-30T02:00:00.000000000', '2007-01-30T03:00:00.000000000', '2007-01-30T04:00:00.000000000', '2007-01-30T05:00:00.000000000', '2007-01-30T06:00:00.000000000', '2007-01-30T07:00:00.000000000', '2007-01-30T08:00:00.000000000', '2007-01-30T09:00:00.000000000', '2007-01-30T10:00:00.000000000', '2007-01-30T11:00:00.000000000', '2007-01-30T12:00:00.000000000', '2007-01-30T13:00:00.000000000', '2007-01-30T14:00:00.000000000', '2007-01-30T15:00:00.000000000', '2007-01-30T16:00:00.000000000', '2007-01-30T17:00:00.000000000', '2007-01-30T18:00:00.000000000', '2007-01-30T19:00:00.000000000', '2007-01-30T20:00:00.000000000', '2007-01-30T21:00:00.000000000', '2007-01-30T22:00:00.000000000', '2007-01-30T23:00:00.000000000'], dtype='datetime64[ns]')longitude(id_dim)float641.292 -3.975 -5.158 \u0026hellip; 350.8 8.717array([ 1.29210000e+00, -3.97544000e+00, -5.15789000e+00, -5.05148000e+00, -2.92042000e+00, -3.13433000e+00, -5.05036000e+00, -3.08631000e+00, -1.43978000e+00, 1.34839000e+00, -4.62044000e+00, 1.30164000e+00, -6.06422000e+00, -2.44794000e+00, -3.82522000e+00, -4.18525000e+00, 7.43440000e-01, -6.14170000e-01, -5.54283000e+00, -5.12003000e+00, -1.43978000e+00, -2.08013000e+00, -1.14031000e+00, -2.98744000e+00, -1.87486000e+00, -6.31642000e+00, -3.01800000e+00, -6.19006000e+00, -1.86030000e-01, 5.70300000e-02, -4.00220000e+00, 1.32267000e+00, -2.71497000e+00, -2.11667000e+00, -1.43978000e+00, -3.56764000e+00, -1.43978000e+00, -2.72848000e+00, -3.18169000e+00, 1.75083000e+00, -4.98333000e+00, -5.66947000e+00, -3.13433000e+00, -1.11175000e+00, -6.38889000e+00, -4.90583000e+00, -4.76806000e+00, -6.65683000e+00, -2.72848000e+00, -1.43978000e+00, -4.04517000e+00, -4.11094000e+00, -1.43978000e+00, 1.12150002e+01, 1.18000002e+01, 5.11700010e+00, 1.12500000e+01, 3.52666992e+02, 7.56699991e+00, 3.50816986e+02, 8.71700001e+00])latitude(id_dim)float6451.95 51.57 57.9 \u0026hellip; 51.53 53.87array([51.94798 , 51.57 , 57.89525 , 51.7064 , 54.03167 , 51.21525 , 58.45661 , 58.44097 , 55.00744 , 51.95675 , 53.31394 , 52.93436 , 56.62311 , 50.6085 , 53.33167 , 50.36839 , 51.44564 , 54.49008 , 50.103 , 54.84256 , 55.00744 , 57.14406 , 60.15403 , 51.55 , 50.71433 , 49.91847 , 53.44969 , 55.62742 , 53.63103 , 50.78178 , 57.5987 , 51.11439 , 51.51089 , 49.18333 , 55.00744 , 54.65081 , 55.00744 , 51.50002 , 55.98983 , 52.473 , 52.01378 , 54.66475 , 51.21525 , 50.80256 , 58.20711 , 55.74964 , 54.08539 , 55.20678 , 51.50002 , 55.00744 , 52.71906 , 51.21097 , 55.00744 , 58.34999847, 57.68299866, 61.93299866, 64.86699677, 55.36700058, 58. , 51.53300095, 53.86700058])site_name(id_dim)object'Harwich' 'Mumbles' \u0026hellip; 'N/A' 'N/A'array(['Harwich', 'Mumbles', 'Ullapool', 'Milford Haven', 'Heysham', 'Hinkley Point', 'Kinlochbervie', 'Wick', 'North Shields', 'Felixstowe', 'Holyhead', 'Cromer', 'Tobermory', 'Weymouth', 'Llandudno', 'Devonport', 'Sheerness', 'Whitby', 'Newlyn', 'Portpatrick', 'North Shields', 'Aberdeen', 'Lerwick', 'Newport', 'Bournemouth', \u0026quot;St. Mary's\u0026quot;, 'Liverpool, Gladstone Dock', 'Port Ellen (Islay)', 'Immingham', 'Newhaven', 'Moray Firth', 'Dover', 'Avonmouth', 'St. Helier (Jersey)', 'North Shields', 'Workington', 'North Shields', 'Portbury', 'Leith', 'Lowestoft', 'Fishguard', 'Bangor', 'Hinkley Point', 'Portsmouth', 'Stornoway', 'Millport', 'Port Erin', 'Portrush', 'Portbury', 'North Shields', 'Barmouth', 'Ilfracombe', 'North Shields', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A'], dtype=object)Data variables: (1)ntr(id_dim, t_dim)float640.4182 0.4182 \u0026hellip; -0.02699 0.2945array([[ 0.41824902, 0.41824902, 0.41824902, \u0026hellip;, -0.34442645, -0.14399667, 0.11602685], [ 0.21228445, 0.21228445, 0.21228445, \u0026hellip;, 0.11863819, 0.11863819, 0.11863819], [ 0.1459287 , 0.1459287 , 0.1459287 , \u0026hellip;, 0.1846614 , 0.2464215 , 0.31395693], \u0026hellip;, [-0.17406373, -0.17406373, -0.17406373, \u0026hellip;, -0.1497518 , -0.18778463, -0.23589221], [-0.09974818, -0.09974818, -0.09974818, \u0026hellip;, -0.00720227, -0.07338597, -0.16374877], [ 0.75580794, 0.75580794, 0.75580794, \u0026hellip;, -0.2887721 , -0.0269909 , 0.29453278]])Indexes: (0)Attributes: (0)\nThe dataset structure is preserved and has created a new variable called ntr - non-tidal residual.\nBy default, this routines will apply scipy.signal.savgol_filter to the non-tidal residuals to remove some noise. This can be switched off using apply_filter = False.\nThe Doodson X0 filter is an alternative way of estimating non-tidal residuals. This will return a new Tidegauge() object containing filtered ssh data:\ndx0 = tganalysis.doodson_x0_filter(obs_new.dataset, \u0026#34;ssh\u0026#34;) # take a look dx0.dataset Tidegauge object at 0x561eabdbafc0 initialised              /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { \u0026ndash;xr-font-color0: var(\u0026ndash;jp-content-font-color0, rgba(0, 0, 0, 1)); \u0026ndash;xr-font-color2: var(\u0026ndash;jp-content-font-color2, rgba(0, 0, 0, 0.54)); \u0026ndash;xr-font-color3: var(\u0026ndash;jp-content-font-color3, rgba(0, 0, 0, 0.38)); \u0026ndash;xr-border-color: var(\u0026ndash;jp-border-color2, #e0e0e0); \u0026ndash;xr-disabled-color: var(\u0026ndash;jp-layout-color3, #bdbdbd); \u0026ndash;xr-background-color: var(\u0026ndash;jp-layout-color0, white); \u0026ndash;xr-background-color-row-even: var(\u0026ndash;jp-layout-color1, white); \u0026ndash;xr-background-color-row-odd: var(\u0026ndash;jp-layout-color2, #eeeeee); }\nhtml[theme=dark], body[data-theme=dark], body.vscode-dark { \u0026ndash;xr-font-color0: rgba(255, 255, 255, 1); \u0026ndash;xr-font-color2: rgba(255, 255, 255, 0.54); \u0026ndash;xr-font-color3: rgba(255, 255, 255, 0.38); \u0026ndash;xr-border-color: #1F1F1F; \u0026ndash;xr-disabled-color: #515151; \u0026ndash;xr-background-color: #111111; \u0026ndash;xr-background-color-row-even: #111111; \u0026ndash;xr-background-color-row-odd: #313131; }\n.xr-wrap { display: block !important; min-width: 300px; max-width: 700px; }\n.xr-text-repr-fallback { /* fallback to plain text repr when CSS is not injected (untrusted notebook) */ display: none; }\n.xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(\u0026ndash;xr-border-color); }\n.xr-header \u0026gt; div, .xr-header \u0026gt; ul { display: inline; margin-top: 0; margin-bottom: 0; }\n.xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; }\n.xr-obj-type { color: var(\u0026ndash;xr-font-color2); }\n.xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; }\n.xr-section-item { display: contents; }\n.xr-section-item input { display: none; }\n.xr-section-item input + label { color: var(\u0026ndash;xr-disabled-color); }\n.xr-section-item input:enabled + label { cursor: pointer; color: var(\u0026ndash;xr-font-color2); }\n.xr-section-item input:enabled + label:hover { color: var(\u0026ndash;xr-font-color0); }\n.xr-section-summary { grid-column: 1; color: var(\u0026ndash;xr-font-color2); font-weight: 500; }\n.xr-section-summary \u0026gt; span { display: inline-block; padding-left: 0.5em; }\n.xr-section-summary-in:disabled + label { color: var(\u0026ndash;xr-font-color2); }\n.xr-section-summary-in + label:before { display: inline-block; content: \u0026lsquo;►\u0026rsquo;; font-size: 11px; width: 15px; text-align: center; }\n.xr-section-summary-in:disabled + label:before { color: var(\u0026ndash;xr-disabled-color); }\n.xr-section-summary-in:checked + label:before { content: \u0026lsquo;▼\u0026rsquo;; }\n.xr-section-summary-in:checked + label \u0026gt; span { display: none; }\n.xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; }\n.xr-section-inline-details { grid-column: 2 / -1; }\n.xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; }\n.xr-section-summary-in:checked ~ .xr-section-details { display: contents; }\n.xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; }\n.xr-array-wrap \u0026gt; label { grid-column: 1; vertical-align: top; }\n.xr-preview { color: var(\u0026ndash;xr-font-color3); }\n.xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; }\n.xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; }\n.xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; }\n.xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; }\n.xr-dim-list li { display: inline-block; padding: 0; margin: 0; }\n.xr-dim-list:before { content: \u0026lsquo;('; }\n.xr-dim-list:after { content: \u0026lsquo;)'; }\n.xr-dim-list li:not(:last-child):after { content: \u0026lsquo;,'; padding-right: 5px; }\n.xr-has-index { font-weight: bold; }\n.xr-var-list, .xr-var-item { display: contents; }\n.xr-var-item \u0026gt; div, .xr-var-item label, .xr-var-item \u0026gt; .xr-var-name span { background-color: var(\u0026ndash;xr-background-color-row-even); margin-bottom: 0; }\n.xr-var-item \u0026gt; .xr-var-name:hover span { padding-right: 5px; }\n.xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; div, .xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; label, .xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; .xr-var-name span { background-color: var(\u0026ndash;xr-background-color-row-odd); }\n.xr-var-name { grid-column: 1; }\n.xr-var-dims { grid-column: 2; }\n.xr-var-dtype { grid-column: 3; text-align: right; color: var(\u0026ndash;xr-font-color2); }\n.xr-var-preview { grid-column: 4; }\n.xr-index-preview { grid-column: 2 / 5; color: var(\u0026ndash;xr-font-color2); }\n.xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; }\n.xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; }\n.xr-var-attrs, .xr-var-data, .xr-index-data { display: none; background-color: var(\u0026ndash;xr-background-color) !important; padding-bottom: 5px !important; }\n.xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data, .xr-index-data-in:checked ~ .xr-index-data { display: block; }\n.xr-var-data \u0026gt; table { float: right; }\n.xr-var-name span, .xr-var-data, .xr-index-name div, .xr-index-data, .xr-attrs { padding-left: 25px !important; }\n.xr-attrs, .xr-var-attrs, .xr-var-data, .xr-index-data { grid-column: 1 / -1; }\ndl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; }\n.xr-attrs dt, .xr-attrs dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; }\n.xr-attrs dt { font-weight: normal; grid-column: 1; }\n.xr-attrs dt:hover span { display: inline-block; background: var(\u0026ndash;xr-background-color); padding-right: 10px; }\n.xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; }\n.xr-icon-database, .xr-icon-file-text2, .xr-no-icon { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } \u0026lt;xarray.Dataset\u0026gt; Dimensions: (t_dim: 720, id_dim: 61) Coordinates: latitude (id_dim) float64 51.95 51.57 57.9 51.71 \u0026hellip; 58.0 51.53 53.87 site_name (id_dim) object 'Harwich' 'Mumbles' 'Ullapool' \u0026hellip; 'N/A' 'N/A' longitude (id_dim) float64 1.292 -3.975 -5.158 -5.051 \u0026hellip; 7.567 350.8 8.717 Dimensions without coordinates: t_dim, id_dim Data variables: time (t_dim) datetime64[ns] 2007-01-01 \u0026hellip; 2007-01-30T23:00:00 ssh (id_dim, t_dim) float64 nan nan nan nan nan \u0026hellip; nan nan nan nanxarray.DatasetDimensions:t_dim: 720id_dim: 61Coordinates: (3)latitude(id_dim)float6451.95 51.57 57.9 \u0026hellip; 51.53 53.87array([51.94798 , 51.57 , 57.89525 , 51.7064 , 54.03167 , 51.21525 , 58.45661 , 58.44097 , 55.00744 , 51.95675 , 53.31394 , 52.93436 , 56.62311 , 50.6085 , 53.33167 , 50.36839 , 51.44564 , 54.49008 , 50.103 , 54.84256 , 55.00744 , 57.14406 , 60.15403 , 51.55 , 50.71433 , 49.91847 , 53.44969 , 55.62742 , 53.63103 , 50.78178 , 57.5987 , 51.11439 , 51.51089 , 49.18333 , 55.00744 , 54.65081 , 55.00744 , 51.50002 , 55.98983 , 52.473 , 52.01378 , 54.66475 , 51.21525 , 50.80256 , 58.20711 , 55.74964 , 54.08539 , 55.20678 , 51.50002 , 55.00744 , 52.71906 , 51.21097 , 55.00744 , 58.34999847, 57.68299866, 61.93299866, 64.86699677, 55.36700058, 58. , 51.53300095, 53.86700058])site_name(id_dim)object'Harwich' 'Mumbles' \u0026hellip; 'N/A' 'N/A'array(['Harwich', 'Mumbles', 'Ullapool', 'Milford Haven', 'Heysham', 'Hinkley Point', 'Kinlochbervie', 'Wick', 'North Shields', 'Felixstowe', 'Holyhead', 'Cromer', 'Tobermory', 'Weymouth', 'Llandudno', 'Devonport', 'Sheerness', 'Whitby', 'Newlyn', 'Portpatrick', 'North Shields', 'Aberdeen', 'Lerwick', 'Newport', 'Bournemouth', \u0026quot;St. Mary's\u0026quot;, 'Liverpool, Gladstone Dock', 'Port Ellen (Islay)', 'Immingham', 'Newhaven', 'Moray Firth', 'Dover', 'Avonmouth', 'St. Helier (Jersey)', 'North Shields', 'Workington', 'North Shields', 'Portbury', 'Leith', 'Lowestoft', 'Fishguard', 'Bangor', 'Hinkley Point', 'Portsmouth', 'Stornoway', 'Millport', 'Port Erin', 'Portrush', 'Portbury', 'North Shields', 'Barmouth', 'Ilfracombe', 'North Shields', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A'], dtype=object)longitude(id_dim)float641.292 -3.975 -5.158 \u0026hellip; 350.8 8.717array([ 1.29210000e+00, -3.97544000e+00, -5.15789000e+00, -5.05148000e+00, -2.92042000e+00, -3.13433000e+00, -5.05036000e+00, -3.08631000e+00, -1.43978000e+00, 1.34839000e+00, -4.62044000e+00, 1.30164000e+00, -6.06422000e+00, -2.44794000e+00, -3.82522000e+00, -4.18525000e+00, 7.43440000e-01, -6.14170000e-01, -5.54283000e+00, -5.12003000e+00, -1.43978000e+00, -2.08013000e+00, -1.14031000e+00, -2.98744000e+00, -1.87486000e+00, -6.31642000e+00, -3.01800000e+00, -6.19006000e+00, -1.86030000e-01, 5.70300000e-02, -4.00220000e+00, 1.32267000e+00, -2.71497000e+00, -2.11667000e+00, -1.43978000e+00, -3.56764000e+00, -1.43978000e+00, -2.72848000e+00, -3.18169000e+00, 1.75083000e+00, -4.98333000e+00, -5.66947000e+00, -3.13433000e+00, -1.11175000e+00, -6.38889000e+00, -4.90583000e+00, -4.76806000e+00, -6.65683000e+00, -2.72848000e+00, -1.43978000e+00, -4.04517000e+00, -4.11094000e+00, -1.43978000e+00, 1.12150002e+01, 1.18000002e+01, 5.11700010e+00, 1.12500000e+01, 3.52666992e+02, 7.56699991e+00, 3.50816986e+02, 8.71700001e+00])Data variables: (2)time(t_dim)datetime64[ns]2007-01-01 \u0026hellip; 2007-01-30T23:00:00array(['2007-01-01T00:00:00.000000000', '2007-01-01T01:00:00.000000000', '2007-01-01T02:00:00.000000000', '2007-01-01T03:00:00.000000000', '2007-01-01T04:00:00.000000000', '2007-01-01T05:00:00.000000000', '2007-01-01T06:00:00.000000000', '2007-01-01T07:00:00.000000000', '2007-01-01T08:00:00.000000000', '2007-01-01T09:00:00.000000000', '2007-01-01T10:00:00.000000000', '2007-01-01T11:00:00.000000000', '2007-01-01T12:00:00.000000000', '2007-01-01T13:00:00.000000000', '2007-01-01T14:00:00.000000000', '2007-01-01T15:00:00.000000000', '2007-01-01T16:00:00.000000000', '2007-01-01T17:00:00.000000000', '2007-01-01T18:00:00.000000000', '2007-01-01T19:00:00.000000000', '2007-01-01T20:00:00.000000000', '2007-01-01T21:00:00.000000000', '2007-01-01T22:00:00.000000000', '2007-01-01T23:00:00.000000000', '2007-01-02T00:00:00.000000000', '2007-01-02T01:00:00.000000000', '2007-01-02T02:00:00.000000000', '2007-01-02T03:00:00.000000000', '2007-01-02T04:00:00.000000000', '2007-01-02T05:00:00.000000000', '2007-01-02T06:00:00.000000000', '2007-01-02T07:00:00.000000000', '2007-01-02T08:00:00.000000000', '2007-01-02T09:00:00.000000000', '2007-01-02T10:00:00.000000000', '2007-01-02T11:00:00.000000000', '2007-01-02T12:00:00.000000000', '2007-01-02T13:00:00.000000000', '2007-01-02T14:00:00.000000000', '2007-01-02T15:00:00.000000000', \u0026hellip; '2007-01-29T10:00:00.000000000', '2007-01-29T11:00:00.000000000', '2007-01-29T12:00:00.000000000', '2007-01-29T13:00:00.000000000', '2007-01-29T14:00:00.000000000', '2007-01-29T15:00:00.000000000', '2007-01-29T16:00:00.000000000', '2007-01-29T17:00:00.000000000', '2007-01-29T18:00:00.000000000', '2007-01-29T19:00:00.000000000', '2007-01-29T20:00:00.000000000', '2007-01-29T21:00:00.000000000', '2007-01-29T22:00:00.000000000', '2007-01-29T23:00:00.000000000', '2007-01-30T00:00:00.000000000', '2007-01-30T01:00:00.000000000', '2007-01-30T02:00:00.000000000', '2007-01-30T03:00:00.000000000', '2007-01-30T04:00:00.000000000', '2007-01-30T05:00:00.000000000', '2007-01-30T06:00:00.000000000', '2007-01-30T07:00:00.000000000', '2007-01-30T08:00:00.000000000', '2007-01-30T09:00:00.000000000', '2007-01-30T10:00:00.000000000', '2007-01-30T11:00:00.000000000', '2007-01-30T12:00:00.000000000', '2007-01-30T13:00:00.000000000', '2007-01-30T14:00:00.000000000', '2007-01-30T15:00:00.000000000', '2007-01-30T16:00:00.000000000', '2007-01-30T17:00:00.000000000', '2007-01-30T18:00:00.000000000', '2007-01-30T19:00:00.000000000', '2007-01-30T20:00:00.000000000', '2007-01-30T21:00:00.000000000', '2007-01-30T22:00:00.000000000', '2007-01-30T23:00:00.000000000'], dtype='datetime64[ns]')ssh(id_dim, t_dim)float64nan nan nan nan \u0026hellip; nan nan nan nanarray([[nan, nan, nan, \u0026hellip;, nan, nan, nan], [nan, nan, nan, \u0026hellip;, nan, nan, nan], [nan, nan, nan, \u0026hellip;, nan, nan, nan], \u0026hellip;, [nan, nan, nan, \u0026hellip;, nan, nan, nan], [nan, nan, nan, \u0026hellip;, nan, nan, nan], [nan, nan, nan, \u0026hellip;, nan, nan, nan]])Indexes: (0)Attributes: (0)\nWe can compare these analyses e.g. the observed timeseries against the harmonic reconstruction. Noting we can in principle extend the harmoninc reconstruction beyond the observation time window.\nplt.figure() plt.plot( tide_obs.dataset.time, tide_obs.dataset.reconstructed.isel(id_dim=stn_id), label=\u0026#39;reconstructed from harmonics\u0026#39;, color=\u0026#39;orange\u0026#39; ) plt.plot( obs_new.dataset.time, obs_new.dataset.ssh.isel(id_dim=stn_id) , label=\u0026#39;observed\u0026#39;, color=\u0026#39;blue\u0026#39; ) plt.legend() plt.xticks(rotation=45) plt.show() We can also look closer at the difference between the observed timeseries and the harmonic reconstruction, that is the non-tidal residual. And we can compare the observed and modelled non-harmonic residual and contrast the different methods of removing the tides. Here we contrast using a harmonic analysis (and creating a \u0026ldquo;non-tidal residual\u0026rdquo;) with the DoodsonX0 filter. Note that the timeseries is far too short for a sensible analysis and that this is really a demonstration of concept.\nplt.figure() plt.subplot(2,1,1) plt.plot( dx0.dataset.time, dx0.isel(id_dim=stn_id).dataset.ssh, label=\u0026#39;obs: doodsonX0 filtered\u0026#39;, color=\u0026#39;orange\u0026#39; ) plt.plot( ntr_obs.dataset.time, ntr_obs.isel(id_dim=stn_id).dataset.ntr, label=\u0026#39;obs: non-tidal residual\u0026#39;, color=\u0026#39;blue\u0026#39; ) plt.title(\u0026#39;analysis comparison: non-tidal residual vs doodsonX0\u0026#39;) plt.legend() plt.xticks(rotation=45) plt.subplot(2,1,2) plt.plot( ntr_mod.dataset.time, ntr_mod.isel(id_dim=stn_id).dataset.ntr, label=\u0026#39;model: non-tidal residual\u0026#39;, color=\u0026#39;g\u0026#39; ) plt.plot( ntr_obs.dataset.time, ntr_obs.isel(id_dim=stn_id).dataset.ntr, label=\u0026#39;obs: non-tidal residual\u0026#39;, color=\u0026#39;blue\u0026#39; ) plt.title(\u0026#39;model vs observation: non-tidal residual\u0026#39;) plt.legend() plt.xticks(rotation=45) plt.tight_layout() Threshold Statistics for non-tidal residuals This is a simple extreme value analysis of whatever data you use. It will count the number of peaks and the total time spent over each threshold provided. It will also count the numbers of daily and monthly maxima over each threshold. To this, a Tidegauge object and an array of thresholds (in metres) should be passed. The method return peak_count_*, time_over_threshold_*, dailymax_count_*, monthlymax_count_*:\nthresh_mod = tganalysis.threshold_statistics(ntr_mod.dataset, thresholds=np.arange(-2, 2, 0.1)) thresh_obs = tganalysis.threshold_statistics(ntr_obs.dataset, thresholds=np.arange(-2, 2, 0.1)) # Have a look thresh_obs             /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { \u0026ndash;xr-font-color0: var(\u0026ndash;jp-content-font-color0, rgba(0, 0, 0, 1)); \u0026ndash;xr-font-color2: var(\u0026ndash;jp-content-font-color2, rgba(0, 0, 0, 0.54)); \u0026ndash;xr-font-color3: var(\u0026ndash;jp-content-font-color3, rgba(0, 0, 0, 0.38)); \u0026ndash;xr-border-color: var(\u0026ndash;jp-border-color2, #e0e0e0); \u0026ndash;xr-disabled-color: var(\u0026ndash;jp-layout-color3, #bdbdbd); \u0026ndash;xr-background-color: var(\u0026ndash;jp-layout-color0, white); \u0026ndash;xr-background-color-row-even: var(\u0026ndash;jp-layout-color1, white); \u0026ndash;xr-background-color-row-odd: var(\u0026ndash;jp-layout-color2, #eeeeee); }\nhtml[theme=dark], body[data-theme=dark], body.vscode-dark { \u0026ndash;xr-font-color0: rgba(255, 255, 255, 1); \u0026ndash;xr-font-color2: rgba(255, 255, 255, 0.54); \u0026ndash;xr-font-color3: rgba(255, 255, 255, 0.38); \u0026ndash;xr-border-color: #1F1F1F; \u0026ndash;xr-disabled-color: #515151; \u0026ndash;xr-background-color: #111111; \u0026ndash;xr-background-color-row-even: #111111; \u0026ndash;xr-background-color-row-odd: #313131; }\n.xr-wrap { display: block !important; min-width: 300px; max-width: 700px; }\n.xr-text-repr-fallback { /* fallback to plain text repr when CSS is not injected (untrusted notebook) */ display: none; }\n.xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(\u0026ndash;xr-border-color); }\n.xr-header \u0026gt; div, .xr-header \u0026gt; ul { display: inline; margin-top: 0; margin-bottom: 0; }\n.xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; }\n.xr-obj-type { color: var(\u0026ndash;xr-font-color2); }\n.xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; }\n.xr-section-item { display: contents; }\n.xr-section-item input { display: none; }\n.xr-section-item input + label { color: var(\u0026ndash;xr-disabled-color); }\n.xr-section-item input:enabled + label { cursor: pointer; color: var(\u0026ndash;xr-font-color2); }\n.xr-section-item input:enabled + label:hover { color: var(\u0026ndash;xr-font-color0); }\n.xr-section-summary { grid-column: 1; color: var(\u0026ndash;xr-font-color2); font-weight: 500; }\n.xr-section-summary \u0026gt; span { display: inline-block; padding-left: 0.5em; }\n.xr-section-summary-in:disabled + label { color: var(\u0026ndash;xr-font-color2); }\n.xr-section-summary-in + label:before { display: inline-block; content: \u0026lsquo;►\u0026rsquo;; font-size: 11px; width: 15px; text-align: center; }\n.xr-section-summary-in:disabled + label:before { color: var(\u0026ndash;xr-disabled-color); }\n.xr-section-summary-in:checked + label:before { content: \u0026lsquo;▼\u0026rsquo;; }\n.xr-section-summary-in:checked + label \u0026gt; span { display: none; }\n.xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; }\n.xr-section-inline-details { grid-column: 2 / -1; }\n.xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; }\n.xr-section-summary-in:checked ~ .xr-section-details { display: contents; }\n.xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; }\n.xr-array-wrap \u0026gt; label { grid-column: 1; vertical-align: top; }\n.xr-preview { color: var(\u0026ndash;xr-font-color3); }\n.xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; }\n.xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; }\n.xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; }\n.xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; }\n.xr-dim-list li { display: inline-block; padding: 0; margin: 0; }\n.xr-dim-list:before { content: \u0026lsquo;('; }\n.xr-dim-list:after { content: \u0026lsquo;)'; }\n.xr-dim-list li:not(:last-child):after { content: \u0026lsquo;,'; padding-right: 5px; }\n.xr-has-index { font-weight: bold; }\n.xr-var-list, .xr-var-item { display: contents; }\n.xr-var-item \u0026gt; div, .xr-var-item label, .xr-var-item \u0026gt; .xr-var-name span { background-color: var(\u0026ndash;xr-background-color-row-even); margin-bottom: 0; }\n.xr-var-item \u0026gt; .xr-var-name:hover span { padding-right: 5px; }\n.xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; div, .xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; label, .xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; .xr-var-name span { background-color: var(\u0026ndash;xr-background-color-row-odd); }\n.xr-var-name { grid-column: 1; }\n.xr-var-dims { grid-column: 2; }\n.xr-var-dtype { grid-column: 3; text-align: right; color: var(\u0026ndash;xr-font-color2); }\n.xr-var-preview { grid-column: 4; }\n.xr-index-preview { grid-column: 2 / 5; color: var(\u0026ndash;xr-font-color2); }\n.xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; }\n.xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; }\n.xr-var-attrs, .xr-var-data, .xr-index-data { display: none; background-color: var(\u0026ndash;xr-background-color) !important; padding-bottom: 5px !important; }\n.xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data, .xr-index-data-in:checked ~ .xr-index-data { display: block; }\n.xr-var-data \u0026gt; table { float: right; }\n.xr-var-name span, .xr-var-data, .xr-index-name div, .xr-index-data, .xr-attrs { padding-left: 25px !important; }\n.xr-attrs, .xr-var-attrs, .xr-var-data, .xr-index-data { grid-column: 1 / -1; }\ndl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; }\n.xr-attrs dt, .xr-attrs dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; }\n.xr-attrs dt { font-weight: normal; grid-column: 1; }\n.xr-attrs dt:hover span { display: inline-block; background: var(\u0026ndash;xr-background-color); padding-right: 10px; }\n.xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; }\n.xr-icon-database, .xr-icon-file-text2, .xr-no-icon { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } \u0026lt;xarray.Dataset\u0026gt; Dimensions: (t_dim: 720, id_dim: 61, threshold: 40) Coordinates: latitude (id_dim) float64 51.95 51.57 57.9 \u0026hellip; 51.53 53.87 site_name (id_dim) object 'Harwich' 'Mumbles' \u0026hellip; 'N/A' 'N/A' longitude (id_dim) float64 1.292 -3.975 \u0026hellip; 350.8 8.717\n threshold (threshold) float64 -2.0 -1.9 -1.8 \u0026hellip; 1.7 1.8 1.9 Dimensions without coordinates: t_dim, id_dim Data variables: time (t_dim) datetime64[ns] 2007-01-01 \u0026hellip; 2007-01-30\u0026hellip; peak_count_ntr (id_dim, threshold) float64 40.0 40.0 \u0026hellip; 2.0 1.0 time_over_threshold_ntr (id_dim, threshold) float64 720.0 720.0 \u0026hellip; 5.0 4.0 dailymax_count_ntr (id_dim, threshold) float64 30.0 30.0 \u0026hellip; 2.0 1.0 monthlymax_count_ntr (id_dim, threshold) float64 1.0 1.0 1.0 \u0026hellip; 1.0 1.0xarray.DatasetDimensions:t_dim: 720id_dim: 61threshold: 40Coordinates: (4)latitude(id_dim)float6451.95 51.57 57.9 \u0026hellip; 51.53 53.87array([51.94798 , 51.57 , 57.89525 , 51.7064 , 54.03167 , 51.21525 , 58.45661 , 58.44097 , 55.00744 , 51.95675 , 53.31394 , 52.93436 , 56.62311 , 50.6085 , 53.33167 , 50.36839 , 51.44564 , 54.49008 , 50.103 , 54.84256 , 55.00744 , 57.14406 , 60.15403 , 51.55 , 50.71433 , 49.91847 , 53.44969 , 55.62742 , 53.63103 , 50.78178 , 57.5987 , 51.11439 , 51.51089 , 49.18333 , 55.00744 , 54.65081 , 55.00744 , 51.50002 , 55.98983 , 52.473 , 52.01378 , 54.66475 , 51.21525 , 50.80256 , 58.20711 , 55.74964 , 54.08539 , 55.20678 , 51.50002 , 55.00744 , 52.71906 , 51.21097 , 55.00744 , 58.34999847, 57.68299866, 61.93299866, 64.86699677, 55.36700058, 58. , 51.53300095, 53.86700058])site_name(id_dim)object'Harwich' 'Mumbles' \u0026hellip; 'N/A' 'N/A'array(['Harwich', 'Mumbles', 'Ullapool', 'Milford Haven', 'Heysham', 'Hinkley Point', 'Kinlochbervie', 'Wick', 'North Shields', 'Felixstowe', 'Holyhead', 'Cromer', 'Tobermory', 'Weymouth', 'Llandudno', 'Devonport', 'Sheerness', 'Whitby', 'Newlyn', 'Portpatrick', 'North Shields', 'Aberdeen', 'Lerwick', 'Newport', 'Bournemouth', \u0026quot;St. Mary's\u0026quot;, 'Liverpool, Gladstone Dock', 'Port Ellen (Islay)', 'Immingham', 'Newhaven', 'Moray Firth', 'Dover', 'Avonmouth', 'St. Helier (Jersey)', 'North Shields', 'Workington', 'North Shields', 'Portbury', 'Leith', 'Lowestoft', 'Fishguard', 'Bangor', 'Hinkley Point', 'Portsmouth', 'Stornoway', 'Millport', 'Port Erin', 'Portrush', 'Portbury', 'North Shields', 'Barmouth', 'Ilfracombe', 'North Shields', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A'], dtype=object)longitude(id_dim)float641.292 -3.975 -5.158 \u0026hellip; 350.8 8.717array([ 1.29210000e+00, -3.97544000e+00, -5.15789000e+00, -5.05148000e+00, -2.92042000e+00, -3.13433000e+00, -5.05036000e+00, -3.08631000e+00, -1.43978000e+00, 1.34839000e+00, -4.62044000e+00, 1.30164000e+00, -6.06422000e+00, -2.44794000e+00, -3.82522000e+00, -4.18525000e+00, 7.43440000e-01, -6.14170000e-01, -5.54283000e+00, -5.12003000e+00, -1.43978000e+00, -2.08013000e+00, -1.14031000e+00, -2.98744000e+00, -1.87486000e+00, -6.31642000e+00, -3.01800000e+00, -6.19006000e+00, -1.86030000e-01, 5.70300000e-02, -4.00220000e+00, 1.32267000e+00, -2.71497000e+00, -2.11667000e+00, -1.43978000e+00, -3.56764000e+00, -1.43978000e+00, -2.72848000e+00, -3.18169000e+00, 1.75083000e+00, -4.98333000e+00, -5.66947000e+00, -3.13433000e+00, -1.11175000e+00, -6.38889000e+00, -4.90583000e+00, -4.76806000e+00, -6.65683000e+00, -2.72848000e+00, -1.43978000e+00, -4.04517000e+00, -4.11094000e+00, -1.43978000e+00, 1.12150002e+01, 1.18000002e+01, 5.11700010e+00, 1.12500000e+01, 3.52666992e+02, 7.56699991e+00, 3.50816986e+02, 8.71700001e+00])threshold(threshold)float64-2.0 -1.9 -1.8 -1.7 \u0026hellip; 1.7 1.8 1.9array([-2.000000e+00, -1.900000e+00, -1.800000e+00, -1.700000e+00, -1.600000e+00, -1.500000e+00, -1.400000e+00, -1.300000e+00, -1.200000e+00, -1.100000e+00, -1.000000e+00, -9.000000e-01, -8.000000e-01, -7.000000e-01, -6.000000e-01, -5.000000e-01, -4.000000e-01, -3.000000e-01, -2.000000e-01, -1.000000e-01, 1.776357e-15, 1.000000e-01, 2.000000e-01, 3.000000e-01, 4.000000e-01, 5.000000e-01, 6.000000e-01, 7.000000e-01, 8.000000e-01, 9.000000e-01, 1.000000e+00, 1.100000e+00, 1.200000e+00, 1.300000e+00, 1.400000e+00, 1.500000e+00, 1.600000e+00, 1.700000e+00, 1.800000e+00, 1.900000e+00])Data variables: (5)time(t_dim)datetime64[ns]2007-01-01 \u0026hellip; 2007-01-30T23:00:00array(['2007-01-01T00:00:00.000000000', '2007-01-01T01:00:00.000000000', '2007-01-01T02:00:00.000000000', '2007-01-01T03:00:00.000000000', '2007-01-01T04:00:00.000000000', '2007-01-01T05:00:00.000000000', '2007-01-01T06:00:00.000000000', '2007-01-01T07:00:00.000000000', '2007-01-01T08:00:00.000000000', '2007-01-01T09:00:00.000000000', '2007-01-01T10:00:00.000000000', '2007-01-01T11:00:00.000000000', '2007-01-01T12:00:00.000000000', '2007-01-01T13:00:00.000000000', '2007-01-01T14:00:00.000000000', '2007-01-01T15:00:00.000000000', '2007-01-01T16:00:00.000000000', '2007-01-01T17:00:00.000000000', '2007-01-01T18:00:00.000000000', '2007-01-01T19:00:00.000000000', '2007-01-01T20:00:00.000000000', '2007-01-01T21:00:00.000000000', '2007-01-01T22:00:00.000000000', '2007-01-01T23:00:00.000000000', '2007-01-02T00:00:00.000000000', '2007-01-02T01:00:00.000000000', '2007-01-02T02:00:00.000000000', '2007-01-02T03:00:00.000000000', '2007-01-02T04:00:00.000000000', '2007-01-02T05:00:00.000000000', '2007-01-02T06:00:00.000000000', '2007-01-02T07:00:00.000000000', '2007-01-02T08:00:00.000000000', '2007-01-02T09:00:00.000000000', '2007-01-02T10:00:00.000000000', '2007-01-02T11:00:00.000000000', '2007-01-02T12:00:00.000000000', '2007-01-02T13:00:00.000000000', '2007-01-02T14:00:00.000000000', '2007-01-02T15:00:00.000000000', \u0026hellip; '2007-01-29T10:00:00.000000000', '2007-01-29T11:00:00.000000000', '2007-01-29T12:00:00.000000000', '2007-01-29T13:00:00.000000000', '2007-01-29T14:00:00.000000000', '2007-01-29T15:00:00.000000000', '2007-01-29T16:00:00.000000000', '2007-01-29T17:00:00.000000000', '2007-01-29T18:00:00.000000000', '2007-01-29T19:00:00.000000000', '2007-01-29T20:00:00.000000000', '2007-01-29T21:00:00.000000000', '2007-01-29T22:00:00.000000000', '2007-01-29T23:00:00.000000000', '2007-01-30T00:00:00.000000000', '2007-01-30T01:00:00.000000000', '2007-01-30T02:00:00.000000000', '2007-01-30T03:00:00.000000000', '2007-01-30T04:00:00.000000000', '2007-01-30T05:00:00.000000000', '2007-01-30T06:00:00.000000000', '2007-01-30T07:00:00.000000000', '2007-01-30T08:00:00.000000000', '2007-01-30T09:00:00.000000000', '2007-01-30T10:00:00.000000000', '2007-01-30T11:00:00.000000000', '2007-01-30T12:00:00.000000000', '2007-01-30T13:00:00.000000000', '2007-01-30T14:00:00.000000000', '2007-01-30T15:00:00.000000000', '2007-01-30T16:00:00.000000000', '2007-01-30T17:00:00.000000000', '2007-01-30T18:00:00.000000000', '2007-01-30T19:00:00.000000000', '2007-01-30T20:00:00.000000000', '2007-01-30T21:00:00.000000000', '2007-01-30T22:00:00.000000000', '2007-01-30T23:00:00.000000000'], dtype='datetime64[ns]')peak_count_ntr(id_dim, threshold)float6440.0 40.0 40.0 40.0 \u0026hellip; 2.0 2.0 1.0array([[40., 40., 40., \u0026hellip;, 0., 0., 0.], [22., 22., 22., \u0026hellip;, 0., 0., 0.], [35., 35., 35., \u0026hellip;, 0., 0., 0.], \u0026hellip;, [39., 39., 39., \u0026hellip;, 0., 0., 0.], [42., 42., 42., \u0026hellip;, 0., 0., 0.], [38., 38., 38., \u0026hellip;, 2., 2., 1.]])time_over_threshold_ntr(id_dim, threshold)float64720.0 720.0 720.0 \u0026hellip; 8.0 5.0 4.0array([[720., 720., 720., \u0026hellip;, 0., 0., 0.], [720., 720., 720., \u0026hellip;, 0., 0., 0.], [720., 720., 720., \u0026hellip;, 0., 0., 0.], \u0026hellip;, [720., 720., 720., \u0026hellip;, 0., 0., 0.], [720., 720., 720., \u0026hellip;, 0., 0., 0.], [720., 720., 720., \u0026hellip;, 8., 5., 4.]])dailymax_count_ntr(id_dim, threshold)float6430.0 30.0 30.0 30.0 \u0026hellip; 3.0 2.0 1.0array([[30., 30., 30., \u0026hellip;, 0., 0., 0.], [30., 30., 30., \u0026hellip;, 0., 0., 0.], [30., 30., 30., \u0026hellip;, 0., 0., 0.], \u0026hellip;, [30., 30., 30., \u0026hellip;, 0., 0., 0.], [30., 30., 30., \u0026hellip;, 0., 0., 0.], [30., 30., 30., \u0026hellip;, 3., 2., 1.]])monthlymax_count_ntr(id_dim, threshold)float641.0 1.0 1.0 1.0 \u0026hellip; 1.0 1.0 1.0 1.0array([[1., 1., 1., \u0026hellip;, 0., 0., 0.], [1., 1., 1., \u0026hellip;, 0., 0., 0.], [1., 1., 1., \u0026hellip;, 0., 0., 0.], \u0026hellip;, [1., 1., 1., \u0026hellip;, 0., 0., 0.], [1., 1., 1., \u0026hellip;, 0., 0., 0.], [1., 1., 1., \u0026hellip;, 1., 1., 1.]])Indexes: (1)thresholdPandasIndexPandasIndex(Float64Index([ -2.0, -1.9, -1.7999999999999998, -1.6999999999999997, -1.5999999999999996, -1.4999999999999996, -1.3999999999999995, -1.2999999999999994, -1.1999999999999993, -1.0999999999999992, -0.9999999999999991, -0.899999999999999, -0.7999999999999989, -0.6999999999999988, -0.5999999999999988, -0.49999999999999867, -0.3999999999999986, -0.2999999999999985, -0.1999999999999984, -0.09999999999999831, 1.7763568394002505e-15, 0.10000000000000187, 0.20000000000000195, 0.30000000000000204, 0.40000000000000213, 0.5000000000000022, 0.6000000000000023, 0.7000000000000024, 0.8000000000000025, 0.9000000000000026, 1.0000000000000027, 1.1000000000000028, 1.2000000000000028, 1.300000000000003, 1.400000000000003, 1.500000000000003, 1.6000000000000032, 1.7000000000000033, 1.8000000000000034, 1.9000000000000035], dtype='float64', name='threshold'))Attributes: (0)  plt.plot( thresh_obs.threshold, thresh_obs.peak_count_ntr.mean(dim=\u0026#39;id_dim\u0026#39;) ) #plt.ylim([0,15]) plt.title(f\u0026#34;Observed peaks over threshold\u0026#34;) plt.xlabel(\u0026#39;Analysis threshold (m)\u0026#39;) plt.ylabel(\u0026#39;event count\u0026#39;) Text(0, 0.5, 'event count')  Note that the non-tidal residual is a noisy timeseries (computed as a difference between two timeseries) so peaks do not necessarily correspond to peaks in total water level. For this reason time_over_threshold_* can be useful. Below we see that about 28% (y-axis) of the observations of non-tidal residual exceed 20cm (x-axis). Again, threshold statistics are more useful with a longer record.\nnormalised_event_count = 100 * thresh_obs.time_over_threshold_ntr.isel(id_dim=stn_id) / thresh_obs.time_over_threshold_ntr.isel(id_dim=stn_id).max() plt.plot( thresh_obs.threshold, normalised_event_count ) plt.ylim([20,40]) plt.xlim([0.0,0.4]) plt.title(f\u0026#34;time over threshold\u0026#34;) plt.xlabel(\u0026#39;Analysis threshold (m)\u0026#39;) plt.ylabel(\u0026#39;normalised event count (%)\u0026#39;) plt.plot(thresh_obs.threshold[22], normalised_event_count[22], \u0026#39;r+\u0026#39;, markersize=20) [\u0026lt;matplotlib.lines.Line2D at 0x7fcad850f8e0\u0026gt;]  Other TidegaugeAnalysis methods Calculate errors The difference() routine will calculate differences, absolute_differences and squared differenced for all variables. Corresponding new variables are created with names diff_*, abs_diff_* and square_diff_*\nntr_diff = tganalysis.difference(ntr_obs.dataset, ntr_mod.dataset) ssh_diff = tganalysis.difference(obs_new.dataset, model_new.dataset) # Take a look ntr_diff.dataset Tidegauge object at 0x561eabdbafc0 initialised Tidegauge object at 0x561eabdbafc0 initialised              /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { \u0026ndash;xr-font-color0: var(\u0026ndash;jp-content-font-color0, rgba(0, 0, 0, 1)); \u0026ndash;xr-font-color2: var(\u0026ndash;jp-content-font-color2, rgba(0, 0, 0, 0.54)); \u0026ndash;xr-font-color3: var(\u0026ndash;jp-content-font-color3, rgba(0, 0, 0, 0.38)); \u0026ndash;xr-border-color: var(\u0026ndash;jp-border-color2, #e0e0e0); \u0026ndash;xr-disabled-color: var(\u0026ndash;jp-layout-color3, #bdbdbd); \u0026ndash;xr-background-color: var(\u0026ndash;jp-layout-color0, white); \u0026ndash;xr-background-color-row-even: var(\u0026ndash;jp-layout-color1, white); \u0026ndash;xr-background-color-row-odd: var(\u0026ndash;jp-layout-color2, #eeeeee); }\nhtml[theme=dark], body[data-theme=dark], body.vscode-dark { \u0026ndash;xr-font-color0: rgba(255, 255, 255, 1); \u0026ndash;xr-font-color2: rgba(255, 255, 255, 0.54); \u0026ndash;xr-font-color3: rgba(255, 255, 255, 0.38); \u0026ndash;xr-border-color: #1F1F1F; \u0026ndash;xr-disabled-color: #515151; \u0026ndash;xr-background-color: #111111; \u0026ndash;xr-background-color-row-even: #111111; \u0026ndash;xr-background-color-row-odd: #313131; }\n.xr-wrap { display: block !important; min-width: 300px; max-width: 700px; }\n.xr-text-repr-fallback { /* fallback to plain text repr when CSS is not injected (untrusted notebook) */ display: none; }\n.xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(\u0026ndash;xr-border-color); }\n.xr-header \u0026gt; div, .xr-header \u0026gt; ul { display: inline; margin-top: 0; margin-bottom: 0; }\n.xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; }\n.xr-obj-type { color: var(\u0026ndash;xr-font-color2); }\n.xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; }\n.xr-section-item { display: contents; }\n.xr-section-item input { display: none; }\n.xr-section-item input + label { color: var(\u0026ndash;xr-disabled-color); }\n.xr-section-item input:enabled + label { cursor: pointer; color: var(\u0026ndash;xr-font-color2); }\n.xr-section-item input:enabled + label:hover { color: var(\u0026ndash;xr-font-color0); }\n.xr-section-summary { grid-column: 1; color: var(\u0026ndash;xr-font-color2); font-weight: 500; }\n.xr-section-summary \u0026gt; span { display: inline-block; padding-left: 0.5em; }\n.xr-section-summary-in:disabled + label { color: var(\u0026ndash;xr-font-color2); }\n.xr-section-summary-in + label:before { display: inline-block; content: \u0026lsquo;►\u0026rsquo;; font-size: 11px; width: 15px; text-align: center; }\n.xr-section-summary-in:disabled + label:before { color: var(\u0026ndash;xr-disabled-color); }\n.xr-section-summary-in:checked + label:before { content: \u0026lsquo;▼\u0026rsquo;; }\n.xr-section-summary-in:checked + label \u0026gt; span { display: none; }\n.xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; }\n.xr-section-inline-details { grid-column: 2 / -1; }\n.xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; }\n.xr-section-summary-in:checked ~ .xr-section-details { display: contents; }\n.xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; }\n.xr-array-wrap \u0026gt; label { grid-column: 1; vertical-align: top; }\n.xr-preview { color: var(\u0026ndash;xr-font-color3); }\n.xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; }\n.xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; }\n.xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; }\n.xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; }\n.xr-dim-list li { display: inline-block; padding: 0; margin: 0; }\n.xr-dim-list:before { content: \u0026lsquo;('; }\n.xr-dim-list:after { content: \u0026lsquo;)'; }\n.xr-dim-list li:not(:last-child):after { content: \u0026lsquo;,'; padding-right: 5px; }\n.xr-has-index { font-weight: bold; }\n.xr-var-list, .xr-var-item { display: contents; }\n.xr-var-item \u0026gt; div, .xr-var-item label, .xr-var-item \u0026gt; .xr-var-name span { background-color: var(\u0026ndash;xr-background-color-row-even); margin-bottom: 0; }\n.xr-var-item \u0026gt; .xr-var-name:hover span { padding-right: 5px; }\n.xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; div, .xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; label, .xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; .xr-var-name span { background-color: var(\u0026ndash;xr-background-color-row-odd); }\n.xr-var-name { grid-column: 1; }\n.xr-var-dims { grid-column: 2; }\n.xr-var-dtype { grid-column: 3; text-align: right; color: var(\u0026ndash;xr-font-color2); }\n.xr-var-preview { grid-column: 4; }\n.xr-index-preview { grid-column: 2 / 5; color: var(\u0026ndash;xr-font-color2); }\n.xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; }\n.xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; }\n.xr-var-attrs, .xr-var-data, .xr-index-data { display: none; background-color: var(\u0026ndash;xr-background-color) !important; padding-bottom: 5px !important; }\n.xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data, .xr-index-data-in:checked ~ .xr-index-data { display: block; }\n.xr-var-data \u0026gt; table { float: right; }\n.xr-var-name span, .xr-var-data, .xr-index-name div, .xr-index-data, .xr-attrs { padding-left: 25px !important; }\n.xr-attrs, .xr-var-attrs, .xr-var-data, .xr-index-data { grid-column: 1 / -1; }\ndl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; }\n.xr-attrs dt, .xr-attrs dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; }\n.xr-attrs dt { font-weight: normal; grid-column: 1; }\n.xr-attrs dt:hover span { display: inline-block; background: var(\u0026ndash;xr-background-color); padding-right: 10px; }\n.xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; }\n.xr-icon-database, .xr-icon-file-text2, .xr-no-icon { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } \u0026lt;xarray.Dataset\u0026gt; Dimensions: (t_dim: 720, id_dim: 61) Coordinates:\n time (t_dim) datetime64[ns] 2007-01-01 \u0026hellip; 2007-01-30T23:00:00 site_name (id_dim) object 'Harwich' 'Mumbles' \u0026hellip; 'N/A' 'N/A' longitude (id_dim) float64 1.292 -3.975 -5.158 \u0026hellip; 7.567 350.8 8.717 latitude (id_dim) float64 51.95 51.57 57.9 \u0026hellip; 58.0 51.53 53.87 Dimensions without coordinates: t_dim, id_dim Data variables: diff_ntr (id_dim, t_dim) float64 nan nan nan \u0026hellip; 0.05885 0.3435 abs_diff_ntr (id_dim, t_dim) float64 nan nan nan \u0026hellip; 0.05885 0.3435 square_diff_ntr (id_dim, t_dim) float64 nan nan nan \u0026hellip; 0.003464 0.118xarray.DatasetDimensions:t_dim: 720id_dim: 61Coordinates: (4)time(t_dim)datetime64[ns]2007-01-01 \u0026hellip; 2007-01-30T23:00:00array(['2007-01-01T00:00:00.000000000', '2007-01-01T01:00:00.000000000', '2007-01-01T02:00:00.000000000', \u0026hellip;, '2007-01-30T21:00:00.000000000', '2007-01-30T22:00:00.000000000', '2007-01-30T23:00:00.000000000'], dtype='datetime64[ns]')site_name(id_dim)object'Harwich' 'Mumbles' \u0026hellip; 'N/A' 'N/A'array(['Harwich', 'Mumbles', 'Ullapool', 'Milford Haven', 'Heysham', 'Hinkley Point', 'Kinlochbervie', 'Wick', 'North Shields', 'Felixstowe', 'Holyhead', 'Cromer', 'Tobermory', 'Weymouth', 'Llandudno', 'Devonport', 'Sheerness', 'Whitby', 'Newlyn', 'Portpatrick', 'North Shields', 'Aberdeen', 'Lerwick', 'Newport', 'Bournemouth', \u0026quot;St. Mary's\u0026quot;, 'Liverpool, Gladstone Dock', 'Port Ellen (Islay)', 'Immingham', 'Newhaven', 'Moray Firth', 'Dover', 'Avonmouth', 'St. Helier (Jersey)', 'North Shields', 'Workington', 'North Shields', 'Portbury', 'Leith', 'Lowestoft', 'Fishguard', 'Bangor', 'Hinkley Point', 'Portsmouth', 'Stornoway', 'Millport', 'Port Erin', 'Portrush', 'Portbury', 'North Shields', 'Barmouth', 'Ilfracombe', 'North Shields', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A', 'N/A'], dtype=object)longitude(id_dim)float641.292 -3.975 -5.158 \u0026hellip; 350.8 8.717array([ 1.29210000e+00, -3.97544000e+00, -5.15789000e+00, -5.05148000e+00, -2.92042000e+00, -3.13433000e+00, -5.05036000e+00, -3.08631000e+00, -1.43978000e+00, 1.34839000e+00, -4.62044000e+00, 1.30164000e+00, -6.06422000e+00, -2.44794000e+00, -3.82522000e+00, -4.18525000e+00, 7.43440000e-01, -6.14170000e-01, -5.54283000e+00, -5.12003000e+00, -1.43978000e+00, -2.08013000e+00, -1.14031000e+00, -2.98744000e+00, -1.87486000e+00, -6.31642000e+00, -3.01800000e+00, -6.19006000e+00, -1.86030000e-01, 5.70300000e-02, -4.00220000e+00, 1.32267000e+00, -2.71497000e+00, -2.11667000e+00, -1.43978000e+00, -3.56764000e+00, -1.43978000e+00, -2.72848000e+00, -3.18169000e+00, 1.75083000e+00, -4.98333000e+00, -5.66947000e+00, -3.13433000e+00, -1.11175000e+00, -6.38889000e+00, -4.90583000e+00, -4.76806000e+00, -6.65683000e+00, -2.72848000e+00, -1.43978000e+00, -4.04517000e+00, -4.11094000e+00, -1.43978000e+00, 1.12150002e+01, 1.18000002e+01, 5.11700010e+00, 1.12500000e+01, 3.52666992e+02, 7.56699991e+00, 3.50816986e+02, 8.71700001e+00])latitude(id_dim)float6451.95 51.57 57.9 \u0026hellip; 51.53 53.87array([51.94798 , 51.57 , 57.89525 , 51.7064 , 54.03167 , 51.21525 , 58.45661 , 58.44097 , 55.00744 , 51.95675 , 53.31394 , 52.93436 , 56.62311 , 50.6085 , 53.33167 , 50.36839 , 51.44564 , 54.49008 , 50.103 , 54.84256 , 55.00744 , 57.14406 , 60.15403 , 51.55 , 50.71433 , 49.91847 , 53.44969 , 55.62742 , 53.63103 , 50.78178 , 57.5987 , 51.11439 , 51.51089 , 49.18333 , 55.00744 , 54.65081 , 55.00744 , 51.50002 , 55.98983 , 52.473 , 52.01378 , 54.66475 , 51.21525 , 50.80256 , 58.20711 , 55.74964 , 54.08539 , 55.20678 , 51.50002 , 55.00744 , 52.71906 , 51.21097 , 55.00744 , 58.34999847, 57.68299866, 61.93299866, 64.86699677, 55.36700058, 58. , 51.53300095, 53.86700058])Data variables: (3)diff_ntr(id_dim, t_dim)float64nan nan nan \u0026hellip; 0.05885 0.3435array([[ nan, nan, nan, \u0026hellip;, -0.32699312, -0.12564265, 0.13424107], [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, 0.27731735, 0.3287257 , 0.39544093], \u0026hellip;, [ nan, nan, nan, \u0026hellip;, -0.13998686, -0.18434602, -0.23884966], [ nan, nan, nan, \u0026hellip;, 0.09161719, 0.03131287, -0.05355329], [ nan, nan, nan, \u0026hellip;, -0.16968976, 0.05885439, 0.34348952]])abs_diff_ntr(id_dim, t_dim)float64nan nan nan \u0026hellip; 0.05885 0.3435array([[ nan, nan, nan, \u0026hellip;, 0.32699312, 0.12564265, 0.13424107], [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, 0.27731735, 0.3287257 , 0.39544093], \u0026hellip;, [ nan, nan, nan, \u0026hellip;, 0.13998686, 0.18434602, 0.23884966], [ nan, nan, nan, \u0026hellip;, 0.09161719, 0.03131287, 0.05355329], [ nan, nan, nan, \u0026hellip;, 0.16968976, 0.05885439, 0.34348952]])square_diff_ntr(id_dim, t_dim)float64nan nan nan \u0026hellip; 0.003464 0.118array([[ nan, nan, nan, \u0026hellip;, 0.1069245 , 0.01578608, 0.01802066], [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, 0.07690491, 0.10806059, 0.15637353], \u0026hellip;, [ nan, nan, nan, \u0026hellip;, 0.01959632, 0.03398346, 0.05704916], [ nan, nan, nan, \u0026hellip;, 0.00839371, 0.0009805 , 0.00286796], [ nan, nan, nan, \u0026hellip;, 0.02879461, 0.00346384, 0.11798505]])Indexes: (1)timePandasIndexPandasIndex(DatetimeIndex(['2007-01-01 00:00:00', '2007-01-01 01:00:00', '2007-01-01 02:00:00', '2007-01-01 03:00:00', '2007-01-01 04:00:00', '2007-01-01 05:00:00', '2007-01-01 06:00:00', '2007-01-01 07:00:00', '2007-01-01 08:00:00', '2007-01-01 09:00:00', \u0026hellip; '2007-01-30 14:00:00', '2007-01-30 15:00:00', '2007-01-30 16:00:00', '2007-01-30 17:00:00', '2007-01-30 18:00:00', '2007-01-30 19:00:00', '2007-01-30 20:00:00', '2007-01-30 21:00:00', '2007-01-30 22:00:00', '2007-01-30 23:00:00'], dtype='datetime64[ns]', name='time', length=720, freq=None))Attributes: (0)  We can then easily get mean errors, MAE and MSE\nmean_stats = ntr_diff.dataset.mean(dim=\u0026#34;t_dim\u0026#34;, skipna=True) ","excerpt":"This tutorial gives an overview of some of validation tools available when using the Tidegauge …","ref":"/COAsT/docs/examples/notebooks/tidegauge/tidegauge_validation_tutorial/","title":"Tidegauge validation tutorial"},{"body":"Tutorial for processing tabulated tide gauge data. Tidal highs and lows can be scraped from a website such as:\nhttps://www.ntslf.org/tides/tidepred?port=Liverpool\nand format them into a csv file:\nLIVERPOOL (GLADSTONE DOCK) TZ: UT(GMT)/BST Units: METRES Datum: Chart Datum\n01/10/2020 06:29 1.65\n01/10/2020 11:54 9.01\n01/10/2020 18:36 1.87\nThe data can be used in the following demonstration.\nimport coast import numpy as np Load and plot High and Low Water data.\nprint(\u0026#34;load and plot HLW data\u0026#34;) filnam = \u0026#34;./example_files/Gladstone_2020-10_HLW.txt\u0026#34; load and plot HLW data  Set the start and end dates.\ndate_start = np.datetime64(\u0026#34;2020-10-12 23:59\u0026#34;) date_end = np.datetime64(\u0026#34;2020-10-14 00:01\u0026#34;) Initiate a TideGauge object, if a filename is passed it assumes it is a GESLA type object.\ntg = coast.Tidegauge() Tidegauge object at 0x55d4e8c40fc0 initialised  Specify the data read as a High Low Water dataset.\ntg.read_hlw(filnam, date_start, date_end) Show dataset. If timezone is specified then it is presented as requested, otherwise uses UTC.\nprint(\u0026#34;Try the TideGauge.show() method:\u0026#34;) tg.show(timezone=\u0026#34;Europe/London\u0026#34;) Try the TideGauge.show() method:  Do a basic plot of these points.\ntg.dataset.plot.scatter(x=\u0026#34;time\u0026#34;, y=\u0026#34;ssh\u0026#34;) \u0026lt;matplotlib.collections.PathCollection at 0x7f3b64748be0\u0026gt;  There is a method to locate HLW events around an approximate date and time. First state the time of interest.\ntime_guess = np.datetime64(\u0026#34;2020-10-13 12:48\u0026#34;) Then recover all the HLW events in a +/- window, of specified size (iteger hrs). The default winsize = 2 (hrs).\nHLW = tg.get_tide_table_times(np.datetime64(\u0026#34;2020-10-13 12:48\u0026#34;), method=\u0026#34;window\u0026#34;, winsize=24) Alternatively recover the closest HLW event to the input timestamp.\nHLW = tg.get_tide_table_times(np.datetime64(\u0026#34;2020-10-13 12:48\u0026#34;), method=\u0026#34;nearest_1\u0026#34;) Or the nearest two events to the input timestamp.\nHLW = tg.get_tide_table_times(np.datetime64(\u0026#34;2020-10-13 12:48\u0026#34;), method=\u0026#34;nearest_2\u0026#34;) Extract the Low Tide value.\nprint(\u0026#34;Try the TideGauge.get_tidetabletimes() methods:\u0026#34;) print(\u0026#34;LT:\u0026#34;, HLW[np.argmin(HLW.data)].values, \u0026#34;m at\u0026#34;, HLW[np.argmin(HLW.data)].time.values) Try the TideGauge.get_tidetabletimes() methods: LT: 2.83 m at 2020-10-13T14:36:00.000000000  Extract the High Tide value.\nprint(\u0026#34;HT:\u0026#34;, HLW[np.argmax(HLW.data)].values, \u0026#34;m at\u0026#34;, HLW[np.argmax(HLW.data)].time.values) HT: 8.01 m at 2020-10-13T07:59:00.000000000  Or use the the nearest High Tide method to get High Tide.\nHT = tg.get_tide_table_times(np.datetime64(\u0026#34;2020-10-13 12:48\u0026#34;), method=\u0026#34;nearest_HW\u0026#34;) print(\u0026#34;HT:\u0026#34;, HT.values, \u0026#34;m at\u0026#34;, HT.time.values) HT: [8.01] m at 2020-10-13T07:59:00.000000000  The get_tidetabletimes() method can take extra paremeters such as a window size, an integer number of hours to seek either side of the guess.\nHLW = tg.get_tide_table_times(np.datetime64(\u0026#34;2020-10-13 12:48\u0026#34;), winsize=2, method=\u0026#34;nearest_1\u0026#34;) HLW = tg.get_tide_table_times(np.datetime64(\u0026#34;2020-10-13 12:48\u0026#34;), winsize=1, method=\u0026#34;nearest_1\u0026#34;) ","excerpt":"Tutorial for processing tabulated tide gauge data. Tidal highs and lows can be scraped from a …","ref":"/COAsT/docs/examples/notebooks/tidegauge/tidetable_tutorial/","title":"Tidetable tutorial"},{"body":"This is a demonstration script for using the Transect class in the COAsT package. This object has strict data formatting requirements, which are outlined in tranect.py.\nTransect subsetting (a vertical slice of data between two coordinates): Creating them and performing some custom diagnostics with them.\nIn this tutorial we take a look at subsetting the model data along a transect (a custom straight line) and creating some bespoke diagnostics along it. We look at:\n1. Creating a TRANSECT object, defined between two points. 2. Plotting data along a transect. 3. Calculating flow normal to the transect  Import relevant packages import coast import matplotlib.pyplot as plt Define filepaths for data and configuration root = \u0026#34;./\u0026#34; # And by defining some file paths dn_files = root + \u0026#34;./example_files/\u0026#34; fn_nemo_dat_t = dn_files + \u0026#34;nemo_data_T_grid.nc\u0026#34; fn_nemo_dat_u = dn_files + \u0026#34;nemo_data_U_grid.nc\u0026#34; fn_nemo_dat_v = dn_files + \u0026#34;nemo_data_V_grid.nc\u0026#34; fn_nemo_dom = dn_files + \u0026#34;coast_example_nemo_domain.nc\u0026#34; # Configuration files describing the data files fn_config_t_grid = root + \u0026#34;./config/example_nemo_grid_t.json\u0026#34; fn_config_f_grid = root + \u0026#34;./config/example_nemo_grid_f.json\u0026#34; fn_config_u_grid = root + \u0026#34;./config/example_nemo_grid_u.json\u0026#34; fn_config_v_grid = root + \u0026#34;./config/example_nemo_grid_v.json\u0026#34; Load data variables that are on the NEMO t-grid nemo_t = coast.Gridded(fn_data=fn_nemo_dat_t, fn_domain=fn_nemo_dom, config=fn_config_t_grid) Now create a transect using the coast.TransectT object. The transect is between the points (54 N 15 W) and (56 N, 12 W). This needs to be passed the corresponding NEMO object and transect end points. The model points closest to these coordinates will be selected as the transect end points.\ntran_t = coast.TransectT(nemo_t, (54, -15), (56, -12)) # Inspect the data #tran_t.data # uncomment to print data object summary Plot the data # It is simple to plot a scalar such as temperature along the transect: temp_mean = tran_t.data.temperature.mean(dim=\u0026#34;t_dim\u0026#34;) plt.figure() temp_mean.plot.pcolormesh(y=\u0026#34;depth_0\u0026#34;, yincrease=False) plt.show() Create a nemo f-grid object With NEMO’s staggered grid, the first step is to define the transect on the f-grid so that the velocity components are between f-points. We do not need any model data on the f-grid, just the grid information, so create a nemo f-grid object\nnemo_f = coast.Gridded(fn_domain=fn_nemo_dom, config=fn_config_f_grid) Transect on the f-grid tran_f = coast.TransectF(nemo_f, (54, -15), (56, -12)) # Inspect the data #tran_f.data # uncomment to print data object summary Load model data on the u- and v- grids nemo_u = coast.Gridded(fn_data=fn_nemo_dat_u, fn_domain=fn_nemo_dom, config=fn_config_u_grid) nemo_v = coast.Gridded(fn_data=fn_nemo_dat_v, fn_domain=fn_nemo_dom, config=fn_config_v_grid) Calculate the flow across the transect tran_f.calc_flow_across_transect(nemo_u, nemo_v) # The flow across the transect is stored in a new dataset where the variables are all defined at the points between f-points. #tran_f.data_cross_tran_flow # uncomment to print data object summary Plot the time averaged velocity across the transect # To do this we can plot the ‘normal_velocities’ variable. cross_velocity_mean = tran_f.data_cross_tran_flow.normal_velocities.mean(dim=\u0026#34;t_dim\u0026#34;) plt.figure() cross_velocity_mean.rolling(r_dim=2).mean().plot.pcolormesh(yincrease=False, y=\u0026#34;depth_0\u0026#34;, cbar_kwargs={\u0026#34;label\u0026#34;: \u0026#34;m/s\u0026#34;}) plt.show() Plot volume transport across the transect # To do this we can plot the ‘normal_transports’ variable. plt.figure() cross_transport_mean = tran_f.data_cross_tran_flow.normal_transports.mean(dim=\u0026#34;t_dim\u0026#34;) cross_transport_mean.rolling(r_dim=2).mean().plot() plt.ylabel(\u0026#34;Sv\u0026#34;) plt.show() ","excerpt":"This is a demonstration script for using the Transect class in the COAsT package. This object has …","ref":"/COAsT/docs/examples/notebooks/gridded/transect_tutorial/","title":"Transect tutorial"},{"body":"An example of using COAsT to analysis observational profile data alongside gridded NEMO data.\nLoad modules import coast import glob # For getting file paths import gsw import matplotlib.pyplot as plt import datetime import numpy as np import xarray as xr import coast._utils.general_utils as general_utils import scipy as sp # ====================== UNIV PARAMS =========================== path_examples = \u0026#34;./example_files/\u0026#34; path_config = \u0026#34;./config/\u0026#34; load and preprocess profile and model data fn_wod_var = path_examples + \u0026#34;WOD_example_ragged_standard_level.nc\u0026#34; fn_wod_config = path_config + \u0026#34;example_wod_profiles.json\u0026#34; wod_profile_1d = coast.Profile(config=fn_wod_config) wod_profile_1d.read_wod(fn_wod_var) ./config/example_wod_profiles.json  Reshape into 2D. Choose which observed variables you want.\nvar_user_want = [\u0026#34;salinity\u0026#34;, \u0026#34;temperature\u0026#34;, \u0026#34;nitrate\u0026#34;, \u0026#34;oxygen\u0026#34;, \u0026#34;dic\u0026#34;, \u0026#34;phosphate\u0026#34;, \u0026#34;alkalinity\u0026#34;] wod_profile = coast.Profile.reshape_2d(wod_profile_1d, var_user_want) Depth OK reshape successful salinity observed variable exist OK reshape successful temperature observed variable exist OK reshape successful nitrate variable not in observations oxygen observed variable exist OK reshape successful dic observed variable exist OK reshape successful phosphate observed variable exist OK reshape successful alkalinity variable not in observations  Keep subset.\nwod_profile_sub = wod_profile.subset_indices_lonlat_box(lonbounds=[90, 120], latbounds=[-5, 5]) #wod_profile_sub.dataset # uncomment to print data object summary SEAsia read BGC. Note in this simple test nemo data are only for 3 months from 1990 so the comparisons are not going to be correct but just as a demo.\nfn_seasia_domain = path_examples + \u0026#34;coast_example_domain_SEAsia.nc\u0026#34; fn_seasia_config_bgc = path_config + \u0026#34;example_nemo_bgc.json\u0026#34; fn_seasia_var = path_examples + \u0026#34;coast_example_SEAsia_BGC_1990.nc\u0026#34; seasia_bgc = coast.Gridded( fn_data=fn_seasia_var, fn_domain=fn_seasia_domain, config=fn_seasia_config_bgc, multiple=True ) Domain file does not have mask so this is just a trick.\nseasia_bgc.dataset[\u0026#34;landmask\u0026#34;] = seasia_bgc.dataset.bottom_level == 0 seasia_bgc.dataset = seasia_bgc.dataset.rename({\u0026#34;depth_0\u0026#34;: \u0026#34;depth\u0026#34;}) model_profiles = wod_profile_sub.obs_operator(seasia_bgc) #model_profiles.dataset # uncomment to print data object summary Remove any points that are far from model.\ntoo_far = 5 keep_indices = model_profiles.dataset.interp_dist \u0026lt;= too_far model_profiles = model_profiles.isel(id_dim=keep_indices) wod_profile = wod_profile_sub.isel(id_dim=keep_indices) #wod_profile.dataset # uncomment to print data object summary Plot profiles Transform observed DIC from mmol/l to mmol C/ m^3 that the model has.\nfig = plt.figure() plt.plot(1000 * wod_profile.dataset.dic[8, :], wod_profile.dataset.depth[8, :], linestyle=\u0026#34;\u0026#34;, marker=\u0026#34;o\u0026#34;) plt.plot(model_profiles.dataset.dic[8, :], model_profiles.dataset.depth[:, 8], linestyle=\u0026#34;\u0026#34;, marker=\u0026#34;o\u0026#34;) plt.ylim([2500, 0]) plt.title(\u0026#34;DIC vs depth\u0026#34;) plt.show() fig = plt.figure() plt.plot(wod_profile.dataset.oxygen[8, :], wod_profile.dataset.depth[8, :], linestyle=\u0026#34;\u0026#34;, marker=\u0026#34;o\u0026#34;) plt.plot(model_profiles.dataset.oxygen[8, :], model_profiles.dataset.depth[:, 8], linestyle=\u0026#34;\u0026#34;, marker=\u0026#34;o\u0026#34;) plt.ylim([2500, 0]) plt.title(\u0026#34;Oxygen vs depth\u0026#34;) plt.show() Perform profile analysis to evaluate differences Interpolate seasia to profile depths, using ProfileAnalysis class.\nreference_depths = wod_profile.dataset.depth[20, :].values model_profiles.dataset = model_profiles.dataset[[\u0026#34;dic\u0026#34;]] / 1000 pa = coast.ProfileAnalysis() model_interpolated = pa.interpolate_vertical(model_profiles, wod_profile) Calculate differences.\ndifferences = pa.difference(model_interpolated, wod_profile) #differences.dataset.load() # uncomment to print data object summary ","excerpt":"An example of using COAsT to analysis observational profile data alongside gridded NEMO data.\nLoad …","ref":"/COAsT/docs/examples/notebooks/profile/wod_bgc_ragged_example/","title":"Wod bgc ragged example"},{"body":"Intro Remote access to Copernicus Marine Environment Monitoring Service CMEMS datasets is enabled via OPeNDAP and Pydap.\nOPeNDAP allows COAsT to stream data from Copernicus without downloading specific subsets or the dataset as a whole.\nIn order to access CMEMS data, you must first create an account.\nAfter you have created your account, or if you already have one, a product ID can be selected from the product catalogue.\nExample import coast # Authenticate with Copernicus and select a database. database = coast.Copernicus(USERNAME, PASSWORD, \u0026#34;nrt\u0026#34;) # Instantiate a product with its ID forecast = database.get_product(\u0026#34;global-analysis-forecast-phy-001-024\u0026#34;) # Create a COAsT object with the relevant config file nemo_t = coast.Gridded(fn_data=forecast, config=\u0026#34;./config/example_cmems_grid_t.json\u0026#34;) Look inside the COAsT gridded object: nemo_t.dataset\n\u0026lt;xarray.Dataset\u0026gt; Dimensions: (x_dim: 4320, y_dim: 2041, z_dim: 50, t_dim: 912) Coordinates: longitude (x_dim) float32 -180.0 -179.9 -179.8 ... 179.8 179.8 179.9 latitude (y_dim) float32 -80.0 -79.92 -79.83 -79.75 ... 89.83 89.92 90.0 * z_dim (z_dim) float32 0.494 1.541 2.646 ... 5.275e+03 5.728e+03 time (t_dim) datetime64[ns] 2020-01-01T12:00:00 ... 2022-06-30T12... Dimensions without coordinates: x_dim, y_dim, t_dim Data variables: mlotst (t_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 2041, 4320), meta=np.ndarray\u0026gt; ssh (t_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 2041, 4320), meta=np.ndarray\u0026gt; bottomT (t_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 2041, 4320), meta=np.ndarray\u0026gt; sithick (t_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 2041, 4320), meta=np.ndarray\u0026gt; siconc (t_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 2041, 4320), meta=np.ndarray\u0026gt; usi (t_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 2041, 4320), meta=np.ndarray\u0026gt; vsi (t_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 2041, 4320), meta=np.ndarray\u0026gt; temperature (t_dim, z_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 50, 2041, 4320), meta=np.ndarray\u0026gt; salinity (t_dim, z_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 50, 2041, 4320), meta=np.ndarray\u0026gt; uo (t_dim, z_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 50, 2041, 4320), meta=np.ndarray\u0026gt; vo (t_dim, z_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 50, 2041, 4320), meta=np.ndarray\u0026gt; Attributes: (12/24) title: daily mean fields from Global Ocean Physics Analysis ... easting: longitude northing: latitude history: 2022/06/21 00:05:41 MERCATOR OCEAN Netcdf creation source: MERCATOR PSY4QV3R1 institution: MERCATOR OCEAN ... longitude_min: -180.0 longitude_max: 179.91667 latitude_min: -80.0 latitude_max: 90.0 z_min: 0.494025 z_max: 5727.917 Or plot a snapshot of surface temperature. (This lazy loaded so may take time to render)\nimport matplotlib.pyplot as plt plt.pcolormesh( nemo_t.dataset.temperature.isel(t_dim=1,z_dim=1)) plt.show() ","excerpt":"Intro Remote access to Copernicus Marine Environment Monitoring Service CMEMS datasets is enabled …","ref":"/COAsT/docs/examples/remote-datasets/copernicus/","title":"Copernicus"},{"body":"","excerpt":"","ref":"/COAsT/docs/examples/remote-datasets/","title":"Remote Datasets"},{"body":"A short script to install COAsT in a conda environment, download and run some build tests.\n# Fresh build module load anaconda/3-5.1.0 # or whatever it takes to activate conda yes | conda env remove --name test_env yes | conda create -n test_env python=3.8 # create a new environment conda activate test_env yes | conda install -c conda-forge -c bodc coast yes | conda install -c conda-forge cartopy=0.18.0 # used for some of the map plotting # Download bits and bobs rm -rf coast_test mkdir coast_test cd coast_test git clone https://github.com/British-Oceanographic-Data-Centre/COAsT.git wget -c https://linkedsystems.uk/erddap/files/COAsT_example_files/COAsT_example_files.zip \u0026amp;\u0026amp; unzip COAsT_example_files.zip ln -s COAsT/unit_testing/ . ln -s COAsT_example_files example_files # Run unit tests python COAsT/unit_testing/unit_test.py \u0026gt; coast_test.txt ## If OK then clean up cd .. rm -rf coast_test Or, trialling a new (Oct 2022) workflow which seems to dig deeper with useful feedback\n# create a new conda env with: conda env update --prune --file environment.yml # run the unit tests with: pip install . \u0026amp;\u0026amp; pytest unit_testing/unit_test.py -s ","excerpt":"A short script to install COAsT in a conda environment, download and run some build tests.\n# Fresh …","ref":"/COAsT/docs/contributing_package/build_test/","title":"Build test"},{"body":"To date the workflow has been to unit test anything and everything that goes into the develop branch and then periodically push to master less frequently and issue a new github release.\nWith the push to master Git Actions build the conda and pip packages and the package receives a zenodo update (https://zenodo.org/account/settings/github/repository/British-Oceanographic-Data-Centre/COAsT) and DOI.\n1. Push to master Any push to master initiates the Git Actions to build and release the package. It is advisable then to prepare the release in develop and only ever pull into master from develop. (Pulling from master to develop could bring unexpected Git Actions to develop). In order for the package builds to work the version of the package must be unique. The version of the package is set in file setup.py. E.g. shown as 2.0.1 below:\n# setup.py ... PACKAGE = SimpleNamespace(**{ \u0026#34;name\u0026#34;: \u0026#34;COAsT\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;2.0.1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;This is the Coast Ocean Assessment Tool\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://www.bodc.ac.uk\u0026#34;, \u0026#34;download_url\u0026#34;: \u0026#34;https://github.com/British-Oceanographic-Data-Centre/COAsT/\u0026#34;, .... Package version also appears in CITATION.cff file, which therefore also needs updating. E.g.:\n... title: British-Oceanographic-Data-Centre/COAsT: v2.0.1 version: v2.0.1 date-released: 2022-04-07 Version numbering follows the semantic versioning convention. Briefly, given a version number MAJOR.MINOR.PATCH, increment the:\n MAJOR version when you make incompatible API changes, MINOR version when you add functionality in a backwards compatible manner, and PATCH version when you make backwards compatible bug fixes. Additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format.  2. Issue new release Then issue a new release, with the new version label, and annotate the major changes.\n","excerpt":"To date the workflow has been to unit test anything and everything that goes into the develop branch …","ref":"/COAsT/docs/contributing_package/push_to_master/","title":"Push to master"},{"body":"AMM15 - 1.5km resolution Atlantic Margin Model \u0026#34;\u0026#34;\u0026#34; AMM15_example_plot.py Make simple AMM15 SST plot. \u0026#34;\u0026#34;\u0026#34; #%% import coast import numpy as np import xarray as xr import matplotlib.pyplot as plt import matplotlib.colors as colors # colormap fiddling ################################################# #%% Loading data ################################################# config = \u0026#39;AMM15\u0026#39; dir_nam = \u0026#34;/projectsa/NEMO/gmaya/2013p2/\u0026#34; fil_nam = \u0026#34;20130415_25hourm_grid_T.nc\u0026#34; dom_nam = \u0026#34;/projectsa/NEMO/gmaya/AMM15_GRID/amm15.mesh_mask.cs3x.nc\u0026#34; config = \u0026#34;/work/jelt/GitHub/COAsT/config/example_nemo_grid_t.json\u0026#34; sci_t = coast.Gridded(dir_nam + fil_nam, dom_nam, config=config) # , chunks=chunks) chunks = { \u0026#34;x_dim\u0026#34;: 10, \u0026#34;y_dim\u0026#34;: 10, \u0026#34;t_dim\u0026#34;: 10, } # Chunks are prescribed in the config json file, but can be adjusted while the data is lazy loaded. sci_t.dataset.chunk(chunks) # create an empty w-grid object, to store stratification sci_w = coast.Gridded(fn_domain=dom_nam, config=config.replace(\u0026#34;t_nemo\u0026#34;, \u0026#34;w_nemo\u0026#34;)) sci_w.dataset.chunk({\u0026#34;x_dim\u0026#34;: 10, \u0026#34;y_dim\u0026#34;: 10}) # Can reset after loading config json print(\u0026#39;* Loaded \u0026#39;,config, \u0026#39; data\u0026#39;) ################################################# #%% subset of data and domain ## ################################################# # Pick out a North Sea subdomain print(\u0026#39;* Extract North Sea subdomain\u0026#39;) ind_sci = sci_t.subset_indices([51,-4], [62,15]) sci_nwes_t = sci_t.isel(y_dim=ind_sci[0], x_dim=ind_sci[1]) #nwes = northwest europe shelf ind_sci = sci_w.subset_indices([51,-4], [62,15]) sci_nwes_w = sci_w.isel(y_dim=ind_sci[0], x_dim=ind_sci[1]) #nwes = northwest europe shelf #%% Apply masks to temperature and salinity if config == \u0026#39;AMM15\u0026#39;: sci_nwes_t.dataset[\u0026#39;temperature_m\u0026#39;] = sci_nwes_t.dataset.temperature.where( sci_nwes_t.dataset.mask.expand_dims(dim=sci_nwes_t.dataset[\u0026#39;t_dim\u0026#39;].sizes) \u0026gt; 0) sci_nwes_t.dataset[\u0026#39;salinity_m\u0026#39;] = sci_nwes_t.dataset.salinity.where( sci_nwes_t.dataset.mask.expand_dims(dim=sci_nwes_t.dataset[\u0026#39;t_dim\u0026#39;].sizes) \u0026gt; 0) else: # Apply fake masks to temperature and salinity sci_nwes_t.dataset[\u0026#39;temperature_m\u0026#39;] = sci_nwes_t.dataset.temperature sci_nwes_t.dataset[\u0026#39;salinity_m\u0026#39;] = sci_nwes_t.dataset.salinity #%% Plots fig = plt.figure() plt.pcolormesh( sci_t.dataset.longitude, sci_t.dataset.latitude, sci_t.dataset.temperature.isel(z_dim=0).squeeze()) #plt.xlabel(\u0026#39;longitude\u0026#39;) #plt.ylabel(\u0026#39;latitude\u0026#39;) #plt.colorbar() plt.axis(\u0026#39;off\u0026#39;) plt.show() fig.savefig(\u0026#39;AMM15_SST_nocolorbar.png\u0026#39;, dpi=120)    India subcontinent maritime domain. WCSSP India configuration #%% import coast import numpy as np import xarray as xr import dask import matplotlib.pyplot as plt import matplotlib.colors as colors # colormap fiddling ################################################# #%% Loading data ################################################# dir_nam = \u0026#34;/projectsa/COAsT/NEMO_example_data/MO_INDIA/\u0026#34; fil_nam = \u0026#34;ind_1d_cat_20180101_20180105_25hourm_grid_T.nc\u0026#34; dom_nam = \u0026#34;domain_cfg_wcssp.nc\u0026#34; config_t = \u0026#34;/work/jelt/GitHub/COAsT/config/example_nemo_grid_t.json\u0026#34; sci_t = coast.Gridded(dir_nam + fil_nam, dir_nam + dom_nam, config=config_t) #%% Plot fig = plt.figure() plt.pcolormesh( sci_t.dataset.longitude, sci_t.dataset.latitude, sci_t.dataset.temperature.isel(t_dim=0).isel(z_dim=0)) plt.xlabel(\u0026#39;longitude\u0026#39;) plt.ylabel(\u0026#39;latitude\u0026#39;) plt.title(\u0026#39;WCSSP India SST\u0026#39;) plt.colorbar() plt.show() fig.savefig(\u0026#39;WCSSP_India_SST.png\u0026#39;, dpi=120)    South East Asia, 1/12 deg configuration (ACCORD: SEAsia_R12) #%% import coast import numpy as np import xarray as xr import dask import matplotlib.pyplot as plt import matplotlib.colors as colors # colormap fiddling ################################################# #%% Loading data ################################################# dir_nam = \u0026#34;/projectsa/COAsT/NEMO_example_data/SEAsia_R12/\u0026#34; fil_nam = \u0026#34;SEAsia_R12_5d_20120101_20121231_gridT.nc\u0026#34; dom_nam = \u0026#34;domain_cfg_ORCA12_adj.nc\u0026#34; config_t = \u0026#34;/work/jelt/GitHub/COAsT/config/example_nemo_grid_t.json\u0026#34; sci_t = coast.Gridded(dir_nam + fil_nam, dir_nam + dom_nam, config=config_t) #%% Plot fig = plt.figure() plt.pcolormesh( sci_t.dataset.longitude, sci_t.dataset.latitude, sci_t.dataset.soce.isel(t_dim=0).isel(z_dim=0)) plt.xlabel(\u0026#39;longitude\u0026#39;) plt.ylabel(\u0026#39;latitude\u0026#39;) plt.title(\u0026#39;SE Asia, surface salinity (psu)\u0026#39;) plt.colorbar() plt.show() fig.savefig(\u0026#39;SEAsia_R12_SSS.png\u0026#39;, dpi=120)    ","excerpt":"AMM15 - 1.5km resolution Atlantic Margin Model \u0026#34;\u0026#34;\u0026#34; AMM15_example_plot.py Make simple …","ref":"/COAsT/docs/examples/configs_gallery/","title":"Configuration Gallery"},{"body":"__________________________________________________________________________________________ ______ ___ _ _________ .' ___ | .' `. / \\ | _ _ | / .' \\_|/ .-. \\ / _ \\ .--.|_/ | | \\_| | | | | | | / ___ \\ ( (`\\] | | \\ `.___.'\\\\ `-' /_/ / \\ \\_ `'.'. _| |_ `.____ .' `.___.'|____| |____|[\\__) )|_____| Coastal Ocean Assessment Toolbox __________________________________________________________________________________________ COAsT is a Python package for managing and analysing high resolution NEMO output. Here you can find information on obtaining, installing and using COAsT as well as guidelines for contributing to the project.\nThis documentation site is still under construction but you can still find guidelines for contributing to the package and this website. See below for description of each section.\n","excerpt":"__________________________________________________________________________________________ ______ …","ref":"/COAsT/docs/","title":"Documentation"},{"body":"Objects setup_dask_client()\nIndexed()\nIndexed.apply_config_mappings()\nIndexed.insert_dataset()\nIndex class.\nsetup_dask_client() def setup_dask_client(workers=2, threads=2, memory_limit_per_worker=2GB):  \nNone\n Indexed() class Indexed(Coast): None Indexed.apply_config_mappings() def Indexed.apply_config_mappings(self):  \nApplies json configuration and mappings\n Indexed.insert_dataset() def Indexed.insert_dataset(self, dataset, apply_config_mappings=False):  \nInsert a dataset straight into this object instance\n ","excerpt":"Objects setup_dask_client()\nIndexed()\nIndexed.apply_config_mappings()\nIndexed.insert_dataset()\nIndex …","ref":"/COAsT/docs/reference/","title":"Index"},{"body":"  #td-cover-block-0 { background-image: url(/COAsT/about/featured-background_hu14d69772da4446f8c45afbc4cad362c8_132726_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/COAsT/about/featured-background_hu14d69772da4446f8c45afbc4cad362c8_132726_1920x1080_fill_q75_catmullrom_top.jpg); } }  About COAsT A site using the Docsy Hugo theme. --        COAsT is a Python package for managing and analysing high resolution NEMO output Read more here     This site was based off the Docsy Hugo theme.    ","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/COAsT/about/","title":"About Goldydocs"},{"body":"  #td-cover-block-0 { background-image: url(/COAsT/featured-background_hu14d69772da4446f8c45afbc4cad362c8_132726_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/COAsT/featured-background_hu14d69772da4446f8c45afbc4cad362c8_132726_1920x1080_fill_q75_catmullrom_top.jpg); } }  Welcome to the documentation: A Docsy site for COAsT Learn More   Download   COAsT\n\n        This site provides visibility into the COAsT python framework.       Download from Anaconda.org Get the COAsT framework!\nRead more …\n   Contributions welcome! We do a Pull Request contributions workflow on GitHub. New users are always welcome!\nRead more …\n   Follow us on Twitter! For announcement of latest features etc.\nRead more …\n    ","excerpt":"#td-cover-block-0 { background-image: …","ref":"/COAsT/","title":"COAsT"},{"body":"","excerpt":"","ref":"/COAsT/community/","title":"Community"},{"body":"","excerpt":"","ref":"/COAsT/search/","title":"Search Results"}]