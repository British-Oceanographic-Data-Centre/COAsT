[{"body":"This script is designed to be a brief introduction to the Gridded class including:\n1. Creation of a Gridded object 2. Loading data into the Gridded object. 3. Combining Gridded output and Gridded domain data. 4. Interrogating the Gridded object. 5. Basic manipulation ans subsetting 6. Looking at the data with matplotlib  Up to date as of: 05/10/2021\nLoading and Interrogating Begin by importing COAsT and define some file paths for NEMO output data and a NEMO domain, as an example of model data suitable for the Gridded object.\nimport coast import matplotlib.pyplot as plt import datetime import numpy as np fn_nemo_dat = \u0026#34;./example_files/coast_example_nemo_data.nc\u0026#34; fn_nemo_dom = \u0026#34;./example_files/coast_example_nemo_domain.nc\u0026#34; fn_config_t_grid = \u0026#34;./config/example_nemo_grid_t.json\u0026#34; We can create a new Gridded object by simple calling coast.Gridded(). By passing this a NEMO data file and a NEMO domain file, COAsT will combine the two into a single xarray dataset within the Gridded object. Each individual Gridded object should be for a specified NEMO grid type, which is specified in a configuration file which is also passed as an argument. The Dask library is switched on by default, chunking can be specified in the configuration file.\nnemo_t = coast.Gridded(fn_data = fn_nemo_dat, fn_domain = fn_nemo_dom, config=fn_config_t_grid) Our new Gridded object nemo_t contains a variable called dataset, which holds information on the two files we passed. Let\u0026rsquo;s have a look at this:\nnemo_t.dataset This is an xarray dataset, which has all the information on netCDF style structures. You can see dimensions, coordinates and data variables. At the moment, none of the actual data is loaded to memory and will remain that way until it needs to be accessed.\nAlong with temperature (which has been renamed from votemper) a number of other things have happen under the hood:\n1. The dimensions have been renamed to `t_dim`, `x_dim`, `y_dim`, `z_dim` 2. The coordinates have been renamed to `time`, `longitude`, `latitude` and `depth_0`. These are the coordinates for this grid (the t-grid). Also `depth_0` has been calculated as the 3D depth array at time zero. 3. The variables `e1`, `e2` and `e3_0` have been created. These are the metrics for the t-grid in the x-dim, y-dim and z_dim (at time zero) directions.  So we see that the Gridded class has standardised some variable names and created an object based on this discretisation grid by combining the appropriate grid information with all the variables on that grid.\nWe can interact with this as an xarray Dataset object. So to extract a specific variable (say temperature):\nssh = nemo_t.dataset.ssh ssh Or as a numpy array:\nssh_np = ssh.values ssh_np.shape Then lets plot up a single time snapshot of ssh using matplotlib:\nplt.pcolormesh(nemo_t.dataset.longitude, nemo_t.dataset.latitude, nemo_t.dataset.ssh[0]) Some Manipulation There are currently some basic subsetting routines for Gridded objects, to cut out specified regions of data. Fundamentally, this can be done using xarray\u0026rsquo;s isel or sel routines to index the data. In this case, the Gridded object will pass arguments straight through to xarray.isel.\nLets get the indices of all model points within 111km km of (5W, 55N):\nind_y, ind_x = nemo_t.subset_indices_by_distance(centre_lon=-5, centre_lat=55, radius=111) ind_x.shape Now create a new, smaller subsetted Gridded object by passing those indices to isel.\nnemo_t_subset = nemo_t.isel(x_dim=ind_x, y_dim=ind_y) nemo_t_subset.dataset Alternatively, xarray.isel can be applied directly to the xarray.Dataset object.\nA longitude/latitude box of data can also be extracted using Gridded.subset_indices().\n","excerpt":"This script is designed to be a brief introduction to the Gridded class including:\n1. Creation of a …","ref":"/COAsT/docs/examples/intro_gridded_class/","title":"The Gridded class"},{"body":"COAsT (Coastal Ocean Assessment Toolkit) is a diagnostics and assessment python toolbox for kilometric scale regional models. The aim is that this toolbox is community-ready and flexible.\nThe initial focus will be on delivering a limited number of novel diagnostics for NEMO configurations, but that the toolbox would be expanded to include other diagnostics and other ocean models.\n","excerpt":"COAsT (Coastal Ocean Assessment Toolkit) is a diagnostics and assessment python toolbox for …","ref":"/COAsT/docs/overview/","title":"Overview"},{"body":"Python as a language comes with more stringent recommendations than most when it comes to code styling. This is advantageous in our case as it gives us an obvious set of guidelines to adopt.\nWhen it comes to simple code styling, much of what\u0026rsquo;s recommended here will be copied from Python Enhancement Proposal (PEP) 8, an officially proposed and accepted Python style guide.\nCode Styling Conventions Let\u0026rsquo;s keep things simple to start with\u0026hellip;\n  Indentation should be achieved with spaces rather than tabs and each new level of indentation should be indented by four columns (i.e four spaces).\n  Any single line, including its indentation characters, should not exceed 79 characters in length.\n  Top-level (i.e at the module/file level rather than inside a function or class) function and class definitions should be separated by two blank lines.\n  Method (functions within a class) definitions are separated by a single blank line.\n  Usually, \u0026ldquo;import\u0026rdquo; statements should be on separate lines, that is to say that you should have one line per distinct module or package import. An exception to this rule is when multiple objects are imported from a single module or package, using a \u0026ldquo;from\u0026rdquo; statement, in which case individual objects can be imported on the same line, separated by commas.\n  PEP 8 does not make a recommendation relating to the use of double or single quotes in general use, but for the sake of consistency, this document suggests the use of double quotes wherever practical. This recommendation is intended for the sake of consistency with triple-quoted strings, as per Docstring Conventions (PEP 257).\n  Operators should be separated by single columns (i.e one space) either side, unless inside parentheses, in which case no whitespace is required.\n  Comments (beginning with the # character) should be indented as if they were code. In the case of inline comments, separate the comment with two spaces following the code it shares the line with.\n  All functions should contain a docstring, which provides basic information on its usage. For this project, the reStructuredText docstring format is suggested.\n  When it comes to naming variables and functions, snake case (lower_case_words_separated_by_underscores) is preferred. There are however a few exceptions to this rule: Class names should be styled as camel case (EveryNewWordIsCapitalised). Constants (Variables that should not be changed) can be indicated by the use of screaming snake case (UPPER_CASE_WORDS_SEPARATED_BY_UNDERSCORES). Note that this library currently targets Python 3.7, so the use of typing.Final official support for constant variables, new as of Python 3.8: is not currently supported.\n  In general, it is suggested to avoid the use of single-character variable names, but this is acceptable in certain cases, such as when defining coordinates (such as x, y and z), as these will be commonly recognized and enforcing different rules could cause confusion. PEP 8 advises the following regarding names to avoid: \u0026ldquo;Never use the characters \u0026lsquo;l\u0026rsquo; (lowercase letter el), \u0026lsquo;O\u0026rsquo; (uppercase letter oh), or \u0026lsquo;I\u0026rsquo; (uppercase letter eye) as single character variable names.\u0026rdquo; These specific characters should be avoided because they present an accessibility issue, as under many fonts these characters may be difficult to distinguish or completely indistinguishable from numerals one (1) and zero (0).\n  In the interest of readability, where named iterator variables are required, this document suggests the use of double characters (e.g. \u0026ldquo;ii\u0026rdquo; rather than \u0026ldquo;i\u0026rdquo;).\n  Object-Oriented Programming The general principles of OOP are fairly straightforward and well documented, so I won\u0026rsquo;t waste your precious time by regurgitating that particular wall of text here. Instead, I\u0026rsquo;ll focus on some general pointers specific to this language and use case.\n  In Python, all class attributes are technically public, but semantically, attributes can be designated as non-public by including leading underscores in the name. For instance, \u0026ldquo;my_variable\u0026rdquo; becomes \u0026ldquo;_my_variable\u0026rdquo;. These attributes are generally referred to as \u0026ldquo;protected\u0026rdquo;.\n  When you define a Python class, it is a best practice to inherit from the base object type. This convention stems from Python 2.X, as classes and types were not originally synonymous. This behaviour is implicit in Python 3.X but the convention has persisted nonetheless. Classes defined this way are referred to as \u0026ldquo;new-style\u0026rdquo; classes.\n  When defining a class that inherits from another, it is important to remember that overridden methods (in particular, this behaviour is important when dealing with __init__ methods) do not implicitly call the parent method. What this means is that unless you want to deliberately prevent the behaviour of the parent class (this is a very niche use-case), it is important to include a reference to the parent method. An example of this is: super().__init__() This functionality is advantageous as it prevents unnecessary duplication of code, which is a key tenet of object-oriented software.\n  ","excerpt":"Python as a language comes with more stringent recommendations than most when it comes to code …","ref":"/COAsT/docs/contributing_package/python_style/","title":"Python: Style"},{"body":"** Notes on Object Structure and Loading (for contributors):\nCOAsT is an object-orientated package, meaning that data is stored within Python object structures. In addition to data storage, these objects contain methods (subroutines) which allow for manipulation of this data. An example of such an object is the Gridded object, which allows for the storage and manipulation of e.g. NEMO output and domain data. It is important to understand how to load data using COAsT and the structure of the resulting objects.\nA Gridded object is created and initialised by passing it the paths of the domain and data files. Ideally, the grid type should also be specified (T, U, V or F in the case of NEMO). For example, to load in data from a file containing data on a NEMO T-grid:\nimport coast fn_data = \u0026quot;\u0026lt;path to T-grid data file(s)\u0026gt;\u0026quot; fn_domain = \u0026quot;\u0026lt;path to domain file\u0026gt;\u0026quot; fn_config = \u0026quot;\u0026lt;path to json config file\u0026gt;\u0026quot; data = coast.Gridded(fn_data, fn_domain, fn_config) Ideally, Gridded model output data should be in grid-specific files, i.e. containing output variables situated on a NEMO T, U, V or F grid, whereas the grid variables are in a single domain file. On loading into COAsT, only the grid specific variables appropriate for the paired data are placed into the Gridded object. A Gridded object therefore contains grid-specific data and all corresponding grid variables. One of the file names can be omitted (to get a data-only or grid only object), however functionality in this case will be limited.\nOnce loaded, data is stored inside the object using an xarray.dataset object. Following on from the previous code example, this can be viewed by calling:\ndata.dataset This reveals all netcdf-type aspects of the data and domain variables that were loaded, including dimensions, coordinates, variables and attributes. For example:\n\u0026lt;xarray.Dataset\u0026gt; Dimensions: (axis_nbounds: 2, t_dim: 7, x_dim: 297, y_dim: 375, z_dim: 51) Coordinates: time (t_dim) datetime64[ns] 2007-01-01T11:58:56 ... 2007-01-31T11:58:56 longitude (y_dim, x_dim) float32 ... latitude (y_dim, x_dim) float32 ... Dimensions without coordinates: axis_nbounds, t_dim, x_dim, y_dim, z_dim Data variables: deptht_bounds (z_dim, axis_nbounds) float32 ... sossheig (t_dim, y_dim, x_dim) float32 ... time_counter_bounds (t_dim, axis_nbounds) datetime64[ns] ... time_instant (t_dim) datetime64[ns] ... temperature (t_dim, z_dim, y_dim, x_dim) float32 ... e1 (y_dim, x_dim) float32 ... e2 (y_dim, x_dim) float32 ... e3_0 (z_dim, y_dim, x_dim) float32 1.0 1.0 1.0 ... 1.0 1.0 Variables may be obtained in a number of ways. For example, to get temperature data, the following are all equivalent:\ntemp = data.dataset.temperature temp = data.dataset['temperature'] temp = data['temperature'] These commands will all return an xarray.dataarray object. Manipulation of this object can be done using xarray commands, for example indexing using [] or xarray.isel. Be aware that indexing will preserve lazy loading, however and direct access or modifying of the data will not. For this reason, if you require a subset of the data, it is best to index first.\nThe names of common grid variables are standardised within the COAsT package using JSON configuration files. For example, the following lists COAsT internal variable followed by the typical NEMO variable names:\n longitude [glamt / glamu / glamv / glamf] latitude [gphit / gphiu / gphiv / gphif] time [time_counter] e1 [e1t / e1u / e1v / e1f] (dx variable) e2 [e1t / e1u / e1v / e1f] (dy variable) e3_0 [e3t_0 / e3u_0 / e3v_0 / e3f_0] (dz variable at time 0)  Longitude, latitude and time are also set as coordinates. You might notice that dimensions are also standardised:\n x_dim The dimension for the x-axis (longitude) y_dim The dimension for the y-axis (latitude) t_dim The dimension for the time axis z_dim The dimension for the depth axis.  Wherever possible, the aim is to ensure that all of the above is consistent across the whole COAsT toolbox. Therefore, you will also find the same names and dimensions in, for example observation objects. Future objects, where applicable, will also follow these conventions. If you (as a contributor) add new objects to the toolbox, following the above template is strongly encouraged. This includes using xarray dataset/dataarray objects where possible, adopting an object oriented approach and adhering to naming conventions.\n","excerpt":"** Notes on Object Structure and Loading (for contributors):\nCOAsT is an object-orientated package, …","ref":"/COAsT/docs/contributing_package/python_structure/","title":"Python: Structure"},{"body":"Prerequisites This package requires;\n python version 3.7+ Anaconda version 3.7  Are there any system requirements for using your project? What languages are supported (if any)? Do users need to already have any software or tools installed?\nBasic use installation via conda or pip This package should be installed by run;\nconda install -c bodc coast However, there is also the option of;\npip install COAsT if you wish to install from source then got to GitHub and follow the README instructions\nThe base package should now be installed on your system. The following packages might be required for some of the advanced plotting features;\n cartopy  Development use installation If you would prefer to work with a clone of the repository in a development python environment do the following. First clone the repoitory in the place where you want to work:\ngit clone https://github.com/British-Oceanographic-Data-Centre/COAsT.git Then start building a python environment. Here (for example) called coast_dev:\nmodule load anaconda/5-2021 # or whatever it takes to activate conda conda config --add channels conda-forge # add conda-forge to your conda channels conda create -n coast_dev python=3.8 # create a new environment. E.g. `coast_dev` conda activate coast_dev # activate new environment Install packages to the environment:\ncd COAsT conda install --file conda_requirements.txt At the time of writing (06/10/2021) the contents of conda_requirements.txt was:\nless COAsT/conda_requirements.txt numpy\u0026gt;=1.16 dask\u0026gt;=2 dask[complete]\u0026gt;=2 xarray\u0026gt;=0.19 matplotlib\u0026gt;=3.4.3 netCDF4\u0026gt;=1 scipy\u0026gt;=1 gsw==3.3.1 utide\u0026gt;=0.2 scikit-learn\u0026gt;=0.2 scikit-image\u0026gt;=0.15 cartopy\u0026gt;=0.20.1 spyder\u0026gt;=4 Obtaining Example files In order to try the Examples, example data files and configuration files are recommended.\nExample data files Download example files and link them into a new directory:\nrm -rf coast_demo mkdir coast_demo cd coast_demo wget -c https://linkedsystems.uk/erddap/files/COAsT_example_files/COAsT_example_files.zip \u0026amp;\u0026amp; unzip COAsT_example_files.zip ln -s COAsT_example_files example_files Example configuration files To facilitate loading different types of data, key information is passed to COAsT using configuration files. The config files used in the Examples are in the repository, or can be downloaded as static files:\ncd ../coast_demo wget -c https://github.com/British-Oceanographic-Data-Centre/COAsT/archive/refs/heads/master.zip \u0026amp;\u0026amp; unzip COAsT-master.zip ln -s COAsT-master/config config Preparation for Workshop Package Installation with conda Assuming a linux environment and that you have anaconda on your system:\n## Fresh build in new conda environment module load anaconda/5-2021 # or whatever it takes to activate conda yes | conda env remove --name workshop_env # remove environment \u0026#39;workshop_env\u0026#39; if it exists yes | conda create --name workshop_env python=3.8 # create a new environment conda activate workshop_env # activate new environment yes | conda install -c bodc coast=1.2.7 # install COAsT within new environment yes | conda install -c conda-forge gsw=3.3.1 # enforce the GSW package number (something fishy with the build process bumped up this version number) yes | conda install -c conda-forge cartopy=0.20.1 # install cartopy Then obtain the Example data and configuration files (as above).\nTest it! The below example works best with the COAsT example data. Start by opening a python terminal and then importing COAsT:\nimport coast Before using coast, we will just check that Anaconda has installed correct package versions. In the python console copy the following:\nimport gsw import matplotlib print(gsw.__version__) print(matplotlib.__version__) The output should be\n3.3.1 3.4.3 If it is, great carry on. If it is not, problems may occur with some functionality in coast. Please get in contact using the contacts in the workshop email.\nTake a look at the example pages for more information on specific objects and methods.\n","excerpt":"Prerequisites This package requires;\n python version 3.7+ Anaconda version 3.7  Are there any system …","ref":"/COAsT/docs/getting-started/","title":"Getting Started"},{"body":"Here we give a short tutorial of how to use the Altimetry object for reading data and comparing to NEMO data.\nBegin by importing coast and other packages\nimport coast And by defining some file paths. There are the example files that can be obtained with the COAsT package:\nfn_nemo_dat = \u0026quot;./example_files/COAsT_example_NEMO_data.nc\u0026quot; fn_nemo_dom = \u0026quot;./example_files/COAsT_example_NEMO_domain.nc\u0026quot; fn_config_t_grid = \u0026quot;./config/example_nemo_grid_t.json\u0026quot; fn_altimetry = './example_files/COAsT_example_altimetry_data.nc' We need to load in a Gridded object for doing things with NEMO data.\nnemo = coast.Gridded(fn_nemo_dat, fn_nemo_dom, config=fn_config_t_grid) And now we can load in our Altimetry data. By default, Altimetry is set up to read in CMEMS netCDF files. However, if no path is supplied, then the object\u0026rsquo;s dataset will be initialised as None. Custom data can then be loaded if desired, as long as it follows the data formatting for Altimetry.\naltimetry = coast.Altimetry(fn_altimetry, config=\u0026quot;./config/example_altimetry.json\u0026quot;) Before going any further, lets just cut out the bit of the altimetry that is over the model domain. This can be done using subset_indices_lonlat_box to find relevant indices and then isel to extract them. The data has also been thinned slightly.\nind = altimetry.subset_indices_lonlat_box([-10,10], [45,60]) ind = ind[::4] altimetry = altimetry.isel(t_dim=ind) Before comparing our observations to the model, we will interpolate a model variable to the same time and geographical space as the altimetry. This is done using the obs_operator() method:\naltimetry.obs_operator(nemo, mod_var_name='ssh', time_interp='nearest') Doing this has created a new interpolated variable called interp_ssh and saved it back into our Altimetry object. Take a look at altimetry.dataset to see for yourself.\nNext we will compare this interpolated variable to an observed variable using some basic metrics. The basic_stats() routine can be used for this, which calculates some simple metrics including differences, RMSE and correlations. NOTE: This may not be a wise choice of variables.\nstats = altimetry.basic_stats('interp_ssh', 'sla_filtered') Take a look inside stats.dataset to see all of the new variables. When using basic stats, the returned object is also an ALTIMETRY object, so all of the same methods can be applied. Alternatively, if you want to save the new metrics to the original altimetry object, set create_new_object = False.\nNow we will do a more complex comparison using the Continuous Ranked Probability Score (CRPS). For this, we need to hand over the model object, a model variable and an observed variable. We also give it a neighbourhood radius in km (nh_radius).\ncrps = altimetry.crps(nemo, model_var_name = 'ssh', obs_var_name = 'sla_filtered', nh_radius = 20) Again, take a look inside crps.dataset to see some new variables. Similarly to basic_stats, create_new_object can be set to false to save output to the original altimetry object.\nAltimetry has a ready built quick_plot() routine for taking a look at any of the observed or derived quantities above. So to take a look at the \u0026lsquo;sla_filtered\u0026rsquo; variable:\nfig, ax = altimetry.quick_plot('sla_filtered') As stats and crps are also Altimetry objects, quick_plot() can also be used:\nfig, ax = crps.quick_plot('crps') fig, ax = stats.quick_plot('absolute_error') ","excerpt":"Here we give a short tutorial of how to use the Altimetry object for reading data and comparing to …","ref":"/COAsT/docs/examples/altimetry/","title":"Altimetry"},{"body":"The Github page for this package can be found here\nThe rest of this page is coming soon.\n","excerpt":"The Github page for this package can be found here\nThe rest of this page is coming soon.","ref":"/COAsT/docs/contributing-docs/github_workflow/","title":"Github Workflow"},{"body":"COAsT utilises Python’s default logging library and includes a simple setup function for those unfamiliar with how to use it.\nimport coast coast.logging_util.setup_logging() This is all you need to enable full logging output to the console.\nBy default, setup_logging will use the \u0026ldquo;DEBUG\u0026rdquo; logging level, if you want to adjust this, you can use the flags from the logging library.\nimport coast import logging coast.logging_util.setup_logging(level=logging.INFO) Alternative logging levels in increasing levels of severity. Note logs are reported at the chosen severity level and higher:\n..., level=logging.DEBUG) # Detailed information, typically of interest only when diagnosing problems. ..., level=logging.INFO) # Confirmation that things are working as expected. ..., level=logging.WARNING) # An indication that something unexpected happened, or indicative of some problem in the near future (e.g. ‘disk space low’). The software is still working as expected. ..., level=logging.ERROR) # Due to a more serious problem, the software has not been able to perform some function ..., level=logging.CRITICAL) # A serious error, indicating that the program itself may be unable to continue running For more info on logging levels, see the relevant Python documentation.\nLogging output will be printed in the console once enabled by default, but output can be directed to any Stream, for instance, to an opened file.\nimport coast file = open(\u0026#34;coast.log\u0026#34;, \u0026#34;w\u0026#34;) coast.logging_util.setup_logging(stream=file) coast.logging_util.info(\u0026#34;Hello World!\u0026#34;) # Your use of COAsT would go here, this line is included as an example file.close() ","excerpt":"COAsT utilises Python’s default logging library and includes a simple setup function for those …","ref":"/COAsT/docs/contributing_package/python_logging/","title":"Logging"},{"body":"Example data are provided for the following tutorial. Download these files and place the example_files directory in your working directory.\nIn addition configuration files are used to pass information about the example data files to COAsT. These can be downloaded or linked to a local version of the COAsT repository. These files should be places in a config diretory in your working directory.\nThe following tutorial is split into sections:\n","excerpt":"Example data are provided for the following tutorial. Download these files and place the …","ref":"/COAsT/docs/examples/","title":"Examples"},{"body":"Here you will find information needed to contribute code changes to the COAsT package.\n","excerpt":"Here you will find information needed to contribute code changes to the COAsT package.","ref":"/COAsT/docs/contributing_package/","title":"Contributing: COAsT"},{"body":"We use Hugo Extended Version to format and generate our website, the Docsy theme for styling and site structure, and GitHub pages to manage the deployment of the site. Hugo is an open-source static site generator that provides us with templates, content organisation in a standard directory structure, and a website generation engine. You write the pages in Markdown (or HTML if you want), and Hugo wraps them up into a website.\nAll submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests.\nUpdating a single page If you\u0026rsquo;ve just spotted something you\u0026rsquo;d like to change while using the docs, Docsy has a shortcut for you:\n Click Edit this page in the top right hand corner of the page. If you don\u0026rsquo;t already have an up to date fork of the project repo, you are prompted to get one - click Fork this repository and propose changes or Update your Fork to get an up to date version of the project to edit. The appropriate page in your fork is displayed in edit mode. make your edit submit a pull request with a summary of the changes  Previewing your changes locally If you want to run your own local Hugo server to preview your changes as you work:\n  Follow the instructions in Getting started to install Hugo and any other tools you need. You\u0026rsquo;ll need at least Hugo version 0.45 (we recommend using the most recent available version), and it must be the extended version, which supports SCSS.\n  Fork the COAsT-site repo repo into your own project, then create a local copy using git clone. Don’t forget to use --recurse-submodules or you won’t pull down some of the code you need to generate a working site.\ngit clone --recurse-submodules --depth 1 https://github.com/British-Oceanographic-Data-Centre/COAsT-site.git   Run npm install to install Node.js dependencies.\n  Run hugo server in the site root directory. By default your site will be available at http://localhost:1313/COAsT. Now that you\u0026rsquo;re serving your site locally, Hugo will watch for changes to the content and automatically refresh your site.\n  Continue with the usual GitHub workflow to edit files, commit them, push the changes up to your fork, and create a pull request.\n  Creating an issue If you\u0026rsquo;ve found a problem in the docs, but you\u0026rsquo;re not sure how to fix it yourself, please create an issue in the COAsT-site repo. You can also create an issue about a specific page by clicking the Create Issue button in the top right hand corner of the page.\nUseful resources  Docsy user guide: All about Docsy, including how it manages navigation, look and feel, and multi-language support. Hugo documentation: Comprehensive reference for Hugo. Github Hello World!: A basic introduction to GitHub concepts and workflow.  ","excerpt":"We use Hugo Extended Version to format and generate our website, the Docsy theme for styling and …","ref":"/COAsT/docs/contributing-docs/","title":"Contributing: Documentation"},{"body":"What is lazy\u0026hellip; \u0026hellip;loading Lazy loading determines if data is read into memory straight away (on that line of code execution) or if the loading is delayed until the data is physical altered by some function (normally mathematical in nature)\n\u0026hellip;evaluation Lazy evaluation is about delaying the execution of a method/function call until the value is physical required, normally as a graph or printed to screen. Lazy evaluation can also help with memory management, useful with large dataset, by allowing for optimisation on the chained methods calls.\nLazy loading and Lazy evaluation are offer used together, though it is not mandatory and always worth checking that both are happening.\nBeing Lazy in COAsT There are two way to be Lazy within the COAsT package.\n xarray Dask  xarray COAsT uses xarray to load NetCDF files in, by default this will be Lazy, the raw data values will not be brought into memory.\nyou can slice and subset the data while still having the lazy loading honoured, it is not until the data is altered, say via a call to NumPy.cumsum, that the required data will be loaded into memory.\nNote the data on disk (in the NetCDF file) is never altered, only the values in memory are changed.\nimport xarray as xr import NumPy as np dataset_domain = xr.open_dataset(fn_domain) e3w_0 = dataset_domain.e3w_0 # still lazy loaded e3w_0_cs = np.cumsum(e3w_0[1:, :, :], axis=0) # now in memory Dask When in use Dask will provide lazy evaluation on top of the lazy loading.\nusing the same example as above, a file loaded in using xarray, this time with the chunks option set, will not only lazy load the data, but will turn on Dask, now using either the xarray or Dask wrapper functions will mean the NumPy cumsum call is not evaluated right way, in fact it will not be evaluated until either the compute function is called, or a greedy method from another library is used.\nimport xarray as xr dataset_domain = xr.open_dataset(fn_domain, chunks={\u0026#34;t\u0026#34;: 1}) e3w_0 = dataset_domain.e3w_0 # still lazy loaded e3w_0_cs = e3w_0[1:, :, :].cumsum(axis=0) # Dask backed Lazy evaluation We discuss Dask even more here.\n","excerpt":"What is lazy\u0026hellip; \u0026hellip;loading Lazy loading determines if data is read into memory straight …","ref":"/COAsT/docs/contributing_package/lazy-loading/","title":"working Lazily"},{"body":"What is Dask Dask is a python library that allows code to be run in parallel based on the hardware your running on. This means Dask works just as well on your laptop as on your large server.\nUsing Dask Dask is included in the xarray library. When loading a data source (file/NumPy array) Dask is automatically initiated with the chunks variable in the config file. However the chunking may not be optimal but you can adjust it before computation are made.\nnemo_t = coast.Gridded( fn_data=dn_files+fn_nemo_grid_t_dat, fn_domain=dn_files+fn_nemo_dom, config=fn_config) chunks = { \u0026#34;x_dim\u0026#34;: 10, \u0026#34;y_dim\u0026#34;: 10, \u0026#34;t_dim\u0026#34;: 10, } # Chunks are prescribed in the config json file, but can be adjusted while the data is lazy loaded. nemo_t.dataset.chunk(chunks) chunks tell Dask where to break your data across the different processor tasks.\nDirect Dask Dask can be imported and used directly\nimport Dask.array as da big_array = da.multiple(array1,array2) Dask arrays follow the NumPy API. This means that most NumPy functions have a Dask version.\nPotential Issues Dask objects are immutable. This means that the classic approach, pre-allocation follow by modification will not work.\nThe following code will error.\nimport Dask.array as da e3w_0 = da.squeeze(dataset_domain.e3w_0) depth_0 = da.zero_like(e3w_0) depth_0[0, :, :] = 0.5 * e3w_0[0, :, :] # this line will error out option 1 Continue using NumPy function but wrapping the final value in a Dask array. This final Dask object will still be in-memory.\ne3w_0 = np.squeeze(dataset_domain.e3w_0) depth_0 = np.zeros_like(e3w_0) depth_0[0, :, :] = 0.5 * e3w_0[0, :, :] depth_0[1:, :, :] = depth_0[0, :, :] + np.cumsum(e3w_0[1:, :, :], axis=0) depth_0 = da.array(depth_0) option 2 Dask offers a feature called delayed. This can be used as a modifier on your complex methods as follows;\n@Dask.delayed def set_timezero_depths(self, dataset_domain): # complex workings these do not return the computed answer, rather it returns a delayed object. These delayed object get stacked, as more delayed methods are called. When the value is needed, it can be computed like so;\nne = coast.Gridded(...) # come complex delayed methods called ne.data_variable.compute() Dask will now work out a computing path via all the required methods using as many processor tasks as possible.\nVisualising the Graph Dask is fundamentally a computational graph library, to understand what is happening in the background it can help to see these graphs (on smaller/simpler problems). This can be achieved by running;\nne = coast.Gridded(...) # come complex delayed methods called ne.data_variable.visualize() this will output a png image of the graph in the calling directory and could look like this;\n  ","excerpt":"What is Dask Dask is a python library that allows code to be run in parallel based on the hardware …","ref":"/COAsT/docs/contributing_package/dask/","title":"Dask"},{"body":"This is a demonstration script for using the Climatology object in the COAsT package. This object has methods for analysing climatological data. Further examples can be found in the COAsT github repository.\nClimatological means This section shows and example of how to use the Climatology.make_climatology() method to calculates mean over a given period of time. This method doesn\u0026rsquo;t take different years into account, unless using the \u0026lsquo;years\u0026rsquo; frequency. (See the Multi-year climatological means section for multi-yeared data.)\nBegin by importing coast:\nimport coast And by defining some file paths for the data:\n# Path to a data file. fn_nemo_dat = \u0026#34;./example_files/coast_example_nemo_data.nc\u0026#34; # Set path for domain file if required. fn_nemo_dom = \u0026#34;./example_files/coast_example_nemo_domain.nc\u0026#34; # Set path for model configuration file config = \u0026#34;./config/example_nemo_grid_t.json\u0026#34; # Read in data (This example uses NEMO data.) nemo = coast.Gridded(fn_nemo_dat, fn_nemo_dom, config=config) Calculate the climatology:\n# Optional (This specifies an output file path.) fn_out = \u0026#34;/path/to/outputfile.nc\u0026#34; # String is appended to \u0026#34;time.\u0026#34; to create a valid xarray time period. (i.e. time.season, time.month...) climatology_frequency = \u0026#34;month\u0026#34; clim = coast.Climatology() # Not writing output to file: clim_mean = clim.make_climatology(nemo, climatology_frequency) # Writing output to file (may require a large amount of memory.) clim_mean = clim.make_climatology(nemo, climatology_frequency, fn_out=fn_out) Below shows the structure of a dataset returned, containing 3 months worth of meaned data:\n\u0026lt;xarray.Dataset\u0026gt; Dimensions: (y_dim: 1345, x_dim: 1458, z_dim: 51, month: 3) Coordinates: nav_lat (y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1345, 1458), meta=np.ndarray\u0026gt; nav_lon (y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1345, 1458), meta=np.ndarray\u0026gt; deptht (z_dim) float32 3.038 9.367 16.1 ... 5.618e+03 5.822e+03 * month (month) int64 1 2 3 Dimensions without coordinates: y_dim, x_dim, z_dim Data variables: temperature (month, z_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 51, 1345, 1458), meta=np.ndarray\u0026gt; ssh (month, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 1345, 1458), meta=np.ndarray\u0026gt; salinity (month, z_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 51, 1345, 1458), meta=np.ndarray\u0026gt; Multi-year climatological means This section shows an example of how to use the Climatology.multiyear_averages() method to generate annual averages across specified periods of time. This method is designed to be compatible with multi-year datasets, but will work with single year datasets too.\nBegin by importing coast and helpful coast utilities:\nimport coast from coast import seasons And by defining some file paths for the data:\n# Path to a single or multiple NEMO files. fn_nemo_data = \u0026#34;/Path/to/Nemo/*.nc\u0026#34; # Set path for domain file if required. fn_nemo_domain = \u0026#34;/Path/to/domain/domain.nc\u0026#34; fn_config_t_grid = \u0026#34;/Path/to/config/file.json\u0026#34; # Read in multiyear data (This example uses NEMO data from multiple datafiles.) nemo = coast.Gridded(fn_data=fn_nemo_data, fn_domain=fn_nemo_domain, config=fn_config_t_grid, multiple=True) Now calculate means of each season across multiple years for specified data:\nclim = coast.Climatology() # Using seasons module to specify time period. # SPRING, SUMMER, AUTUMN, WINTER, ALL are valid values for seasons. clim_multiyear = clim.multiyear_averages(nemo, seasons.ALL, time_var=\u0026#39;time\u0026#39;, time_dim=\u0026#39;t_dim\u0026#39;) # Or explicitly defining specific month periods. # A list of tuples defining start and end month integers. The start months should be in chronological order. month_periods = [(6,8), (12,2)] # Specifies June -\u0026gt; August and December -\u0026gt; February for each year of data. clim_multiyear = clim.multiyear_averages(nemo, month_periods , time_var=\u0026#39;time\u0026#39;, time_dim=\u0026#39;t_dim\u0026#39;) Below shows the structure of a dataset returned from this method:\n\u0026lt;xarray.Dataset\u0026gt; Dimensions: (y_dim: 1345, x_dim: 1458, z_dim: 51, year_period: 3) Coordinates: nav_lat (y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1345, 1458), meta=np.ndarray\u0026gt; nav_lon (y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1345, 1458), meta=np.ndarray\u0026gt; deptht (z_dim) float32 3.038 9.367 ... 5.618e+03 5.822e+03 * year_period (year_period) MultiIndex - year_period_level_0 (year_period) int64 2004 2005 2005 - year_period_level_1 (year_period) object 'Dec-Feb' 'Mar-May' 'Dec-Feb' Dimensions without coordinates: y_dim, x_dim, z_dim Data variables: temperature (year_period, z_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 51, 1345, 1458), meta=np.ndarray\u0026gt; ssh (year_period, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 1345, 1458), meta=np.ndarray\u0026gt; salinity (year_period, z_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 51, 1345, 1458), meta=np.ndarray\u0026gt; done Data can be accessed by selecting on the year-period MultiIndex:\n# Selecting salinity data variable based on year: clim_multiyear.sel(year_period=(2004))[\u0026#39;salinity\u0026#39;] # Selecting salinity data variable based on year and period: clim_multiyear.sel(year_period=(2004,\u0026#39;Dec-Feb\u0026#39;))[\u0026#39;salinity\u0026#39;] ","excerpt":"This is a demonstration script for using the Climatology object in the COAsT package. This object …","ref":"/COAsT/docs/examples/climatology/","title":"Climatology"},{"body":"This page will walk you though a simple setup for hugo extended - which is needed if want to view any changes you make to this site locally.\nFor more details please read this.\nInstallation Manual  Download hugo extended from GitHub Unzip into preferred location (I use C:\\hugo) Add to OS PATH  optional but makes usage easier    Via a Package Manager On Windows you can use Chocolately to install with:\nchoco install hugo-extended Or on macOS/Linux you can use Homebrew to install with:\nbrew install hugo Try it out! You should now be able to try the following in a terminal\n$ hugo --help if you have cloned the COAsT-site repo you should also now be able to;\n$ cd COAsT-site $ hugo server the above will start a local hugo powered version of the website. you can edit any of the files under /content and see your changes at http://localhost:1313/COAsT/\n","excerpt":"This page will walk you though a simple setup for hugo extended - which is needed if want to view …","ref":"/COAsT/docs/contributing-docs/hugo/","title":"setting up Hugo"},{"body":"The Profile object is able to store and manipulate depth profile data. This object is a child of the INDEXED object and has dimensions (\u0026lsquo;profile\u0026rsquo;,\u0026lsquo;t_dim\u0026rsquo;). Please ensure any variables in this objects dataset also adhere to this dimension order.\nBelow is a set of examples of how you can use a Profile object for validation of a Gridded object.\nYou may want to take bits and pieces from this script to use as you wish.\nMany of these routines utilise xarray and Dask\u0026rsquo;s lazy loading and chunking capabilities. Not all of them do however. If working with very large datasets it is recommended that the resulting datasets are saved to file at every step and read into a new profile object, with new chunking, before continuing. This can be done easily by calling xarrays to_netcdf().\n1. Setup First, Import some things to make this script work\nimport coast import xarray as xr import numpy as np import coast.general_utils as general_utils import matplotlib.pyplot as plt Define some file paths\nfn_dat = \u0026quot;/Users/dbyrne/Projects/coast/workshops/07092021/data/25hourm.grid_T_20180101.nc\u0026quot; fn_dom = \u0026quot;/Users/dbyrne/Projects/coast/workshops/07092021/data/mesh_mask.nc\u0026quot; fn_en4 = \u0026quot;/Users/dbyrne/Projects/coast/workshops/07092021/data/EN.4*\u0026quot; Create a Gridded object using your data and domain files.\nnemo = coast.Gridded(fn_dat, fn_dom, multiple=True, config=\u0026quot;./config/example_nemo_grid_t.json\u0026quot;) Create a landmask array and put it into the nemo object. Here, using the bottom_level == 0 variable from the domain file is enough.\nnemo.dataset[\u0026quot;landmask\u0026quot;] = nemo.dataset.bottom_level == 0 nemo.dataset = nemo.dataset.rename({\u0026quot;depth_0\u0026quot;: \u0026quot;depth\u0026quot;}) 2. Read in EN4 data and subset to domain Create a Profile object. Here we use EN4 data.\nprofile = coast.Profile(fn_en4, multiple=True, config=\u0026quot;./config/example_en4_profiles.json\u0026quot;) Do some processing of the EN4 data. Here, we cut out a box of the EN4 data for the region over out model domain. The process_en4() routine is called to handle quality control of the data.\nGet indices of obs within a geographical box and index the profile\nind = profile.subset_indices_lonlat_box([-25.47, 16.25], [44, 63.5])[0] profile = profile.isel(profile=ind) 3. Process EN4 data Create a new profile object called processed, containing the quality controlled data. NOTE: .compute() or .load() will need to be called on processed.dataset to access data\nprocessed = profile.process_en4() At this point, you can continue is the dataset is small, or save this processed data to file using processed.dataset.to_netcdf()\n4. Interpolate Gridded object to Profile locations Extract profiles from the model at nearest locations and times For this we use Profile.obs_operator().\nmodel_profiles = processed.obs_operator(nemo) Inside this new profile object, there will be the extracted data and 5 new variables describing the spatial and time differences in the interpolation and the x, y and t indices used in the nearest neighbour.\nLets remove any points where the nearest neighbour was further than x km from the original point.\ntoo_far = 5 keep_indices = model_profiles.dataset.interp_dist \u0026lt;= too_far model_profiles = model_profiles.isel(profile=keep_indices) processed = processed.isel(profile=keep_indices) 5. Interpolate Obs and Gridded onto Reference Depths Interpolate both model and observed profiles onto reference depths.\nDefine our reference depths\nreference_depths = np.arange(0, 500, 2) Take just temperature for this example\nmodel_profiles.dataset = model_profiles.dataset[[\u0026quot;temperature\u0026quot;]] Vertical interpolation of model profiles using interpolate_vertical().\nmodel_profiles_interp = model_profiles.interpolate_vertical(reference_depths, interp_method=\u0026quot;linear\u0026quot;) Vertical interpolation of processed observations\nprocessed.dataset = processed.dataset[[\u0026quot;temperature\u0026quot;, \u0026quot;depth\u0026quot;]] processed_interp = processed.interpolate_vertical(reference_depths, interp_method=\u0026quot;linear\u0026quot;) 6. Calculate Errors \u0026amp; Differences Data differences/errors. Now that we have our model and observed data on the same depths, we can do some differencing to get errors. We use the Profile.difference() routine, which will calculate differences, absolute differences and square differences\nCalculate differences as obs minus model\ndifferences = processed_interp.difference(model_profiles_interp) Load the differences to memory.\ndifferences.dataset.load() 7. Calculate Regional Average Errors Average the differences into masked regions By averaging the differences into regions, we can get an idea of the errors or anomalies in, for example, the North Sea. For this we use the coast.Mask_maker() object as an aid\nFirst let\u0026rsquo;s make some masks to define regions of the model domain\nmm = coast.MaskMaker() Make some variables easier to access\nbath = nemo.dataset.bathymetry.values lon = nemo.dataset.longitude.values lat = nemo.dataset.latitude.values Make a North Sea and Whole Domain mask\nmm_north_sea = mm.region_def_nws_north_sea(lon, lat, bath) mm_whole_domain = np.ones(lon.shape) mask_list = [mm_north_sea, mm_whole_domain] mask_names = [\u0026quot;North Sea\u0026quot;, \u0026quot;Whole Domain\u0026quot;] Turn mask list into an xarray dataset\nmask_list = coast.MaskMaker.make_mask_dataset(lon, lat, mask_list) Determine whether each profile is in each masked region or not\nmask_indices = model_profiles_interp.determine_mask_indices(mask_list) Do average differences for each region\nmask_means = differences.mask_means(mask_indices) 8. Calculate Surface and Bottom Values/Errors Surface and bottom averaging. We can use a couple of routines to get surface and bottom values for the profile data. This can be used to give us maps of e.g. SST. It also allows us to calculate more statistics such as the CRPS\nLets get surface values by averaging over the top 5m of data\nsurface = 5 model_profiles_surface = model_profiles.depth_means([0, surface]) obs_profiles_surface = processed.depth_means([0, surface]) Get differences\nsurface_errors = obs_profiles_surface.difference(model_profiles_surface) Lets get bottom values by averaging over the bottom 30m, except whether depth is \u0026lt;100m, then average over the bottom 10m\nmodel_profiles_bottom = model_profiles.bottom_means([10, 30], [100, np.inf]) obs_bathymetry = model_profiles.dataset[\u0026quot;bathymetry\u0026quot;].values processed.dataset[\u0026quot;bathymetry\u0026quot;] = ([\u0026quot;profile\u0026quot;], obs_bathymetry) obs_profiles_bottom = processed.bottom_means([10, 30], [100, np.inf]) Get differences\nbottom_errors = obs_profiles_bottom.difference(model_profiles_bottom) ","excerpt":"The Profile object is able to store and manipulate depth profile data. This object is a child of the …","ref":"/COAsT/docs/examples/profile/","title":"Profile"},{"body":"","excerpt":"","ref":"/COAsT/docs/reference/","title":"Reference"},{"body":"This is a demonstration script for using the Tidegauge object in the COAsT package. This object has strict data formatting requirements, which are outlined in tidegauge.py.\nBegin by importing coast and other packages\nimport coast import datetime And by defining some file paths from the COAsT example files\nfn_nemo_dat = './example_files/coast_example_nemo_data.nc' fn_nemo_dom = './example_files/coast_example_nemo_domain.nc' fn_config_t_grid = './config/example_nemo_grid_t.json', fn_tidegauge = './example_files/tide_gauges/lowestoft-p024-uk-bodc' fn_tidegauge_mult = './example_files/tide_gauges/l*' We need to load in a Gridded object for doing things with NEMO.\nnemo = coast.Gridded(fn_nemo_dat, fn_nemo_dom, config=fn_config_t_grid) And now we can load in our tidegauge data. By default, Tidegauge is set up to read in GESLA ASCII files. However, if no path is supplied, then the object\u0026rsquo;s dataset will be initialised as None. Custom data can then be loaded if desired, as long as it follows the data formatting for Tidegauge. Here we load data between two specified dates:\ndate0 = datetime.datetime(2007,1,10) date1 = datetime.datetime(2007,1,12) tidegauge = coast.Tidegauge(fn_tidegauge, date_start = date0, date_end = date1) Before comparing our observations to the model, we will interpolate a model variable to the same time and geographical space as the tidegauge. This is done using the obs_operator() method:\ntidegauge.obs_operator(nemo, mod_var_name='ssh', time_interp='nearest') Doing this has created a new interpolated variable called interp_ssh and saved it back into our Tidegauge object. Take a look at tidegauge.dataset to see for yourself.\nNext we will compare this interpolated variable to an observed variable using some basic metrics. The basic_stats() routine can be used for this, which calculates some simple metrics including differences, RMSE and correlations. NOTE: This may not be a wise choice of variables.\nstats = tidegauge.basic_stats('interp_ssh', 'sea_level') Take a look inside stats.dataset to see all of the new variables. When using basic stats, the returned object is also an Tidegauge object, so all of the same methods can be applied. Alternatively, if you want to save the new metrics to the original Tidegauge object, set create_new_object = False.\nNow we will do a more complex comparison using the Continuous Ranked Probability Score (CRPS). For this, we need to hand over the model object, a model variable and an observed variable. We also give it a neighbourhood radius in km (nh_radius).\ncrps = tidegauge.crps(nemo, model_var_name = 'ssh', obs_var_name = 'sea_level', nh_radius = 20) Again, take a look inside crps.dataset to see some new variables. Similarly to basic_stats, create_new_object can be set to false to save output to the original tidegauge object.\nTidegauge has ready made quick plotting routines for viewing time series and tide gauge location. To look at the tide gauge location:\nfig, ax = tidegauge.plot_on_map() Or to look at a time series of the sea_level variable:\nfig, ax = tidegauge.plot_timeseries('sea_level', qc_colors=True) Note that start and end dates can also be specified for plot_timeseries().\nAs stats and crps are also Tidegauge objects, the same time series plotting functionality can be used:\ncrps.plot_timeseries('crps') stats.plot_timeseries('absolute_error') Each Tidegauge object only holds data for a single tidegauge. There is some functionality for dealing with multiple gauges in COAsT. To load multiple GESLA tidegauge files, we use the static method create_multiple_tidegauge(). This routine takes a list of files or a wildcard string and loads them all into a list of Tidegauge objects.\nfrom coast.tidegauge import Tidegauge date0 = datetime.datetime(2007,1,10) date1 = datetime.datetime(2007,1,12) tidegauge_list = Tidegauge.create_multiple_tidegauge(fn_tidegauge_mult, date0,date1) Now that we have tidegauge_list, we can plot the locations of all tide gauges as follows:\nfig, ax = Tidegauge.plot_on_map_multiple(tidegauge_list) To do analysis on multiple gauges, a simple looping script can be setup. For example, to obtain basic stats:\nfor tg in tidegauge_list: tg.obs_operator(nemo, 'ssh') tg.basic_stats('interp_ssh', 'sea_level', create_new_object=False) And now some of these new values can be plotted on a map, again using plot_on_map_multiple:\nfig, ax = Tidegauge.plot_on_map_multiple(tidegauge_list, color_var_str='rmse') ","excerpt":"This is a demonstration script for using the Tidegauge object in the COAsT package. This object has …","ref":"/COAsT/docs/examples/tidegauge/","title":"Tidegauge"},{"body":"The TidegaugeMultiple object is for storing multiple tide gauge/time series datasets in one object. It has two dimensions: (\u0026lsquo;id\u0026rsquo;, \u0026lsquo;t_dim\u0026rsquo;) and is a child of the Timeseries and Indexed objects. All timeseries in this object must lie on the same time dimension and indices. If not the case in your data, then it must be preprocessed before use with this object.\nThe Tidegauge object on the other hand, currently only stores one time series at a time. See the examples page for Tidegauge for more information. In the future, these objects will be combined.\nThe TidegaugeMultiple object contains multiple routines suitable for validation of SSH. Below are some examples of how to use this object for this purpose.\nThis script gives an overview of some of validation tools available when using the TidegaugeMultiple object in COAsT.\nFor this a script, a premade netcdf file containing tide gauge data is used.\n1. Setup import xarray as xr import numpy as np import matplotlib.pyplot as plt import coast import datetime Define paths\nfn_dom = \u0026quot;/Users/dbyrne/Projects/coast/workshops/07092021/data/mesh_mask.nc\u0026quot; fn_dat = \u0026quot;/Users/dbyrne/Projects/coast/workshops/07092021/data/sossheig*\u0026quot; fn_tg = \u0026quot;/Users/dbyrne/Projects/coast/workshops/07092021/data/tg_amm15.nc\u0026quot; Create gridded object and load data\nnemo = coast.Gridded(fn_dat, fn_dom, multiple=True, config=\u0026quot;./config/example_nemo_grid_t.json\u0026quot;) Create a landmask array and put it into the nemo object. Here, using the bottom_level == 0 variable from the domain file is enough.\nnemo.dataset[\u0026quot;landmask\u0026quot;] = nemo.dataset.bottom_level == 0 Rename depth_0 to be depth to work with routines expectations\nnemo.dataset = nemo.dataset.rename({\u0026quot;depth_0\u0026quot;: \u0026quot;depth\u0026quot;}) nemo.dataset = nemo.dataset[[\u0026quot;ssh\u0026quot;, \u0026quot;landmask\u0026quot;]] 2. Create TidegaugeMultiple object Create the object and then inset the netcdf dataset\nobs = coast.TidegaugeMultiple() obs.dataset = xr.open_dataset(fn_tg) Cut down data to be only in 2018 to match model data.\nstart_date = datetime.datetime(2018, 1, 1) end_date = datetime.datetime(2018, 12, 31) obs.dataset = coast.general_utils.data_array_time_slice(obs.dataset, start_date, end_date) 3. Interpolate model data onto obs locations Obs_operator is used to extract time series from the model:\nmodel_timeseries = obs.obs_operator(nemo) In this case, transpose the interpolated dataset so dimensions are in the correct order: (\u0026lsquo;id\u0026rsquo;,\u0026lsquo;t_dim\u0026rsquo;)\nmodel_timeseries.dataset = model_timeseries.dataset.transpose() Process the data a little This routine searches for missing values in each dataset and applies them equally to each corresponding dataset\nobs, model_timeseries = obs.match_missing_values(model_timeseries) Subtract means from all time series\nobs = obs.demean_timeseries() model_timeseries = model_timeseries.demean_timeseries() Now you have equivalent and comparable sets of time series that can be easily compared.\n4. Calculate non tidal residuals First, do a harmonic analysis. This routine uses utide\nha_mod = model_timeseries.harmonic_analysis_utide() ha_obs = obs.harmonic_analysis_utide() Create new TidegaugeMultiple objects containign reconstructed tides\ntide_mod = model_timeseries.reconstruct_tide_utide(ha_mod) tide_obs = obs.reconstruct_tide_utide(ha_obs) Get new TidegaugeMultiple objects containing non tidal residuals.\nntr_mod = model_timeseries.calculate_residuals(tide_mod) ntr_obs = obs.calculate_residuals(tide_obs) Other interesting applications here included only reconstructing specified tidal frequency bands and validating this.\n5. Calculate errors The difference() routine will calculate differences, absolute_differences and squared differenced for all variables:\nntr_diff = ntr_obs.difference(ntr_mod) ssh_diff = obs.difference(model_timeseries) We can then easily get mean errors, MAE and MSE\nmean_stats = ntr_diff.dataset.mean(dim=\u0026quot;t_dim\u0026quot;, skipna=True) 6. Threshold Statistics for Non-tidal residuals This is a simple extreme value analysis of whatever data you use. It will count the number of peaks and the total time spent over each threshold provided. It will also count the numbers of daily and monthly maxima over each threshold\nthresh_mod = ntr_mod.threshold_statistics(thresholds=np.arange(0, 2, 0.2)) thresh_obs = ntr_obs.threshold_statistics(thresholds=np.arange(0, 2, 0.2)) ","excerpt":"The TidegaugeMultiple object is for storing multiple tide gauge/time series datasets in one object. …","ref":"/COAsT/docs/examples/tidegaugemultiple/","title":"TidegaugeMultiple"},{"body":"In this tutorial we take a look the following Isobath Contour Methods:\n1. Extract isbath contour between two points 2. Plot contour on map 3. Calculate pressure along contour 4. Calculate flow across contour 5. Calculate pressure gradient driven flow across contour  Create a contour subset of the example dataset Load packages and define some file paths\nimport coast import matplotlib.pyplot as plt fn_nemo_dat_t = \u0026#34;./example_files/nemo_data_T_grid.nc\u0026#34; fn_nemo_dat_u = \u0026#34;./example_files/nemo_data_U_grid.nc\u0026#34; fn_nemo_dat_v = \u0026#34;./example_files/nemo_data_V_grid.nc\u0026#34; fn_nemo_dom = \u0026#34;./example_files/coast_example_nemo_domain.nc\u0026#34; # Configuration files describing the data files fn_config_t_grid = \u0026#34;./config/example_nemo_grid_t.json\u0026#34; fn_config_f_grid = \u0026#34;./config/example_nemo_grid_f.json\u0026#34; fn_config_u_grid = \u0026#34;./config/example_nemo_grid_u.json\u0026#34; fn_config_v_grid = \u0026#34;./config/example_nemo_grid_v.json\u0026#34; To extract isobath contour between two points and create contour object, first create a gridded object with the grid only.\nnemo_f = coast.Gridded(fn_domain=fn_nemo_dom, config=fn_config_f_grid) Then create a contour object on the 200m isobath\ncontours, no_contours = coast.Contour.get_contours(nemo_f, 200) Extract the indices for the contour in a specified box\ny_ind, x_ind, contour = coast.Contour.get_contour_segment(nemo_f, contours[0], [50, -10], [60, 3]) Extract the contour for the specified indices\ncont_f = coast.ContourF(nemo_f, y_ind, x_ind, 200) Plot contour on map plt.figure() coast.Contour.plot_contour(nemo_f, contour) plt.show()    Calculate pressure along contour Repeat the above procedure but on t-points\nnemo_t = coast.Gridded(fn_data=fn_nemo_dat_t, fn_domain=fn_nemo_dom, config=fn_config_t_grid) contours, no_contours = coast.Contour.get_contours(nemo_t, 200) y_ind, x_ind, contour = coast.Contour.get_contour_segment(nemo_t, contours[0], [50, -10], [60, 3]) cont_t = coast.ContourT(nemo_t, y_ind, x_ind, 200) Now construct pressure along this contour segment\ncont_t.construct_pressure(1027) This creates cont_t.data_contour.pressure_s and cont_t.data_contour.pressure_h_zlevels fields.\nCalculate flow across contour Create the contour segement on f-points again\nnemo_f = coast.Gridded(fn_domain=fn_nemo_dom, config=fn_config_f_grid) nemo_u = coast.Gridded(fn_data=fn_nemo_dat_u, fn_domain=fn_nemo_dom, config=fn_config_u_grid) nemo_v = coast.Gridded(fn_data=fn_nemo_dat_v, fn_domain=fn_nemo_dom, config=fn_config_v_grid) contours, no_contours = coast.Contour.get_contours(nemo_f, 200) y_ind, x_ind, contour = coast.Contour.get_contour_segment(nemo_f, contours[0], [50, -10], [60, 3]) cont_f = coast.ContourF(nemo_f, y_ind, x_ind, 200) To calculate the flow across the contour, pass u- and v- gridded velocity objects\ncont_f.calc_cross_contour_flow(nemo_u, nemo_v) This creates fields cont_f.data_cross_flow.normal_velocities and cont_f.data_cross_flow.depth_integrated_normal_transport\nCalculate pressure gradient driven flow across contour The \u0026ldquo;calc_geostrophic_flow()\u0026rdquo; operates on f-grid objects and requires configuration files for the u- and v- grids\ncont_f.calc_geostrophic_flow(nemo_t, config_u=fn_config_u_grid, config_v=fn_config_v_grid, ref_density=1027) This constructs:\n cont_f.data_cross_flow.normal_velocity_hpg cont_f.data_cross_flow.normal_velocity_spg cont_f.data_cross_flow.transport_across_AB_hpg cont_f.data_cross_flow.transport_across_AB_spg ","excerpt":"In this tutorial we take a look the following Isobath Contour Methods:\n1. Extract isbath contour …","ref":"/COAsT/docs/examples/contour/","title":"Contour subsetting"},{"body":"In this tutorial we take a look at subsetting the model data along a transect (a custom straight line) and creating some bespoke diagnostics along it. We look at:\n1. Creating a Transect object, defined between two points. 2. Plotting data along a transect. 3. Calculating flow normal to the transect  Create a transect subset of the example dataset Load packages and define some file paths\nimport coast import xarray as xr import matplotlib.pyplot as plt fn_nemo_dat_t = \u0026#34;./example_files/nemo_data_T_grid.nc\u0026#34; fn_nemo_dat_u = \u0026#34;./example_files/nemo_data_U_grid.nc\u0026#34; fn_nemo_dat_v = \u0026#34;./example_files/nemo_data_V_grid.nc\u0026#34; fn_nemo_dom = \u0026#34;./example_files/coast_example_nemo_domain.nc\u0026#34; # Configuration files describing the data files fn_config_t_grid = \u0026#34;./config/example_nemo_grid_t.json\u0026#34; fn_config_f_grid = \u0026#34;./config/example_nemo_grid_f.json\u0026#34; fn_config_u_grid = \u0026#34;./config/example_nemo_grid_u.json\u0026#34; fn_config_v_grid = \u0026#34;./config/example_nemo_grid_v.json\u0026#34; Load data variables that are on the NEMO t-grid\nnemo_t = coast.Gridded( fn_data = fn_nemo_dat_t, fn_domain = fn_nemo_dom, config=fn_config_t_grid ) Now create a transect between the points (54 N 15 W) and (56 N, 12 W) using the coast.TransectT object. This needs to be passed the corresponding Gridded object and transect end points. The model points closest to these coordinates will be selected as the transect end points.\ntran_t = coast.TransectT( nemo_t, (54,-15), (56,-12) ) tran_t.data where r_dim is the dimension along the transect. It is simple to plot a scalar such as temperature along the transect:\ntemp_mean = tran_t.data.temperature.mean(dim=\u0026#39;t_dim\u0026#39;) temp_mean.plot.pcolormesh(y=\u0026#39;depth_0\u0026#39;, yincrease=False )    Flow across the transect With NEMO’s staggered grid, the first step is to define the transect on the f-grid so that the velocity components are between f-points. We do not need any model data on the f-grid, just the grid information, so create a nemo f-grid object\nnemo_f = coast.Gridded( fn_domain = fn_nemo_dom, config=fn_config_f_grid ) and a transect on the f-grid\ntran_f = coast.TransectF( nemo_f, (54,-15), (56,-12) ) tran_f.data We also need the i- and j-components of velocity so (lazy) load the model data on the u- and v-grid grids\nnemo_u = coast.Gridded( fn_data = fn_nemo_dat_u, fn_domain = fn_nemo_dom, config=fn_config_u_grid ) nemo_v = coast.Gridded( fn_data = fn_nemo_dat_v, fn_domain = fn_nemo_dom, config=fn_config_v_grid ) Now we can calculate the flow across the transect with the method\ntran_f.calc_flow_across_transect(nemo_u,nemo_v) The flow across the transect is stored in a new dataset where the variables are all defined at the points between f-points.\ntran_f.data_cross_tran_flow For example, to plot the time averaged velocity across the transect, we can plot the ‘normal_velocities’ variable\ncross_velocity_mean = tran_f.data_cross_tran_flow.normal_velocities.mean(dim=\u0026#39;t_dim\u0026#39;) cross_velocity_mean.rolling(r_dim=2).mean().plot.pcolormesh(yincrease=False,y=\u0026#39;depth_0\u0026#39;,cbar_kwargs={\u0026#39;label\u0026#39;: \u0026#39;m/s\u0026#39;})    or the volume transport across the transect, we can plot the ‘normal_transports’ variable\ncross_transport_mean = tran_f.data_cross_tran_flow.normal_transports.mean(dim=\u0026#39;t_dim\u0026#39;) cross_transport_mean.rolling(r_dim=2).mean().plot() plt.ylabel(\u0026#39;Sv\u0026#39;)    ","excerpt":"In this tutorial we take a look at subsetting the model data along a transect (a custom straight …","ref":"/COAsT/docs/examples/transect/","title":"Transect subsetting"},{"body":"A short script to install COAsT in a conda environment, download and run some build tests.\n# Fresh build module load anaconda/3-5.1.0 # or whatever it takes to activate conda yes | conda env remove --name test_env yes | conda create -n test_env python=3.8 # create a new environment conda activate test_env yes | conda install -c conda-forge -c bodc coast yes | conda install -c conda-forge cartopy=0.18.0 # used for some of the map plotting # Download bits and bobs rm -rf coast_test mkdir coast_test cd coast_test git clone https://github.com/British-Oceanographic-Data-Centre/COAsT.git wget -c https://linkedsystems.uk/erddap/files/COAsT_example_files/COAsT_example_files.zip \u0026amp;\u0026amp; unzip COAsT_example_files.zip ln -s COAsT/unit_testing/ . ln -s COAsT_example_files example_files # Run unit tests python COAsT/unit_testing/unit_test.py \u0026gt; coast_test.txt ## If OK then clean up cd .. rm -rf coast_test ","excerpt":"A short script to install COAsT in a conda environment, download and run some build tests.\n# Fresh …","ref":"/COAsT/docs/contributing_package/build_test/","title":"Build test"},{"body":"To date the workflow has been to unit test anything and everything that goes into the develop branch and then periodically push to master less frequently and issue a new github release.\nWith the push to master Git Actions build the conda and pip packages and the package receives a zenodo update (https://zenodo.org/account/settings/github/repository/British-Oceanographic-Data-Centre/COAsT) and DOI.\n1. Push to master Any push to master initiates the Git Actions to build and release the package. It is advisable then to prepare the release in develop and only ever pull into master from develop. (Pulling from master to develop could bring unexpected Git Actions to develop). In order for the package builds to work the version of the package must be unique. The version of the package is set in file setup.py. E.g. shown as 0.4.1 below:\n# setup.py ... PACKAGE = SimpleNamespace(**{ \u0026#34;name\u0026#34;: \u0026#34;COAsT\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.4.1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;This is the Coast Ocean Assessment Tool\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://www.bodc.ac.uk\u0026#34;, \u0026#34;download_url\u0026#34;: \u0026#34;https://github.com/British-Oceanographic-Data-Centre/COAsT/\u0026#34;, .... Version numbering follows the semantic versioning convention. Briefly, given a version number MAJOR.MINOR.PATCH, increment the:\n MAJOR version when you make incompatible API changes, MINOR version when you add functionality in a backwards compatible manner, and PATCH version when you make backwards compatible bug fixes. Additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format.  2. Issue new release Then issue a new release, with the new version label, and annotate the major changes.\n","excerpt":"To date the workflow has been to unit test anything and everything that goes into the develop branch …","ref":"/COAsT/docs/contributing_package/push_to_master/","title":"Push to master"},{"body":"Using COAsT to compute the Empirical Orthogonal Functions (EOFs) of your data.\nLoad data and compute EOFs Load packages and define some file paths\nimport coast import xarray as xr import matplotlib.pyplot as plt fn_nemo_dat_t = \u0026#34;./example_files/nemo_data_T_grid.nc\u0026#34; fn_nemo_dom = \u0026#34;./example_files/COAsT_example_NEMO_domain.nc\u0026#34; fn_nemo_config = \u0026#34;./config/example_nemo_grid_t.json\u0026#34; Load data variables that are on the NEMO t-grid\nnemo_t = coast.Gridded( fn_data = fn_nemo_dat_t, fn_domain = fn_nemo_dom, config = fn_nemo_config ) For a variable (or subset of a variable) with two spatial dimensions and one temporal dimension, i.e. (x,y,t), the EOFs, temporal projections and variance explained can be computed by calling the ‘eofs’ method, and passing in the ssh DataArray as an argument. For example, for the sea surface height field, we can do\neof_data = coast.compute_eofs( nemo_t.dataset.ssh ) The method returns an xarray dataset that contains the EOFs, temporal projections and variance as DataArrays\neof_data The variance explained of the first four modes is\neof_data.variance.sel(mode=[1,2,3,4]) And the EOFs and temporal projections can be quick plotted:\neof_data.EOF.sel(mode=[1,2,3,4]).plot.pcolormesh(col=\u0026#39;mode\u0026#39;,col_wrap=2,x=\u0026#39;longitude\u0026#39;,y=\u0026#39;latitude\u0026#39;)    eof_data.temporal_proj.sel(mode=[1,2,3,4]).plot(col=\u0026#39;mode\u0026#39;,col_wrap=2,x=\u0026#39;time\u0026#39;)    The more exotic hilbert complex EOFs can also be computed to investigate the propagation of variability, for example:\nheof_data = coast.compute_hilbert_eofs( nemo_t.dataset.ssh ) heof_data now with the modes expressed by their amplitude and phase, the spatial propagation of the variability can be examined through the EOF_phase.\n","excerpt":"Using COAsT to compute the Empirical Orthogonal Functions (EOFs) of your data.\nLoad data and compute …","ref":"/COAsT/docs/examples/eofs/","title":"Empirical Orthogonal Functions"},{"body":"A demonstration of pycnocline depth and thickness diagnostics. The first and second depth moments of stratification are computed as proxies for pycnocline depth and thickness, suitable for a nearly two-layer fluid.\nNote that in the AMM7 example data the plots are not particularly spectacular as the internal tide is poorly resolved at 7km.\nimport coast import numpy as np import os import xarray as xr import dask import matplotlib.pyplot as plt import matplotlib.colors as colors # colormap fiddling Load in the Data set some aliases and load the t-grid data:\n# set some paths config = \u0026#39;AMM7\u0026#39; fn_nemo_grid_t_dat = \u0026#39;./example_files/nemo_data_T_grid_Aug2015.nc\u0026#39; fn_nemo_dom = \u0026#39;./example_files/coast_example_nemo_domain.nc\u0026#39; config_t = \u0026#34;./config/example_nemo_grid_t.json\u0026#34; config_w = \u0026#34;./config/example_nemo_grid_w.json\u0026#34; Create a Gridded object and load in the data:\nnemo_t = coast.Gridded(fn_nemo_grid_t_dat, fn_nemo_dom, config=config_t) The stratification variables are computed as centred differences of the t-grid variables. These will become w-grid variables. So, create an empty w-grid object, to store stratification. Note how we do not pass a NEMO data file for this load.\nnemo_w = coast.Gridded(fn_domain=dn_files + fn_nemo_dom, config=config_w) Subset the Domain We are not interested in the whole doman so it is computationally efficient to subset the data for the region of interest. Here we will look at the North Sea between (51N: 62N) and (-4E:15E). We will great subset objects for both the t- and w-grids:\nind_2d = nemo_t.subset_indices([51,-4], [62,15]) nemo_nwes_t = nemo_t.isel(y_dim=ind_sci[0], x_dim=ind_sci[1]) #nwes = northwest european shelf ind_2d = nemo_w.subset_indices([51,-4], [62,15]) nemo_nwes_w = nemo_w.isel(y_dim=ind_sci[0], x_dim=ind_sci[1]) #nwes = northwest european shelf nemo_nwes_t.dataset Diagnostic calculations and plotting We can use a COAsT method to construct the in-situ density:\nnemo_nwes_t.construct_density( eos=\u0026#39;EOS10\u0026#39; ) Then we construct stratification using a COAsT method to take the vertical derivative. Noting that the inputs are on t-pts and the outputs are on w-pt\nnemo_nwes_w = nemo_nwes_t.differentiate( \u0026#39;density\u0026#39;, dim=\u0026#39;z_dim\u0026#39;, out_var_str=\u0026#39;rho_dz\u0026#39;, out_obj=nemo_nwes_w ) # --\u0026gt; sci_nwes_w.rho_dz This has created a variable called nemo_nwes_w.rho_dz.\nWe can now use the InternalTide class to construct the first and second moments (over depth) of density. In the limit of an idealised two-layer fluid these converge to the depth and thickness of the interface. I.e. the pycnocline depth and thickness respectively.\n#%% Create internal tide diagnostics object IT = coast.InternalTide(nemo_nwes_t, nemo_nwes_w) #%% Construct pycnocline variables: depth and thickness IT.construct_pycnocline_vars( nemo_nwes_t, nemo_nwes_w ) Finally we plot pycnocline variables (depth and thickness) using an InternalTide method:\nIT.quick_plot()    ","excerpt":"A demonstration of pycnocline depth and thickness diagnostics. The first and second depth moments of …","ref":"/COAsT/docs/examples/stratification/","title":"Stratification diagnostics"},{"body":"This is a demonstration for how to export intermediate data from COAsT to netCDF files for later analysis or storage. The tutorial showcases the xarray.to_netcdf() method. http://xarray.pydata.org/en/stable/generated/xarray.Dataset.to_netcdf.html\nLoad in example data Begin by importing coast and other packages\nimport coast and by defining some file paths\nfn_nemo_dat = \u0026#34;./example_files/coast_example_nemo_data.nc\u0026#34; fn_nemo_dom = \u0026#34;./example_files/coast_example_nemo_domain.nc\u0026#34; config = \u0026#34;./config/example_nemo_grid_t.json\u0026#34; ofile = \u0026#34;example_export_output.nc\u0026#34; # The target filename for output We need to load in a Gridded object to get started.\nnemo = coast.Gridded(fn_nemo_dat, fn_nemo_dom, config=config) Export to netCDF We can export the whole xr.DataSet to a netCDF file\nnemo.dataset.to_netcdf(ofile, mode=\u0026#34;w\u0026#34;, format=\u0026#34;NETCDF4\u0026#34;) Other file formats are available. From the documentation:\nformat: NETCDF4: Data is stored in an HDF5 file, using netCDF4 API features. NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only netCDF 3 compatible API features. NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format, which fully supports 2+ GB files, but is only compatible with clients linked against netCDF version 3.6.0 or later. NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not handle 2+ GB files very well. Similarly other modes are available; \u0026lsquo;w\u0026rsquo; (write) is the default. From the documentation:\nmode ({\u0026quot;w\u0026quot;, \u0026quot;a\u0026quot;}, default: \u0026quot;w\u0026quot;) – Write (‘w’) or append (‘a’) mode. If mode=’w’, any existing file at this location will be overwritten. If mode=’a’, existing variables will be overwritten. Alternatively a single variable (an xr.DataArray object) can be exported\nnemo.dataset[\u0026#39;temperature\u0026#39;].to_netcdf(ofile, format=\u0026#34;NETCDF4\u0026#34;) Similarly xr.DataSets collections of variables or xr.DataArray variables can be exported to netCDF for objects in the TRANSECT, TIDEGAUGE, etc classes.\nCheck the exported file Check the exported file is as you expect. Perhaps using ncdump -h example_export_output.nc Or load file as see that the xarray structure is preserved.\nimport xarray as xr object = xr.open_dataset(ofile) object.close() # close file associated with this object ","excerpt":"This is a demonstration for how to export intermediate data from COAsT to netCDF files for later …","ref":"/COAsT/docs/examples/export_netcdf/","title":"Export to netCDF"},{"body":"Code functionality tests are written in unit_testing/unit_testing.py in the COAsT repository, and therefore contain working examples of the package. These are written to verify the package functionality and to maintain operability following code updates.\nAt the time of writing these included:\n1. Loading \u0026amp; Initialisation a. Loading Gridded NEMO data file b. Load data from existing dataset c. Set Gridded variable name d. Set Gridded grid attribute - dimension names e. Load only Domain f. Calculate depth_0 for t,u,v,w,f grids g. Load a subregion dataset with a full domain (AMM7) h. Load and combine multiple files. i. Load and combine harmonics j. Convert harmonics to a/g and back k. Compute e3[t,u,v,f,w] from SSH  2. General Utility Methods in COAsT a. Copying a COAsT object b. COAsT __getitem__ returns variable c. Renaming variables inside a COAsT object d. day of the week function  3. Diagnostic Methods a. Compute vertical spatial derivative b. Construct density method inside Gridded class c. Construct pycnocline depth and thickness d. Plot pycnocline depth  4. Transect Methods a. Determine and extract transect indices b. Transport velocity and depth calculations c. Transport and velocity plotting d. Construct density on z-levels along the transect. Compare with item 3b. e. Geostrophic velocity \u0026amp; transport calculations  5. Object Manipulation (e.g. indexing, subsetting) a. Subsetting single variable b. Indices by distance c. Find nearest xy indices d. Gridded.interpolate_in_space() e. Gridded.interpolate_in_time()  6. Altimetry Methods a. Load example data b. Subset altimetry data c. Interpolate model onto altimetry using obs_operator d. Calculate CRPS e. ALTIMETRY basic stats f. Altimetry plotting  7. TIDEGAUGE Methods a. Load GESLA data b. Load BODC data c. Load EA data via API d. TIDEGAUGE obs operator e. Tidegauge CRPS f. Tiudegauge basic stats g. Resample TIDEGAUGE h. Apply Doodson XO Filter to data i. Loading multiple tidegauges j. Plotting a single tidegauge location k. Plotting multiple tidegauge locations l. Tidegauge time series plot m. TIDEGAUGE method for tabulated data n. TIDEGAUGE method for finding peaks and troughs, compare neighbours o. TIDEGAUGE method for finding extrema and troughs, fit cubic spline  8. Isobath Contour Methods a. Extract isbath contour between two points b. Plot contour on map c. Calculate pressure along contour d. Calculate flow across contour e. Calculate pressure gradient driven flow across contour  9. EOF methods a. Compute EOFs, projections and variance b. Compute HEOFs, projections and variance  10. Profile Methods a. Load En4 data b. Process En4 data c. Gridded obs_operator d. Vertical Interpolation e. Profile differencing f. Regional averaging g. Surface and Bottm averaging.  11. Plotting Utility a. scatter_with_fit() b. create_geo_axes() c. determine_colorbar_extension() d. determine_clim_by_standard_deviation()  12. Stats Utility a. find_maxima()  13. MASK_MASKER a. Create mask by indices b. Create mask by lonlat  14. CLIMATOLOGY a. Create monthly and seasonal climatology, write to file  N. Example script testing a. tutorials using example_files (E.g. altimetry and tidegauges, ...) b. tutorial on AMM15 data c. tutorial on AMM60 data d. tutorial on Belize data e. tutorial on SEAsia data f. tutorial on WCSSP-India data g. tutorial on internal tides  ","excerpt":"Code functionality tests are written in unit_testing/unit_testing.py in the COAsT repository, and …","ref":"/COAsT/docs/examples/unit_testing/","title":"Unit testing"},{"body":"Configuration file code can be found in coast/config within the COAsT github repository. This code is used internally within the package.\nConfiguration file usage Configuration files are passed into a COAsT class on the instantiation of a new object. For example the Gridded class __init__ method takes an argument config. This argument must be a String or Path object representing a path to the configuration file. E.g.\nconfig_file = \u0026#34;./config/example_nemo_grid_t.json\u0026#34; # path to json config file fn_nemo_dat = \u0026#34;coast_example_nemo_data.nc\u0026#34; fn_nemo_dom = \u0026#34;coast_example_nemo_domain.nc\u0026#34; gridded_obj = coast.Gridded(fn_data=fn_nemo_dat, fn_domain=fn_nemo_dom, config=config_file) For convenience, as indicated above, the path to the configuration file could be alternatively expressed as a path object. E.g.:\nfrom pathlib import Path config_file = Path(\u0026#34;path/to/config_file.json\u0026#34;) Configuration file structure Configuration files must follow a standard structure so that the ConfigParser class can parse the file correctly.\nDepending on the type of configuration file, there are a number of required keys:\nGridded configuration    Key Description     type A string value representing the type of configuration file. In the case of gridded config this will always be \u0026ldquo;gridded\u0026rdquo;.   dimensionality An integer value representing the number of dimensions within the data files.   grid_ref A dictionary containing the type of grid, and a list of grid variables defining the mapping from the domain file to NEMO file.   chunks A dictionary defining a dask chunk shape, used when loading in data files. JSON doesn\u0026rsquo;t support integer keys, and so the dimensions name should be provided as the key instead. An empty dictionary will result in auto chunking. Rechunking can be applied subsequently with the standardised dimension names.   dataset Parent key for holding configuration specific to the dataset files.   domain Parent key for holding configuration specific to domain files. This is an optional key depending on whether a domain file is required or not.   dimension_map Child key of dataset/domain. A dictionary defining the mappings between input data dimension names and the framework\u0026rsquo;s standardised dimension names.   variable_map Child key of dataset/domain. A dictionary defining the mappings between input data variable names and the framework\u0026rsquo;s standardised variable names.   keep_all_vars Optional child key of dataset/domain. If \u0026ldquo;True\u0026rdquo;, all variables from the input datafile will be carried over to the Gridded dataset. If \u0026ldquo;False\u0026rdquo;, only mapped variables will be carried over. \u0026ldquo;False\u0026rdquo; is assumed if the key is not present.   coord_vars Child key of dataset. A list of dataset coordinate variables to apply once dataset is loaded.   static_variables Parent key for holding configuration used for merging domain variables into the main dataset.   not_grid_vars Child key of static_variables. A list of grid independant variables to pull across from the domain file.   delete_vars Child key of static_variables. A list of variables to drop following the merge of domain and dataset.   processing_flags A list of strings referring to any preliminary processing methods to be carried out on the data.    Indexed configuration    Key Description     type A string value representing the type of configuration file. In the case of indexed config this will always be \u0026ldquo;indexed\u0026rdquo;.   dimensionality An integer value representing the number of dimensions within the data files.   chunks A dictionary defining a dask chunk shape, used when loading in data files. JSON doesn\u0026rsquo;t support integer keys, and so the dimensions name should be provided as the key instead. An empty dictionary will result in auto chunking. Rechunking can be applied subsequently with the standardised dimension names.   dataset Parent key for holding configuration specific to the dataset files.   dimension_map Child key of dataset. A dictionary defining the mappings between input data dimension names and the framework\u0026rsquo;s standardised dimension names.   variable_map Child key of dataset. A dictionary defining the mappings between input data variable names and the framework\u0026rsquo;s standardised variable names.   keep_all_vars Optional child key of dataset/domain. If \u0026ldquo;True\u0026rdquo;, all variables from the input datafile will be carried over to the Indexed dataset. If \u0026ldquo;False\u0026rdquo;, only mapped variables will be carried over. \u0026ldquo;False\u0026rdquo; is assumed if the key is not present.   coord_vars Child key of dataset. A list of dataset coordinate variables to apply once dataset is loaded.   processing_flags A list of strings referring to any preliminary processing methods to be carried out on the data.    Example configuration file Below is the template of a gridded configuration file:\n{ \u0026#34;type\u0026#34;: \u0026#34;gridded\u0026#34;, \u0026#34;dimensionality\u0026#34;: 4, \u0026#34;chunks\u0026#34;: { \u0026#34;time_counter\u0026#34;:2, \u0026#34;x\u0026#34;:4, \u0026#34;y\u0026#34;:4 }, \u0026#34;grid_ref\u0026#34;: { \u0026#34;t-grid\u0026#34;: [ \u0026#34;glamt\u0026#34;, \u0026#34;gphit\u0026#34;, \u0026#34;e1t\u0026#34;, \u0026#34;e2t\u0026#34;, \u0026#34;e3t_0\u0026#34;, \u0026#34;deptht_0\u0026#34;, \u0026#34;tmask\u0026#34;, \u0026#34;bottom_level\u0026#34;, \u0026#34;hbatt\u0026#34; ] }, \u0026#34;dataset\u0026#34;: { \u0026#34;dimension_map\u0026#34;: { \u0026#34;time_counter\u0026#34;: \u0026#34;t_dim\u0026#34;, \u0026#34;deptht\u0026#34;: \u0026#34;z_dim\u0026#34;, \u0026#34;y\u0026#34;: \u0026#34;y_dim\u0026#34;, \u0026#34;x\u0026#34;: \u0026#34;x_dim\u0026#34;, \u0026#34;x_grid_T\u0026#34;: \u0026#34;x_dim\u0026#34;, \u0026#34;y_grid_T\u0026#34;: \u0026#34;y_dim\u0026#34; }, \u0026#34;variable_map\u0026#34;: { \u0026#34;time_counter\u0026#34;: \u0026#34;time\u0026#34;, \u0026#34;votemper\u0026#34;: \u0026#34;temperature\u0026#34;, \u0026#34;thetao\u0026#34;: \u0026#34;temperature\u0026#34;, \u0026#34;temp\u0026#34;: \u0026#34;temperature\u0026#34;, \u0026#34;toce\u0026#34;: \u0026#34;temperature\u0026#34;, \u0026#34;so\u0026#34;: \u0026#34;salinity\u0026#34;, \u0026#34;vosaline\u0026#34;: \u0026#34;salinity\u0026#34;, \u0026#34;soce\u0026#34;: \u0026#34;salinity\u0026#34;, \u0026#34;sossheig\u0026#34;: \u0026#34;ssh\u0026#34;, \u0026#34;zos\u0026#34;: \u0026#34;ssh\u0026#34; }, \u0026#34;coord_vars\u0026#34;: [ \u0026#34;longitude\u0026#34;, \u0026#34;latitude\u0026#34;, \u0026#34;time\u0026#34;, \u0026#34;depth_0\u0026#34; ] }, \u0026#34;domain\u0026#34;: { \u0026#34;dimension_map\u0026#34;: { \u0026#34;t\u0026#34;: \u0026#34;t_dim0\u0026#34;, \u0026#34;x\u0026#34;: \u0026#34;x_dim\u0026#34;, \u0026#34;y\u0026#34;: \u0026#34;y_dim\u0026#34;, \u0026#34;z\u0026#34;: \u0026#34;z_dim\u0026#34; }, \u0026#34;variable_map\u0026#34;: { \u0026#34;time_counter\u0026#34;: \u0026#34;time0\u0026#34;, \u0026#34;glamt\u0026#34;: \u0026#34;longitude\u0026#34;, \u0026#34;gphit\u0026#34;: \u0026#34;latitude\u0026#34;, \u0026#34;e1t\u0026#34;: \u0026#34;e1\u0026#34;, \u0026#34;e2t\u0026#34;: \u0026#34;e2\u0026#34;, \u0026#34;e3t_0\u0026#34;: \u0026#34;e3_0\u0026#34;, \u0026#34;tmask\u0026#34;:\u0026#34;mask\u0026#34;, \u0026#34;deptht_0\u0026#34;: \u0026#34;depth_0\u0026#34;, \u0026#34;bottom_level\u0026#34;: \u0026#34;bottom_level\u0026#34;, \u0026#34;hbatt\u0026#34;:\u0026#34;bathymetry\u0026#34; } }, \u0026#34;static_variables\u0026#34;: { \u0026#34;not_grid_vars\u0026#34;: [ \u0026#34;jpiglo\u0026#34;, \u0026#34;jpjglo\u0026#34;, \u0026#34;jpkglo\u0026#34;, \u0026#34;jperio\u0026#34;, \u0026#34;ln_zco\u0026#34;, \u0026#34;ln_zps\u0026#34;, \u0026#34;ln_sco\u0026#34;, \u0026#34;ln_isfcav\u0026#34; ], \u0026#34;delete_vars\u0026#34;: [ \u0026#34;nav_lat\u0026#34;, \u0026#34;nav_lon\u0026#34;, \u0026#34;deptht\u0026#34; ] }, \u0026#34;processing_flags\u0026#34;: [ \u0026#34;example_flag1\u0026#34;, \u0026#34;example_flag2\u0026#34; ] } Example configuration files can be found in the config/ directory within the COAsT github repository.\n","excerpt":"Configuration file code can be found in coast/config within the COAsT github repository. This code …","ref":"/COAsT/docs/examples/configuration_files/","title":"Configuration files"},{"body":"AMM15 - 1.5km resolution Atlantic Margin Model \u0026#34;\u0026#34;\u0026#34; AMM15_example_plot.py Make simple AMM15 SST plot. \u0026#34;\u0026#34;\u0026#34; #%% import coast import numpy as np import xarray as xr import matplotlib.pyplot as plt import matplotlib.colors as colors # colormap fiddling ################################################# #%% Loading data ################################################# config = \u0026#39;AMM15\u0026#39; dir_nam = \u0026#34;/projectsa/NEMO/gmaya/2013p2/\u0026#34; fil_nam = \u0026#34;20130415_25hourm_grid_T.nc\u0026#34; dom_nam = \u0026#34;/projectsa/NEMO/gmaya/AMM15_GRID/amm15.mesh_mask.cs3x.nc\u0026#34; config = \u0026#34;/work/jelt/GitHub/COAsT/config/example_nemo_grid_t.json\u0026#34; sci_t = coast.Gridded(dir_nam + fil_nam, dom_nam, config=config) # , chunks=chunks) chunks = { \u0026#34;x_dim\u0026#34;: 10, \u0026#34;y_dim\u0026#34;: 10, \u0026#34;t_dim\u0026#34;: 10, } # Chunks are prescribed in the config json file, but can be adjusted while the data is lazy loaded. sci_t.dataset.chunk(chunks) # create an empty w-grid object, to store stratification sci_w = coast.Gridded(fn_domain=dom_nam, config=config.replace(\u0026#34;t_nemo\u0026#34;, \u0026#34;w_nemo\u0026#34;)) sci_w.dataset.chunk({\u0026#34;x_dim\u0026#34;: 10, \u0026#34;y_dim\u0026#34;: 10}) # Can reset after loading config json print(\u0026#39;* Loaded \u0026#39;,config, \u0026#39; data\u0026#39;) ################################################# #%% subset of data and domain ## ################################################# # Pick out a North Sea subdomain print(\u0026#39;* Extract North Sea subdomain\u0026#39;) ind_sci = sci_t.subset_indices([51,-4], [62,15]) sci_nwes_t = sci_t.isel(y_dim=ind_sci[0], x_dim=ind_sci[1]) #nwes = northwest europe shelf ind_sci = sci_w.subset_indices([51,-4], [62,15]) sci_nwes_w = sci_w.isel(y_dim=ind_sci[0], x_dim=ind_sci[1]) #nwes = northwest europe shelf #%% Apply masks to temperature and salinity if config == \u0026#39;AMM15\u0026#39;: sci_nwes_t.dataset[\u0026#39;temperature_m\u0026#39;] = sci_nwes_t.dataset.temperature.where( sci_nwes_t.dataset.mask.expand_dims(dim=sci_nwes_t.dataset[\u0026#39;t_dim\u0026#39;].sizes) \u0026gt; 0) sci_nwes_t.dataset[\u0026#39;salinity_m\u0026#39;] = sci_nwes_t.dataset.salinity.where( sci_nwes_t.dataset.mask.expand_dims(dim=sci_nwes_t.dataset[\u0026#39;t_dim\u0026#39;].sizes) \u0026gt; 0) else: # Apply fake masks to temperature and salinity sci_nwes_t.dataset[\u0026#39;temperature_m\u0026#39;] = sci_nwes_t.dataset.temperature sci_nwes_t.dataset[\u0026#39;salinity_m\u0026#39;] = sci_nwes_t.dataset.salinity #%% Plots fig = plt.figure() plt.pcolormesh( sci_t.dataset.longitude, sci_t.dataset.latitude, sci_t.dataset.temperature.isel(z_dim=0).squeeze()) #plt.xlabel(\u0026#39;longitude\u0026#39;) #plt.ylabel(\u0026#39;latitude\u0026#39;) #plt.colorbar() plt.axis(\u0026#39;off\u0026#39;) plt.show() fig.savefig(\u0026#39;AMM15_SST_nocolorbar.png\u0026#39;, dpi=120)    India subcontinent maritime domain. WCSSP India configuration #%% import coast import numpy as np import xarray as xr import dask import matplotlib.pyplot as plt import matplotlib.colors as colors # colormap fiddling ################################################# #%% Loading data ################################################# dir_nam = \u0026#34;/projectsa/COAsT/NEMO_example_data/MO_INDIA/\u0026#34; fil_nam = \u0026#34;ind_1d_cat_20180101_20180105_25hourm_grid_T.nc\u0026#34; dom_nam = \u0026#34;domain_cfg_wcssp.nc\u0026#34; config_t = \u0026#34;/work/jelt/GitHub/COAsT/config/example_nemo_grid_t.json\u0026#34; sci_t = coast.Gridded(dir_nam + fil_nam, dir_nam + dom_nam, config=config_t) #%% Plot fig = plt.figure() plt.pcolormesh( sci_t.dataset.longitude, sci_t.dataset.latitude, sci_t.dataset.temperature.isel(t_dim=0).isel(z_dim=0)) plt.xlabel(\u0026#39;longitude\u0026#39;) plt.ylabel(\u0026#39;latitude\u0026#39;) plt.title(\u0026#39;WCSSP India SST\u0026#39;) plt.colorbar() plt.show() fig.savefig(\u0026#39;WCSSP_India_SST.png\u0026#39;, dpi=120)    South East Asia, 1/12 deg configuration (ACCORD: SEAsia_R12) #%% import coast import numpy as np import xarray as xr import dask import matplotlib.pyplot as plt import matplotlib.colors as colors # colormap fiddling ################################################# #%% Loading data ################################################# dir_nam = \u0026#34;/projectsa/COAsT/NEMO_example_data/SEAsia_R12/\u0026#34; fil_nam = \u0026#34;SEAsia_R12_5d_20120101_20121231_gridT.nc\u0026#34; dom_nam = \u0026#34;domain_cfg_ORCA12_adj.nc\u0026#34; config_t = \u0026#34;/work/jelt/GitHub/COAsT/config/example_nemo_grid_t.json\u0026#34; sci_t = coast.Gridded(dir_nam + fil_nam, dir_nam + dom_nam, config=config_t) #%% Plot fig = plt.figure() plt.pcolormesh( sci_t.dataset.longitude, sci_t.dataset.latitude, sci_t.dataset.soce.isel(t_dim=0).isel(z_dim=0)) plt.xlabel(\u0026#39;longitude\u0026#39;) plt.ylabel(\u0026#39;latitude\u0026#39;) plt.title(\u0026#39;SE Asia, surface salinity (psu)\u0026#39;) plt.colorbar() plt.show() fig.savefig(\u0026#39;SEAsia_R12_SSS.png\u0026#39;, dpi=120)    ","excerpt":"AMM15 - 1.5km resolution Atlantic Margin Model \u0026#34;\u0026#34;\u0026#34; AMM15_example_plot.py Make simple …","ref":"/COAsT/docs/examples/configs_gallery/","title":"Configuration Gallery"},{"body":"__________________________________________________________________________________________ ______ ___ _ _________ .' ___ | .' `. / \\ | _ _ | / .' \\_|/ .-. \\ / _ \\ .--.|_/ | | \\_| | | | | | | / ___ \\ ( (`\\] | | \\ `.___.'\\\\ `-' /_/ / \\ \\_ `'.'. _| |_ `.____ .' `.___.'|____| |____|[\\__) )|_____| Coastal Ocean Assessment Toolbox __________________________________________________________________________________________ COAsT is a Python package for managing and analysing high resolution NEMO output. Here you can find information on obtaining, installing and using COAsT as well as guidelines for contributing to the project.\nThis documentation site is still under construction but you can still find guidelines for contributing to the package and this website. See below for description of each section.\n","excerpt":"__________________________________________________________________________________________ ______ …","ref":"/COAsT/docs/","title":"Documentation"},{"body":"","excerpt":"","ref":"/COAsT/docs/reference/parameter-reference/","title":"Parameter Reference"},{"body":"  #td-cover-block-0 { background-image: url(/COAsT/about/featured-background_hu14d69772da4446f8c45afbc4cad362c8_132726_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/COAsT/about/featured-background_hu14d69772da4446f8c45afbc4cad362c8_132726_1920x1080_fill_q75_catmullrom_top.jpg); } }  About COAsT A site using the Docsy Hugo theme. --        COAsT is a Python package for managing and analysing high resolution NEMO output Read more here     This site was based off the Docsy Hugo theme.    ","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/COAsT/about/","title":"About Goldydocs"},{"body":"  #td-cover-block-0 { background-image: url(/COAsT/featured-background_hu14d69772da4446f8c45afbc4cad362c8_132726_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/COAsT/featured-background_hu14d69772da4446f8c45afbc4cad362c8_132726_1920x1080_fill_q75_catmullrom_top.jpg); } }  Welcome to the documentation: A Docsy site for COAsT Learn More   Download   COAsT\n\n        This is a single web UI providing visibility into the COAsT python framework.       Download from Anaconda.org Get the COAsT framework!\nRead more …\n   Contributions welcome! We do a Pull Request contributions workflow on GitHub. New users are always welcome!\nRead more …\n   Follow us on Twitter! For announcement of latest features etc.\nRead more …\n    ","excerpt":"#td-cover-block-0 { background-image: …","ref":"/COAsT/","title":"COAsT"},{"body":"","excerpt":"","ref":"/COAsT/community/","title":"Community"},{"body":"","excerpt":"","ref":"/COAsT/search/","title":"Search Results"}]