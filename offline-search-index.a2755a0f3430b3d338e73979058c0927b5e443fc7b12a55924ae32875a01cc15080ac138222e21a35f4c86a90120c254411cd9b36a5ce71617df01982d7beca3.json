[{"body":"","excerpt":"","ref":"/COAsT/docs/examples/notebooks/gridded/","title":"Gridded"},{"body":"","excerpt":"","ref":"/COAsT/docs/examples/notebooks/","title":"Notebooks"},{"body":"COAsT (Coastal Ocean Assessment Toolkit) is a diagnostics and assessment python toolbox for kilometric scale regional models. The aim is that this toolbox is community-ready and flexible.\nThe initial focus will be on delivering a limited number of novel diagnostics for NEMO configurations, but that the toolbox would be expanded to include other diagnostics and other ocean models.\n","excerpt":"COAsT (Coastal Ocean Assessment Toolkit) is a diagnostics and assessment python toolbox for …","ref":"/COAsT/docs/overview/","title":"Overview"},{"body":"Python as a language comes with more stringent recommendations than most when it comes to code styling. This is advantageous in our case as it gives us an obvious set of guidelines to adopt.\nWhen it comes to simple code styling, much of what\u0026rsquo;s recommended here will be copied from Python Enhancement Proposal (PEP) 8, an officially proposed and accepted Python style guide.\nCode Styling Conventions Let\u0026rsquo;s keep things simple to start with\u0026hellip;\n  Indentation should be achieved with spaces rather than tabs and each new level of indentation should be indented by four columns (i.e four spaces).\n  Any single line, including its indentation characters, should not exceed 79 characters in length.\n  Top-level (i.e at the module/file level rather than inside a function or class) function and class definitions should be separated by two blank lines.\n  Method (functions within a class) definitions are separated by a single blank line.\n  Usually, \u0026ldquo;import\u0026rdquo; statements should be on separate lines, that is to say that you should have one line per distinct module or package import. An exception to this rule is when multiple objects are imported from a single module or package, using a \u0026ldquo;from\u0026rdquo; statement, in which case individual objects can be imported on the same line, separated by commas.\n  PEP 8 does not make a recommendation relating to the use of double or single quotes in general use, but for the sake of consistency, this document suggests the use of double quotes wherever practical. This recommendation is intended for the sake of consistency with triple-quoted strings, as per Docstring Conventions (PEP 257).\n  Operators should be separated by single columns (i.e one space) either side, unless inside parentheses, in which case no whitespace is required.\n  Comments (beginning with the # character) should be indented as if they were code. In the case of inline comments, separate the comment with two spaces following the code it shares the line with.\n  All functions should contain a docstring, which provides basic information on its usage. For this project, the reStructuredText docstring format is suggested.\n  When it comes to naming variables and functions, snake case (lower_case_words_separated_by_underscores) is preferred. There are however a few exceptions to this rule: Class names should be styled as camel case (EveryNewWordIsCapitalised). Constants (Variables that should not be changed) can be indicated by the use of screaming snake case (UPPER_CASE_WORDS_SEPARATED_BY_UNDERSCORES). Note that this library currently targets Python 3.7, so the use of typing.Final official support for constant variables, new as of Python 3.8: is not currently supported.\n  In general, it is suggested to avoid the use of single-character variable names, but this is acceptable in certain cases, such as when defining coordinates (such as x, y and z), as these will be commonly recognized and enforcing different rules could cause confusion. PEP 8 advises the following regarding names to avoid: \u0026ldquo;Never use the characters \u0026lsquo;l\u0026rsquo; (lowercase letter el), \u0026lsquo;O\u0026rsquo; (uppercase letter oh), or \u0026lsquo;I\u0026rsquo; (uppercase letter eye) as single character variable names.\u0026rdquo; These specific characters should be avoided because they present an accessibility issue, as under many fonts these characters may be difficult to distinguish or completely indistinguishable from numerals one (1) and zero (0).\n  In the interest of readability, where named iterator variables are required, this document suggests the use of double characters (e.g. \u0026ldquo;ii\u0026rdquo; rather than \u0026ldquo;i\u0026rdquo;).\n  Object-Oriented Programming The general principles of OOP are fairly straightforward and well documented, so I won\u0026rsquo;t waste your precious time by regurgitating that particular wall of text here. Instead, I\u0026rsquo;ll focus on some general pointers specific to this language and use case.\n  In Python, all class attributes are technically public, but semantically, attributes can be designated as non-public by including leading underscores in the name. For instance, \u0026ldquo;my_variable\u0026rdquo; becomes \u0026ldquo;_my_variable\u0026rdquo;. These attributes are generally referred to as \u0026ldquo;protected\u0026rdquo;.\n  When you define a Python class, it is a best practice to inherit from the base object type. This convention stems from Python 2.X, as classes and types were not originally synonymous. This behaviour is implicit in Python 3.X but the convention has persisted nonetheless. Classes defined this way are referred to as \u0026ldquo;new-style\u0026rdquo; classes.\n  When defining a class that inherits from another, it is important to remember that overridden methods (in particular, this behaviour is important when dealing with __init__ methods) do not implicitly call the parent method. What this means is that unless you want to deliberately prevent the behaviour of the parent class (this is a very niche use-case), it is important to include a reference to the parent method. An example of this is: super().__init__() This functionality is advantageous as it prevents unnecessary duplication of code, which is a key tenet of object-oriented software.\n  ","excerpt":"Python as a language comes with more stringent recommendations than most when it comes to code …","ref":"/COAsT/docs/contributing_package/python_style/","title":"Python: Style"},{"body":"** Notes on Object Structure and Loading (for contributors):\nCOAsT is an object-orientated package, meaning that data is stored within Python object structures. In addition to data storage, these objects contain methods (subroutines) which allow for manipulation of this data. An example of such an object is the Gridded object, which allows for the storage and manipulation of e.g. NEMO output and domain data. It is important to understand how to load data using COAsT and the structure of the resulting objects.\nA Gridded object is created and initialised by passing it the paths of the domain and data files. Ideally, the grid type should also be specified (T, U, V or F in the case of NEMO). For example, to load in data from a file containing data on a NEMO T-grid:\nimport coast fn_data = \u0026quot;\u0026lt;path to T-grid data file(s)\u0026gt;\u0026quot; fn_domain = \u0026quot;\u0026lt;path to domain file\u0026gt;\u0026quot; fn_config = \u0026quot;\u0026lt;path to json config file\u0026gt;\u0026quot; data = coast.Gridded(fn_data, fn_domain, fn_config) Ideally, Gridded model output data should be in grid-specific files, i.e. containing output variables situated on a NEMO T, U, V or F grid, whereas the grid variables are in a single domain file. On loading into COAsT, only the grid specific variables appropriate for the paired data are placed into the Gridded object. A Gridded object therefore contains grid-specific data and all corresponding grid variables. One of the file names can be omitted (to get a data-only or grid only object), however functionality in this case will be limited.\nOnce loaded, data is stored inside the object using an xarray.dataset object. Following on from the previous code example, this can be viewed by calling:\ndata.dataset This reveals all netcdf-type aspects of the data and domain variables that were loaded, including dimensions, coordinates, variables and attributes. For example:\n\u0026lt;xarray.Dataset\u0026gt; Dimensions: (axis_nbounds: 2, t_dim: 7, x_dim: 297, y_dim: 375, z_dim: 51) Coordinates: time (t_dim) datetime64[ns] 2007-01-01T11:58:56 ... 2007-01-31T11:58:56 longitude (y_dim, x_dim) float32 ... latitude (y_dim, x_dim) float32 ... Dimensions without coordinates: axis_nbounds, t_dim, x_dim, y_dim, z_dim Data variables: deptht_bounds (z_dim, axis_nbounds) float32 ... sossheig (t_dim, y_dim, x_dim) float32 ... time_counter_bounds (t_dim, axis_nbounds) datetime64[ns] ... time_instant (t_dim) datetime64[ns] ... temperature (t_dim, z_dim, y_dim, x_dim) float32 ... e1 (y_dim, x_dim) float32 ... e2 (y_dim, x_dim) float32 ... e3_0 (z_dim, y_dim, x_dim) float32 1.0 1.0 1.0 ... 1.0 1.0 Variables may be obtained in a number of ways. For example, to get temperature data, the following are all equivalent:\ntemp = data.dataset.temperature temp = data.dataset['temperature'] temp = data['temperature'] These commands will all return an xarray.dataarray object. Manipulation of this object can be done using xarray commands, for example indexing using [] or xarray.isel. Be aware that indexing will preserve lazy loading, however and direct access or modifying of the data will not. For this reason, if you require a subset of the data, it is best to index first.\nThe names of common grid variables are standardised within the COAsT package using JSON configuration files. For example, the following lists COAsT internal variable followed by the typical NEMO variable names:\n longitude [glamt / glamu / glamv / glamf] latitude [gphit / gphiu / gphiv / gphif] time [time_counter] e1 [e1t / e1u / e1v / e1f] (dx variable) e2 [e1t / e1u / e1v / e1f] (dy variable) e3_0 [e3t_0 / e3u_0 / e3v_0 / e3f_0] (dz variable at time 0)  Longitude, latitude and time are also set as coordinates. You might notice that dimensions are also standardised:\n x_dim The dimension for the x-axis (longitude) y_dim The dimension for the y-axis (latitude) t_dim The dimension for the time axis z_dim The dimension for the depth axis.  Wherever possible, the aim is to ensure that all of the above is consistent across the whole COAsT toolbox. Therefore, you will also find the same names and dimensions in, for example observation objects. Future objects, where applicable, will also follow these conventions. If you (as a contributor) add new objects to the toolbox, following the above template is strongly encouraged. This includes using xarray dataset/dataarray objects where possible, adopting an object oriented approach and adhering to naming conventions.\n","excerpt":"** Notes on Object Structure and Loading (for contributors):\nCOAsT is an object-orientated package, …","ref":"/COAsT/docs/contributing_package/python_structure/","title":"Python: Structure"},{"body":"","excerpt":"","ref":"/COAsT/docs/examples/notebooks/general/","title":"General utility and analysis tools"},{"body":"Prerequisites This package requires;\n a linux environment or docker for Windows python version 3.8.10 Miniconda  Basic use installation via conda or pip This package should be installed by run;\nconda install -c bodc coast However, there is also the option of;\npip install COAsT Development use installation If you would prefer to work with a clone of the repository in a development python environment do the following. First clone the repository in the place where you want to work:\ngit clone https://github.com/British-Oceanographic-Data-Centre/COAsT.git cd COAsT Then build a python environment:\nconda env update --prune --file environment.yml conda activate coast Building the docker image and executing an interactive environment Warning, building the image is resource heavy.\nAfter cloning the repo (as above).\ndocker build . --tag coast docker compose up -d docker compose exec coast bash You can now start a python session and import coast. docker compose mounts 3 directories from you host machine onto the docker container:\n./example_files:/example_files\n./config:/config\n./example_scripts:/example_scripts\nObtaining Example files In order to try the Examples, example data files and configuration files are recommended.\nExample data files Download example files and link them into a new directory:\nwget -c https://linkedsystems.uk/erddap/files/COAsT_example_files/COAsT_example_files.zip \u0026amp;\u0026amp; unzip COAsT_example_files.zip Example configuration files To facilitate loading different types of data, key information is passed to COAsT using configuration files. The config files used in the Examples are in the repository, or can be downloaded as static files:\nwget -c https://github.com/British-Oceanographic-Data-Centre/COAsT/archive/refs/heads/master.zip \u0026amp;\u0026amp; unzip master.zip Test it! The below example works best with the COAsT example data. Start by opening a python terminal and then importing COAsT:\nimport coast Before using coast, we will just check that Anaconda has installed correct package versions. In the python console copy the following:\nimport gsw import matplotlib print(gsw.__version__) print(matplotlib.__version__) The output should be\n3.4.0 3.5.1 or later. If it is, great carry on. If it is not, problems may occur with some functionality in coast. Please get in contact using the contacts in the workshop email.\nTake a look at the example pages for more information on specific objects and methods.\n","excerpt":"Prerequisites This package requires;\n a linux environment or docker for Windows python version …","ref":"/COAsT/docs/getting-started/","title":"Getting Started"},{"body":"For use on Liverpool servers only\nPrerequisites This package requires;\n python version 3.8+ Anaconda version 4.10+  Are there any system requirements for using this project? What languages are supported (if any)? Do users need to already have any software or tools installed?\nBasic use installation via conda or pip This package should be installed by run;\nconda install -c bodc coast However, there is also the option of;\npip install COAsT if you wish to install from source then got to GitHub and follow the README instructions\nThe base package should now be installed on your system. The following packages might be required for some of the advanced plotting features;\n cartopy  Development use installation If you would prefer to work with a clone of the repository in a development python environment do the following. First clone the repoitory in the place where you want to work:\ngit clone https://github.com/British-Oceanographic-Data-Centre/COAsT.git Then start building a python environment. Here (for example) called coast_dev:\nmodule load anaconda/5-2021 # or whatever it takes to activate conda conda config --add channels conda-forge # add conda-forge to your conda channels conda create -n coast_dev python=3.8 # create a new environment. E.g. `coast_dev` conda activate coast_dev # activate new environment Install packages to the environment:\ncd COAsT conda install --file conda_dev_requirements.txt Obtaining Example files In order to try the Examples, example data files and configuration files are recommended.\nExample data files Download example files and link them into a new directory:\nrm -rf coast_demo mkdir coast_demo cd coast_demo wget -c https://linkedsystems.uk/erddap/files/COAsT_example_files/COAsT_example_files.zip \u0026amp;\u0026amp; unzip COAsT_example_files.zip ln -s COAsT_example_files example_files Example configuration files To facilitate loading different types of data, key information is passed to COAsT using configuration files. The config files used in the Examples are in the repository, or can be downloaded as static files:\ncd ../coast_demo wget -c https://github.com/British-Oceanographic-Data-Centre/COAsT/archive/refs/heads/master.zip \u0026amp;\u0026amp; unzip master.zip ln -s COAsT-master/config config Preparation for Workshop Package Installation with conda Assuming a linux environment and that you have anaconda on your system:\n## Fresh build in new conda environment module load anaconda/5-2021 # or whatever it takes to activate conda yes | conda env remove --name workshop_env # remove environment \u0026#39;workshop_env\u0026#39; if it exists yes | conda create --name workshop_env python=3.8 # create a new environment conda activate workshop_env # activate new environment yes | conda install -c bodc coast=2.0.3 # install COAsT within new environment yes | conda install -c conda-forge cartopy=0.20.2 # install cartopy Then obtain the Example data and configuration files (as above).\nExternal Requirements All required packages should be defined in the environment.yml.\nTest it! The below example works best with the COAsT example data. Start by opening a python terminal and then importing COAsT:\nimport coast Before using coast, we will just check that Anaconda has installed correct package versions. In the python console copy the following:\nimport gsw import matplotlib print(gsw.__version__) print(matplotlib.__version__) The output should be\n3.4.0 3.5.1 or later. If it is, great carry on. If it is not, problems may occur with some functionality in coast. Please get in contact using the contacts in the workshop email.\nTake a look at the example pages for more information on specific objects and methods.\n","excerpt":"For use on Liverpool servers only\nPrerequisites This package requires;\n python version 3.8+ Anaconda …","ref":"/COAsT/docs/getting-started/getting-started-at-liverpool/","title":"Getting Started at Liverpool"},{"body":"GitHub actions diagram This is a collection of flowcharts for all the GitHub actions used across the COAsT and COAsT-site repos\nCOAsT building Packages   let isDark = window.matchMedia('(prefers-color-scheme: dark)').matches; let mermaidTheme = (isDark) ? 'dark' : 'default'; let mermaidConfig = { theme: mermaidTheme, logLevel: 'fatal', securityLevel: 'strict', startOnLoad: true, arrowMarkerAbsolute: false, er: { diagramPadding: 20, layoutDirection: 'TB', minEntityWidth: 100, minEntityHeight: 75, entityPadding: 15, stroke: 'gray', fill: 'honeydew', fontSize: 12, useMaxWidth: true, }, flowchart: { diagramPadding: 8, htmlLabels: true, curve: 'basis', }, sequence: { diagramMarginX: 50, diagramMarginY: 10, actorMargin: 50, width: 150, height: 65, boxMargin: 10, boxTextMargin: 5, noteMargin: 10, messageMargin: 35, messageAlign: 'center', mirrorActors: true, bottomMarginAdj: 1, useMaxWidth: true, rightAngles: false, showSequenceNumbers: false, }, gantt: { titleTopMargin: 25, barHeight: 20, barGap: 4, topPadding: 50, leftPadding: 75, gridLineStartPadding: 35, fontSize: 11, fontFamily: '\"Open-Sans\", \"sans-serif\"', numberSectionStyles: 4, axisFormat: '%Y-%m-%d', topAxis: false, }, }; mermaid.initialize(mermaidConfig);  graph LR; subgraph publish_package - runs on push to master A1[Setup python]-- 3.8 --B1; B1[Install dependencies]--C1; C1[Setup Enviroment]--D1; D1[Build package]--E1; E1[Test Package Install]--F1 F1[Publish to pypi]--G1 G1[Generate Conda Metadata]--H1 H1[Publish to Anaconda] end; subgraph build_package - runs on push to non-master A[Setup python]-- 3.8 and 3.9 --B; B[Install dependencies]--C; C[Setup Enviroment]--D; D[Build package]--E; E[Test Package Install]--F F[Generate Conda Metadata] end;  Verification and Formatting   let isDark = window.matchMedia('(prefers-color-scheme: dark)').matches; let mermaidTheme = (isDark) ? 'dark' : 'default'; let mermaidConfig = { theme: mermaidTheme, logLevel: 'fatal', securityLevel: 'strict', startOnLoad: true, arrowMarkerAbsolute: false, er: { diagramPadding: 20, layoutDirection: 'TB', minEntityWidth: 100, minEntityHeight: 75, entityPadding: 15, stroke: 'gray', fill: 'honeydew', fontSize: 12, useMaxWidth: true, }, flowchart: { diagramPadding: 8, htmlLabels: true, curve: 'basis', }, sequence: { diagramMarginX: 50, diagramMarginY: 10, actorMargin: 50, width: 150, height: 65, boxMargin: 10, boxTextMargin: 5, noteMargin: 10, messageMargin: 35, messageAlign: 'center', mirrorActors: true, bottomMarginAdj: 1, useMaxWidth: true, rightAngles: false, showSequenceNumbers: false, }, gantt: { titleTopMargin: 25, barHeight: 20, barGap: 4, topPadding: 50, leftPadding: 75, gridLineStartPadding: 35, fontSize: 11, fontFamily: '\"Open-Sans\", \"sans-serif\"', numberSectionStyles: 4, axisFormat: '%Y-%m-%d', topAxis: false, }, }; mermaid.initialize(mermaidConfig);  graph LR subgraph formatting - runs on pull requests A[Setup python]-- 3.9 --B; B[Install black]--C; C[Check formatting]-- D; D[Apply formatting] end; subgraph verifiy_package - runs for every push A1[Setup python]-- 3.8 and 3.9 --B1; B1[Install dependencies]--C1; C1[Lint]--D1; D1[Test] end; click B1 \"https://www.github.com\" \"tooltip\"  interactions with other repos   let isDark = window.matchMedia('(prefers-color-scheme: dark)').matches; let mermaidTheme = (isDark) ? 'dark' : 'default'; let mermaidConfig = { theme: mermaidTheme, logLevel: 'fatal', securityLevel: 'strict', startOnLoad: true, arrowMarkerAbsolute: false, er: { diagramPadding: 20, layoutDirection: 'TB', minEntityWidth: 100, minEntityHeight: 75, entityPadding: 15, stroke: 'gray', fill: 'honeydew', fontSize: 12, useMaxWidth: true, }, flowchart: { diagramPadding: 8, htmlLabels: true, curve: 'basis', }, sequence: { diagramMarginX: 50, diagramMarginY: 10, actorMargin: 50, width: 150, height: 65, boxMargin: 10, boxTextMargin: 5, noteMargin: 10, messageMargin: 35, messageAlign: 'center', mirrorActors: true, bottomMarginAdj: 1, useMaxWidth: true, rightAngles: false, showSequenceNumbers: false, }, gantt: { titleTopMargin: 25, barHeight: 20, barGap: 4, topPadding: 50, leftPadding: 75, gridLineStartPadding: 35, fontSize: 11, fontFamily: '\"Open-Sans\", \"sans-serif\"', numberSectionStyles: 4, axisFormat: '%Y-%m-%d', topAxis: false, }, }; mermaid.initialize(mermaidConfig);  flowchart LR subgraph b1[push_notebooks - runs on push to develop] direction LR subgraph b2[COAsT site - markdown ] direction TB a[checkout docsy site] --b b[checkout coast] --c c[create environment] --d d[execute notebooks] --e e[covert notebooks to MD] --f f[move images to static dir] --g g[commit changes] end t[Repository Dispatch] -- event pushed -- b2 end click a \"https://github.com/British-Oceanographic-Data-Centre/COAsT-site\" \"Docsy site for COAsT repo\"    let isDark = window.matchMedia('(prefers-color-scheme: dark)').matches; let mermaidTheme = (isDark) ? 'dark' : 'default'; let mermaidConfig = { theme: mermaidTheme, logLevel: 'fatal', securityLevel: 'strict', startOnLoad: true, arrowMarkerAbsolute: false, er: { diagramPadding: 20, layoutDirection: 'TB', minEntityWidth: 100, minEntityHeight: 75, entityPadding: 15, stroke: 'gray', fill: 'honeydew', fontSize: 12, useMaxWidth: true, }, flowchart: { diagramPadding: 8, htmlLabels: true, curve: 'basis', }, sequence: { diagramMarginX: 50, diagramMarginY: 10, actorMargin: 50, width: 150, height: 65, boxMargin: 10, boxTextMargin: 5, noteMargin: 10, messageMargin: 35, messageAlign: 'center', mirrorActors: true, bottomMarginAdj: 1, useMaxWidth: true, rightAngles: false, showSequenceNumbers: false, }, gantt: { titleTopMargin: 25, barHeight: 20, barGap: 4, topPadding: 50, leftPadding: 75, gridLineStartPadding: 35, fontSize: 11, fontFamily: '\"Open-Sans\", \"sans-serif\"', numberSectionStyles: 4, axisFormat: '%Y-%m-%d', topAxis: false, }, }; mermaid.initialize(mermaidConfig);  flowchart LR subgraph b3[push_docstrings - runs on push to master] direction LR subgraph b4[COAsT site - docstrings ] direction TB a1[checkout docsy site] --b1 b1[checkout coast] --c1 c1[add python] --d1 d1[covert docstrings] --e1 e1[commit changes] end r[Repository Dispatch] -- event pushed -- b4 end click a1 \"https://github.com/British-Oceanographic-Data-Centre/COAsT-site\" \"Docsy site for COAsT repo\"  Generate unit test contents file   let isDark = window.matchMedia('(prefers-color-scheme: dark)').matches; let mermaidTheme = (isDark) ? 'dark' : 'default'; let mermaidConfig = { theme: mermaidTheme, logLevel: 'fatal', securityLevel: 'strict', startOnLoad: true, arrowMarkerAbsolute: false, er: { diagramPadding: 20, layoutDirection: 'TB', minEntityWidth: 100, minEntityHeight: 75, entityPadding: 15, stroke: 'gray', fill: 'honeydew', fontSize: 12, useMaxWidth: true, }, flowchart: { diagramPadding: 8, htmlLabels: true, curve: 'basis', }, sequence: { diagramMarginX: 50, diagramMarginY: 10, actorMargin: 50, width: 150, height: 65, boxMargin: 10, boxTextMargin: 5, noteMargin: 10, messageMargin: 35, messageAlign: 'center', mirrorActors: true, bottomMarginAdj: 1, useMaxWidth: true, rightAngles: false, showSequenceNumbers: false, }, gantt: { titleTopMargin: 25, barHeight: 20, barGap: 4, topPadding: 50, leftPadding: 75, gridLineStartPadding: 35, fontSize: 11, fontFamily: '\"Open-Sans\", \"sans-serif\"', numberSectionStyles: 4, axisFormat: '%Y-%m-%d', topAxis: false, }, }; mermaid.initialize(mermaidConfig);  graph LR subgraph generate-test-contents - runs on pull_request A[checkout COAsT]--B; B[install package]--C; C[make example files dir]-- D; D[run generate_unit_test_contents.py]--E E[commit changes] end;  COAsT-site These are the actions used on the COAsT-site repo.\nConvert to markdown See Interactions with other repos for the related markdown and docstring workflows\nBuild site   let isDark = window.matchMedia('(prefers-color-scheme: dark)').matches; let mermaidTheme = (isDark) ? 'dark' : 'default'; let mermaidConfig = { theme: mermaidTheme, logLevel: 'fatal', securityLevel: 'strict', startOnLoad: true, arrowMarkerAbsolute: false, er: { diagramPadding: 20, layoutDirection: 'TB', minEntityWidth: 100, minEntityHeight: 75, entityPadding: 15, stroke: 'gray', fill: 'honeydew', fontSize: 12, useMaxWidth: true, }, flowchart: { diagramPadding: 8, htmlLabels: true, curve: 'basis', }, sequence: { diagramMarginX: 50, diagramMarginY: 10, actorMargin: 50, width: 150, height: 65, boxMargin: 10, boxTextMargin: 5, noteMargin: 10, messageMargin: 35, messageAlign: 'center', mirrorActors: true, bottomMarginAdj: 1, useMaxWidth: true, rightAngles: false, showSequenceNumbers: false, }, gantt: { titleTopMargin: 25, barHeight: 20, barGap: 4, topPadding: 50, leftPadding: 75, gridLineStartPadding: 35, fontSize: 11, fontFamily: '\"Open-Sans\", \"sans-serif\"', numberSectionStyles: 4, axisFormat: '%Y-%m-%d', topAxis: false, }, }; mermaid.initialize(mermaidConfig);  graph LR subgraph hugo - runs on push to master A[checkout site]--B; B[Setup Hugo] -- v0.70.0 --C; C[Setup Nodejs]-- v12 -- D; D[Build]--E E[Deploy] end;  ","excerpt":"GitHub actions diagram This is a collection of flowcharts for all the GitHub actions used across the …","ref":"/COAsT/docs/contributing-docs/github_actions_flowchart/","title":"Github Actions Flowchart"},{"body":"For COAsT development we use a Github workflow to manage version control and collaboration. Git allows use to keep track of changes made to the COAsT code base, avoid breaking existing code and work as a group on a single package. Any contributor needs to use this workflow to add their code. Below is some guidance on using git with COAsT, including a typical workflow and cheat sheet.\nFor more information on git, see:\nGithub (https://github.com/)\nThe Github page for this package can be found:\nhere\nKey Ideas   The COAsT repository has two core branches: master and develop. The master branch contains the tested code that you install when using Anaconda. This is updated less frequently, and is the \u0026ldquo;user-facing\u0026rdquo; branch of code. Most contributors do not need to edit this branch. The develop branch is the \u0026lsquo;pre-master\u0026rsquo; branch, where working code is kept. This is the leading branch, with the most up-to-date code, although it is not necessarily user-facing. When writing code into your own branch (see below), it is \u0026lsquo;branched\u0026rsquo; from develop and then eventually merged back into develop. You should never make changes directly to either master or develop.\n  There is a \u0026lsquo;local\u0026rsquo; and \u0026lsquo;remote\u0026rsquo; copy of the COAsT repository. The local repository exists only on your machine. The remote repository is the one you see on the Github website and exists separately. The two versions of the repository can be synchronised at a single point using commands such as git pull git push and git fetch (see below). After cloning (downloading) the repository, all modifications you make/add/commit will only be local until you push them to the remote repository.\n  Typical Workflow A typical workflow for editting COAsT in git might look like:\n  Clone Repository: git clone git@github.com:British-Oceanographic-Data-Centre/COAsT.git. This will create a new copy of COAsT on your local system which you can use to interact with git and view/edit the source code. This only needs to be done once.\n  Checkout develop: git checkout develop. Before creating a new branch for your code, you should checkout the develop branch. This will switch your local repository to the develop branch. You can check what branch your current local repository is in by entering git branch \u0026ndash; it should now say develop\n  Create/checkout your new branch: git checkout -b new_branch_name. This will create and checkout your new branch \u0026ndash; right now it is an identical copy of develop. However, any changes you commit to your local repository will be saved into your branch. Once you have created your branch, you can open it as before, using git checkout new_branch_name.\n  Make changes/additions to code: Make any changes you like to COAsT. At this point it is separate from the main branches and it is safe to do so. If in doubt, enter git branch again to ensure you are within your own branch.\n  Add changes to branch: git add modified_file. Using this command will tell git that you have changed/added this file and you want to save it to the branch you are currently in. Upon entering this command, the file changes/additions are not saved to the branch and won\u0026rsquo;t be until the next step. You can remove an added file by entering git reset modified_file and can check which files have changed by typing git status.\n  Commit changes to branch: git commit -m \u0026quot;type a message in quotations\u0026quot;. Entering this command will \u0026ldquo;save\u0026rdquo; the changes you added using git add  in the step above to the branch you are currently in. Once entered, git will identify what has changed since the previous commit. If this is the first commit in your new branch then since the version of develop that you branch from. This will not change any other branch except the one you are in and you can/should do this often with an appropriate message. At this point, all changes are still only on your local machine and will not change the remote repository. It is also possible to undo a commit using git revert, so nothing is unfixable.\n  Continue modifying code: At this point, you may want to continue modifying the code, repeatedly adding changes and commiting them to your local repository, as above.\n  Push your local repository to the remote: git push origin. This will upload the changes you have made in the branch you are in (and only this branch) to the remote (website) repository. If this is the first time you have pushed this branch then an error may appear telling you to repush with the --set-upstream flag enable. Simply copy and paste this command back into the terminal. This will \u0026ldquo;create\u0026rdquo; your branch in the remote repository. Once pushed, github will do some auto-checks to make sure the code works (which it may not, but that is fine). You can continue to modify the code at any point, and push multiple times. This is encouraged if sharing with other collaboraters.\n  Once you are satisfied with your changes, move onto the next steps.\n Make sure your local branch is up to date with the remote: git pull origin when in your branch. This is to ensure that nobody else has changed your branch, or if they have to update your local branch with the changes on the remote.\n  Update your branch with develop:. Before requesting that your branch and its changes be merged back into the develop branch, it is good practice to first merge develop back into your branch. This is because develop may have changed since you started working on your branch and these changes should be merged into your branch to ensure that conflicts are resolved. To do this, first update develop by entering git checkout develop and git pull. This will update the develop branch on your local machine. Then merge develop back into your branch by entering git checkout your_branch and git merge develop. This may say up-to-date (in which case GREAT), or successful (in which case GREAT) or may say there are some conflicts. This happens when more than one person has changed the same piece of code.\n  Resolve Conflicts: This step may not be necessary if there are no conflicts. If git tells you there are conflicts, it will also tell you which files they occur in. For more information/help with conflict resolution see here\n  Create a pull request for your branch. First your most up to date branch using git push origin, even after merging develop in step 9/10. On the website you may then create a \u0026lsquo;pull request\u0026rsquo; which is a formal way of saying you want to merge your branch back into develop. A pull request allows you to ask people to \u0026lsquo;review\u0026rsquo; your branch, share your code, view the changes in your branch and other things. To make a pull request, go to the website, click on the pull requests tab and click Create new pull request. Then select your branch in the right drop down menu and develop in the left. You may then enter a description of the changes you have made and anything else you would like reviewers to see.\n  Reviewers review the code: Requested reviewers take a look at your changes and run the unit_test. Once they are satisfied, they will approve the pull request, or add comments about any problems.\n  Merge branch into develop: Once reviewers are satisfied, you may click Merge branch at the bottom of the pull request. Now your changes will be added into develop! Again, this is fine as the branch has been inspected by reviewers and any change can be reverted using git revert (although this is not encouraged for the develop branch).\n  **Note: After creating a pull request, Github will automatically apply \u0026ldquo;black formatting\u0026rdquo; to the code. This will commit new (small) changes to the branch so you should always do a git pull on your branch to make sure your local version is up to date with the remote.\nCondensed Workflow  git clone git@github.com:British-Oceanographic-Data-Centre/COAsT.git. git checkout develop git checkout -b new_branch_name Make changes git add changed_file git commit -m \u0026quot;what changes have you made\u0026quot; git push origin If your branch changed by anyone else, git pull Repeat steps 4-8 git checkout develop git pull git checkout your_branch git merge develop git push origin Create pull request from your_branch to develop, include description and request reviewers. Reviewers accept, Merge branch.  ","excerpt":"For COAsT development we use a Github workflow to manage version control and collaboration. Git …","ref":"/COAsT/docs/contributing-docs/github_workflow/","title":"Github Workflow"},{"body":"COAsT utilises Python’s default logging library and includes a simple setup function for those unfamiliar with how to use it.\nimport coast coast.logging_util.setup_logging() This is all you need to enable full logging output to the console.\nBy default, setup_logging will use the \u0026ldquo;DEBUG\u0026rdquo; logging level, if you want to adjust this, you can use the flags from the logging library.\nimport coast import logging coast.logging_util.setup_logging(level=logging.INFO) Alternative logging levels in increasing levels of severity. Note logs are reported at the chosen severity level and higher:\n..., level=logging.DEBUG) # Detailed information, typically of interest only when diagnosing problems. ..., level=logging.INFO) # Confirmation that things are working as expected. ..., level=logging.WARNING) # An indication that something unexpected happened, or indicative of some problem in the near future (e.g. ‘disk space low’). The software is still working as expected. ..., level=logging.ERROR) # Due to a more serious problem, the software has not been able to perform some function ..., level=logging.CRITICAL) # A serious error, indicating that the program itself may be unable to continue running For more info on logging levels, see the relevant Python documentation.\nLogging output will be printed in the console once enabled by default, but output can be directed to any Stream, for instance, to an opened file.\nimport coast file = open(\u0026#34;coast.log\u0026#34;, \u0026#34;w\u0026#34;) coast.logging_util.setup_logging(stream=file) coast.logging_util.info(\u0026#34;Hello World!\u0026#34;) # Your use of COAsT would go here, this line is included as an example file.close() ","excerpt":"COAsT utilises Python’s default logging library and includes a simple setup function for those …","ref":"/COAsT/docs/contributing_package/python_logging/","title":"Logging"},{"body":"The examples in Notebooks are tutorials automatically rendered from the python notebooks in COAsT:examples_scripts/notebooks. These can be downloaded and run locally with the example data.\nWithin COAsT, configuration files are used to pass information about the example data files. The configuration files used with the example data can be downloaded or linked to a local version of the COAsT repository. These files should be placed in a config directory in your working directory, and form a useful template for loading new data files.\nThis Examples section is split into:\n","excerpt":"The examples in Notebooks are tutorials automatically rendered from the python notebooks in …","ref":"/COAsT/docs/examples/","title":"Examples"},{"body":"Here you will find information needed to contribute code changes to the COAsT package.\n","excerpt":"Here you will find information needed to contribute code changes to the COAsT package.","ref":"/COAsT/docs/contributing_package/","title":"Contributing: COAsT"},{"body":"We use Hugo Extended Version to format and generate our website, the Docsy theme for styling and site structure, and GitHub pages to manage the deployment of the site. Hugo is an open-source static site generator that provides us with templates, content organisation in a standard directory structure, and a website generation engine. You write the pages in Markdown (or HTML if you want), and Hugo wraps them up into a website.\nAll submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests.\nUpdating a single page If you\u0026rsquo;ve just spotted something you\u0026rsquo;d like to change while using the docs, Docsy has a shortcut for you:\n Click Edit this page in the top right hand corner of the page. If you don\u0026rsquo;t already have an up to date fork of the project repo, you are prompted to get one - click Fork this repository and propose changes or Update your Fork to get an up to date version of the project to edit. The appropriate page in your fork is displayed in edit mode. make your edit submit a pull request with a summary of the changes  Previewing your changes locally If you want to run your own local Hugo server to preview your changes as you work:\n  Follow the instructions in Getting started to install Hugo and any other tools you need. You\u0026rsquo;ll need at least Hugo version 0.45 (we recommend using the most recent available version), and it must be the extended version, which supports SCSS.\n  Fork the COAsT-site repo repo into your own project, then create a local copy using git clone. Don’t forget to use --recurse-submodules or you won’t pull down some of the code you need to generate a working site.\ngit clone --recurse-submodules --depth 1 https://github.com/British-Oceanographic-Data-Centre/COAsT-site.git   Run npm install to install Node.js dependencies.\n  Run hugo server in the site root directory. By default your site will be available at http://localhost:1313/COAsT. Now that you\u0026rsquo;re serving your site locally, Hugo will watch for changes to the content and automatically refresh your site.\n  Continue with the usual GitHub workflow to edit files, commit them, push the changes up to your fork, and create a pull request.\n  Creating an issue If you\u0026rsquo;ve found a problem in the docs, but you\u0026rsquo;re not sure how to fix it yourself, please create an issue in the COAsT-site repo. You can also create an issue about a specific page by clicking the Create Issue button in the top right hand corner of the page.\nUseful resources  Docsy user guide: All about Docsy, including how it manages navigation, look and feel, and multi-language support. Hugo documentation: Comprehensive reference for Hugo. Github Hello World!: A basic introduction to GitHub concepts and workflow.  ","excerpt":"We use Hugo Extended Version to format and generate our website, the Docsy theme for styling and …","ref":"/COAsT/docs/contributing-docs/","title":"Contributing: Documentation"},{"body":"What is lazy\u0026hellip; \u0026hellip;loading Lazy loading determines if data is read into memory straight away (on that line of code execution) or if the loading is delayed until the data is physical altered by some function (normally mathematical in nature)\n\u0026hellip;evaluation Lazy evaluation is about delaying the execution of a method/function call until the value is physical required, normally as a graph or printed to screen. Lazy evaluation can also help with memory management, useful with large dataset, by allowing for optimisation on the chained methods calls.\nLazy loading and Lazy evaluation are offer used together, though it is not mandatory and always worth checking that both are happening.\nBeing Lazy in COAsT There are two way to be Lazy within the COAsT package.\n xarray Dask  xarray COAsT uses xarray to load NetCDF files in, by default this will be Lazy, the raw data values will not be brought into memory.\nyou can slice and subset the data while still having the lazy loading honoured, it is not until the data is altered, say via a call to NumPy.cumsum, that the required data will be loaded into memory.\nNote the data on disk (in the NetCDF file) is never altered, only the values in memory are changed.\nimport xarray as xr import NumPy as np dataset_domain = xr.open_dataset(fn_domain) e3w_0 = dataset_domain.e3w_0 # still lazy loaded e3w_0_cs = np.cumsum(e3w_0[1:, :, :], axis=0) # now in memory Dask When in use Dask will provide lazy evaluation on top of the lazy loading.\nusing the same example as above, a file loaded in using xarray, this time with the chunks option set, will not only lazy load the data, but will turn on Dask, now using either the xarray or Dask wrapper functions will mean the NumPy cumsum call is not evaluated right way, in fact it will not be evaluated until either the compute function is called, or a greedy method from another library is used.\nimport xarray as xr dataset_domain = xr.open_dataset(fn_domain, chunks={\u0026#34;t\u0026#34;: 1}) e3w_0 = dataset_domain.e3w_0 # still lazy loaded e3w_0_cs = e3w_0[1:, :, :].cumsum(axis=0) # Dask backed Lazy evaluation We discuss Dask even more here.\n","excerpt":"What is lazy\u0026hellip; \u0026hellip;loading Lazy loading determines if data is read into memory straight …","ref":"/COAsT/docs/contributing_package/lazy-loading/","title":"working Lazily"},{"body":"What is Dask Dask is a python library that allows code to be run in parallel based on the hardware your running on. This means Dask works just as well on your laptop as on your large server.\nUsing Dask Dask is included in the xarray library. When loading a data source (file/NumPy array) Dask is automatically initiated with the chunks variable in the config file. However the chunking may not be optimal but you can adjust it before computation are made.\nnemo_t = coast.Gridded( fn_data=dn_files+fn_nemo_grid_t_dat, fn_domain=dn_files+fn_nemo_dom, config=fn_config) chunks = { \u0026#34;x_dim\u0026#34;: 10, \u0026#34;y_dim\u0026#34;: 10, \u0026#34;t_dim\u0026#34;: 10, } # Chunks are prescribed in the config json file, but can be adjusted while the data is lazy loaded. nemo_t.dataset.chunk(chunks) chunks tell Dask where to break your data across the different processor tasks.\nDirect Dask Dask can be imported and used directly\nimport Dask.array as da big_array = da.multiple(array1,array2) Dask arrays follow the NumPy API. This means that most NumPy functions have a Dask version.\nPotential Issues Dask objects are immutable. This means that the classic approach, pre-allocation follow by modification will not work.\nThe following code will error.\nimport Dask.array as da e3w_0 = da.squeeze(dataset_domain.e3w_0) depth_0 = da.zero_like(e3w_0) depth_0[0, :, :] = 0.5 * e3w_0[0, :, :] # this line will error out option 1 Continue using NumPy function but wrapping the final value in a Dask array. This final Dask object will still be in-memory.\ne3w_0 = np.squeeze(dataset_domain.e3w_0) depth_0 = np.zeros_like(e3w_0) depth_0[0, :, :] = 0.5 * e3w_0[0, :, :] depth_0[1:, :, :] = depth_0[0, :, :] + np.cumsum(e3w_0[1:, :, :], axis=0) depth_0 = da.array(depth_0) option 2 Dask offers a feature called delayed. This can be used as a modifier on your complex methods as follows;\n@Dask.delayed def set_timezero_depths(self, dataset_domain): # complex workings these do not return the computed answer, rather it returns a delayed object. These delayed object get stacked, as more delayed methods are called. When the value is needed, it can be computed like so;\nne = coast.Gridded(...) # come complex delayed methods called ne.data_variable.compute() Dask will now work out a computing path via all the required methods using as many processor tasks as possible.\nVisualising the Graph Dask is fundamentally a computational graph library, to understand what is happening in the background it can help to see these graphs (on smaller/simpler problems). This can be achieved by running;\nne = coast.Gridded(...) # come complex delayed methods called ne.data_variable.visualize() this will output a png image of the graph in the calling directory and could look like this;\n  ","excerpt":"What is Dask Dask is a python library that allows code to be run in parallel based on the hardware …","ref":"/COAsT/docs/contributing_package/dask/","title":"Dask"},{"body":"Example useage of Profile object. Overview INDEXED type class for storing data from a CTD Profile (or similar down and up observations). The structure of the class is based around having discrete profile locations with independent depth dimensions and coords. The class dataset should contain two dimensions:\n\u0026gt; id_dim :: The profiles dimension. Each element of this dimension contains data (e.g. cast) for an individual location. \u0026gt; z_dim :: The dimension for depth levels. A profile object does not need to have shared depths, so NaNs might be used to pad any depth array.  Alongside these dimensions, the following minimal coordinates should also be available:\n\u0026gt; longitude (id_dim) :: 1D array of longitudes, one for each id_dim \u0026gt; latitude (id_dim) :: 1D array of latitudes, one for each id_dim \u0026gt; time (id_dim) :: 1D array of times, one for each id_dim \u0026gt; depth (id_dim, z_dim) :: 2D array of depths, with different depth levels being provided for each profile. Note that these depth levels need to be stored in a 2D array, so NaNs can be used to pad out profiles with shallower depths. \u0026gt; id_name (id_dim) :: [Optional] Name of id_dim/case or id_dim number.  Introduction to Profile and ProfileAnalysis Below is a description of the available example scripts for this class as well as an overview of validation using Profile and ProfileAnalysis.\nExample Scripts Please see COAsT/example_scripts/notesbooks/runnable_notebooks/profile_validation/*.ipynb and COAsT/example_scripts/profile_validation/*.py for some notebooks and equivalent scripts which demonstrate how to use the Profile and ProfileAnalysis classes for model validation.\n  analysis_preprocess_en4.py : If you\u0026rsquo;re using EN4 data, this kind of script might be your first step for analysis.\n  analysis_extract_and_compare.py: This script shows you how to extract the nearest model profiles, compare them with EN4 observations and get errors throughout the vertical dimension and averaged in surface and bottom zones\n  analysis_extract_and_compare_single_process.py: This script does the same as number 2. However, it is modified slightly to take a command line argument which helps it figure out which dates to analyse. This means that this script can act as a template for jug type parallel processing on, e.g. JASMIN.\n  analysis_mask_means.py: This script demonstrates how to use boolean masks to obtain regional averages of profiles and errors.\n  analysis_average_into_grid_boxes.py: This script demonstrates how to average the data inside a Profile object into regular grid boxes and seasonal climatologies.\n  Load and preprocess profile and model data Start by loading python packages\nimport coast from os import path import numpy as np import matplotlib.pyplot as plt We can create a new Profile object easily:\nprofile = coast.Profile() Currently, this object is empty, and contains no dataset. There are some reading routines currently available in Profile for reading EN4 or WOD data files. These can be used to easily read data into your new profile object:\n# Read WOD data into profile object fn_prof = path.join(\u0026#34;example_files\u0026#34;,\u0026#34;WOD_example_ragged_standard_level.nc\u0026#34;) profile.read_wod( fn_prof ) # Read EN4 data into profile object (OVERWRITES DATASET) fn_prof = path.join(\u0026#34;example_files\u0026#34;, \u0026#34;coast_example_en4_201008.nc\u0026#34;) fn_cfg_prof = path.join(\u0026#34;config\u0026#34;,\u0026#34;example_en4_profiles.json\u0026#34;) profile = coast.Profile(config=fn_cfg_prof) profile.read_en4( fn_prof ) config/example_en4_profiles.json  Alternatively, you can pass an xarray.dataset straight to Profile:\nprofile = coast.Profile( dataset = your_dataset, config = config_file [opt] ) If you are using EN4 data, you can use the process_en4() routine to apply quality control flags to the data (replacing with NaNs):\nprocessed_profile = profile.process_en4() profile = processed_profile We can do some simple spatial and temporal manipulations of this data:\n# Cut out a geographical box profile = profile.subset_indices_lonlat_box(lonbounds = [-15, 15], latbounds = [45, 65]) # Cut out a time window profile = profile.time_slice( date0 = np.datetime64(\u0026#39;2010-01-01\u0026#39;), date1 = np.datetime64(\u0026#34;2010-01-20\u0026#34;)) Inspect profile locations Have a look inside the profile.py class to see what it can do\nprofile.plot_map() /usr/share/miniconda/envs/coast/lib/python3.8/site-packages/cartopy/io/__init__.py:241: DownloadWarning: Downloading: https://naturalearth.s3.amazonaws.com/50m_physical/ne_50m_coastline.zip warnings.warn(f'Downloading: {url}', DownloadWarning)  (\u0026lt;Figure size 640x480 with 2 Axes\u0026gt;, \u0026lt;GeoAxesSubplot: \u0026gt;)  Direct Model comparison using obs_operator() method There are a number of routines available for interpolating in the horizontal, vertical and in time to do direct comparisons of model and profile data. Profile.obs_operator will do a nearest neighbour spatial interpolation of the data in a Gridded object to profile latitudes/longitudes. It will also do a custom time interpolation.\nFirst load some model data: root = \u0026#34;./\u0026#34; # And by defining some file paths dn_files = root + \u0026#34;./example_files/\u0026#34; fn_nemo_dat = path.join(dn_files, \u0026#34;coast_example_nemo_data.nc\u0026#34;) fn_nemo_dom = path.join(dn_files, \u0026#34;coast_example_nemo_domain.nc\u0026#34;) fn_nemo_config = path.join(root, \u0026#34;./config/example_nemo_grid_t.json\u0026#34;) # Create gridded object: nemo = coast.Gridded(fn_nemo_dat, fn_nemo_dom, multiple=True, config=fn_nemo_config) Create a landmask array in Gridded In this example we add a landmask variable to the Gridded dataset. When this is present, the obs_operator will use this to interpolation to the nearest wet point. If not present, it will just take the nearest grid point (not implemented).\nWe also rename the depth at initial time coordinate depth_0 to depth as this is expected by Profile()\nnemo.dataset[\u0026#34;landmask\u0026#34;] = nemo.dataset.bottom_level == 0 nemo.dataset = nemo.dataset.rename({\u0026#34;depth_0\u0026#34;: \u0026#34;depth\u0026#34;}) # profile methods will expect a `depth` coordinate Interpolate model to horizontal observation locations using obs_operator() method # Use obs operator for horizontal remapping of Gridded onto Profile. model_profiles = profile.obs_operator(nemo) Now that we have interpolated the model onto Profiles, we have a new Profile object called model_profiles. This can be used to do some comparisons with our original processed_profile object, which we created above.\nDiscard profiles where the interpolation distance is too large However maybe we first want to restrict the set of model profiles to those that were close to the observations; perhaps, for example, the observational profiles are beyond the model domain. The model resolution would be an appropriate scale to pick\ntoo_far = 7 # distance km keep_indices = model_profiles.dataset.interp_dist \u0026lt;= too_far model_profiles = model_profiles.isel(id_dim=keep_indices) # Also drop the unwanted observational profiles profile = profile.isel(id_dim=keep_indices) Profile analysis Create an object for Profile analysis Let\u0026rsquo;s make our ProfileAnalysis object:\nanalysis = coast.ProfileAnalysis() We can use ProfileAnalysis.interpolate_vertical to interpolate all variables within a Profile object. This can be done onto a set of reference depths or, matching another object\u0026rsquo;s depth coordinates by passing another profile object. Let\u0026rsquo;s interpolate our model profiles onto observations depths, then interpolate both onto a set of reference depths:\n### Set depth averaging settings ref_depth = np.concatenate((np.arange(1, 100, 2), np.arange(100, 300, 5), np.arange(300, 1000, 50))) # Interpolate model profiles onto observation depths model_profiles_interp = analysis.interpolate_vertical(model_profiles, profile, interp_method=\u0026#34;linear\u0026#34;) # Vertical interpolation of model profiles to reference depths model_profiles_interp_ref = analysis.interpolate_vertical(model_profiles_interp, ref_depth) # Interpolation of obs profiles to reference depths profile_interp_ref = analysis.interpolate_vertical(profile, ref_depth) However, there is a problem here as the interpolate_vertical() method tries to map the whole contents of profile to the ref_depth and the profile object contains some binary data from the original qc flags. The data from the qc flags was mapped using process_en4() so the original qc entries can be removed.\n## Strip out old QC variables profile.dataset = profile.dataset.drop_vars([\u0026#39;qc_potential_temperature\u0026#39;,\u0026#39;qc_practical_salinity\u0026#39;, \u0026#39;qc_depth\u0026#39;,\u0026#39;qc_time\u0026#39;, \u0026#39;qc_flags_profiles\u0026#39;,\u0026#39;qc_flags_levels\u0026#39;]) # Interpolation of obs profiles to reference depths profile_interp_ref = analysis.interpolate_vertical(profile, ref_depth) Differencing Now that we have two Profile objects that are horizontally and vertically comparable, we can use difference() to get some basic errors:\ndifferences = analysis.difference(profile_interp_ref, model_profiles_interp_ref) This will return a new Profile object that contains the variable difference, absolute differences and square differences at all depths and means for each profile.\nType\ndifferences.dataset to see what it returns\n# E.g. plot the differences on ind_dim vs z_dim axes differences.dataset.diff_temperature.plot() \u0026lt;matplotlib.collections.QuadMesh at 0x7fd8099472e0\u0026gt;  # or a bit prettier on labelled axes cmap=plt.get_cmap(\u0026#39;seismic\u0026#39;) fig = plt.figure(figsize=(8, 3)) plt.pcolormesh( differences.dataset.time, ref_depth, differences.dataset.diff_temperature.T, label=\u0026#39;abs_diff\u0026#39;, cmap=cmap, vmin=-5, vmax=5) plt.ylim([0,200]) plt.gca().invert_yaxis() plt.ylabel(\u0026#39;depth\u0026#39;) plt.colorbar( label=\u0026#39;temperature diff (obs-model)\u0026#39;) \u0026lt;matplotlib.colorbar.Colorbar at 0x7fd809846100\u0026gt;  Layer Averaging We can use the Profile object to get mean values between specific depth levels or for some layer above the bathymetric depth. The former can be done using ProfileAnalysis.depth_means(), for example the following will return a new Profile object containing the means of all variables between 0m and 5m:\nprofile_surface = analysis.depth_means(profile, [0, 5]) # 0 - 5 metres But since this can work on any Profile object it would be more interesting to apply it to the differences between the interpolated observations and model points\nsurface_def = 10 # in metres model_profiles_surface = analysis.depth_means(model_profiles_interp_ref, [0, surface_def]) obs_profiles_surface = analysis.depth_means(profile_interp_ref, [0, surface_def]) surface_errors = analysis.difference(obs_profiles_surface, model_profiles_surface) # Plot (observation - model) upper 10m averaged temperatures surface_errors.plot_map(var_str=\u0026#34;diff_temperature\u0026#34;) (\u0026lt;Figure size 640x480 with 2 Axes\u0026gt;, \u0026lt;GeoAxesSubplot: \u0026gt;)  This can be done for any arbitrary depth layer defined by two depths.\nHowever, in some cases it may be that one of the depth levels is not defined by a constant, e.g. when calculating bottom means. In this case you may want to calculate averages over a height from the bottom that is conditional on the bottom depth. This can be done using ProfileAnalysis.bottom_means(). For example:\nbottom_height = [10, 50, 100] # Average over bottom heights of 10m, 30m and 100m for... bottom_thresh = [100, 500, np.inf] # ...bathymetry depths less than 100m, 100-500m and 500-infinite model_profiles_bottom = analysis.bottom_means(model_profiles_interp_ref, bottom_height, bottom_thresh) similarly compute the same for the observations\u0026hellip; though first we have to patch in a bathymetry variable that will be expected by the method. Grab it from the model dataset.\nprofile_interp_ref.dataset[\u0026#34;bathymetry\u0026#34;] = ([\u0026#34;id_dim\u0026#34;], model_profiles_interp_ref.dataset[\u0026#34;bathymetry\u0026#34;].values) obs_profiles_bottom = analysis.bottom_means(profile_interp_ref, bottom_height, bottom_thresh) Now the difference can be calculated\nbottom_errors = analysis.difference( obs_profiles_bottom, model_profiles_bottom) # Plot (observation - model) upper 10m averaged temperatures bottom_errors.plot_map(var_str=\u0026#34;diff_temperature\u0026#34;) (\u0026lt;Figure size 640x480 with 2 Axes\u0026gt;, \u0026lt;GeoAxesSubplot: \u0026gt;)  NOTE1: The bathymetry variable does not actually need to contain bathymetric depths, it can also be used to calculate means above any non-constant surface. For example, it could be mixed layer depth.\nNOTE2: This can be done for any Profile object. So, you could use this workflow to also average a Profile derived from the difference() routine.\n# Since they are indexed by \u0026#39;id_dim\u0026#39; they can be plotted against time fig = plt.figure(figsize=(8, 3)) plt.plot( surface_errors.dataset.time, surface_errors.dataset.diff_temperature, \u0026#39;.\u0026#39;, label=\u0026#39;surf T\u0026#39; ) plt.plot( bottom_errors.dataset.time, bottom_errors.dataset.diff_temperature, \u0026#39;.\u0026#39;, label=\u0026#39;bed T\u0026#39; ) plt.xlabel(\u0026#39;time\u0026#39;) plt.ylabel(\u0026#39;temperature errors\u0026#39;) plt.legend() plt.title(\u0026#34;Temperature diff (obs-model)\u0026#34;) Text(0.5, 1.0, 'Temperature diff (obs-model)')  Regional (Mask) Averaging We can use Profile in combination with MaskMaker to calculate averages over regions defined by masks. For example, to get the mean errors in the North Sea. Start by creating a list of boolean masks we would like to use:\nmm = coast.MaskMaker() # Define Regional Masks regional_masks = [] # Define convenient aliases based on nemo data lon = nemo.dataset.longitude.values lat = nemo.dataset.latitude.values bathy = nemo.dataset.bathymetry.values # Add regional mask for whole domain regional_masks.append(np.ones(lon.shape)) # Add regional mask for English Channel regional_masks.append(mm.region_def_nws_english_channel(lon, lat, bathy)) region_names = [\u0026#34;whole_domain\u0026#34;,\u0026#34;english_channel\u0026#34;,] Next, we must make these masks into datasets using MaskMaker.make_mask_dataset. Masks should be 2D datasets defined by booleans. In our example here we have used the latitude/longitude array from the nemo object, however it can be defined however you like.\nmask_list = mm.make_mask_dataset(lon, lat, regional_masks) Then we use ProfileAnalysis.determine_mask_indices to figure out which profiles in a Profile object lie within each regional mask:\nmask_indices = analysis.determine_mask_indices(profile, mask_list) This returns an object called mask_indices, which is required to pass to ProfileAnalysis.mask_means(). This routine will return a new xarray dataset containing averaged data for each region:\nmask_means = analysis.mask_means(profile, mask_indices) which can be visualised or further processed\nfor count_region in range(len(region_names)): plt.plot( mask_means.profile_mean_temperature.isel(dim_mask=count_region), mask_means.profile_mean_depth.isel(dim_mask=count_region), label=region_names[count_region], marker=\u0026#34;.\u0026#34;, linestyle=\u0026#39;none\u0026#39;) plt.ylim([10,1000]) plt.yscale(\u0026#34;log\u0026#34;) plt.gca().invert_yaxis() plt.xlabel(\u0026#39;temperature\u0026#39;); plt.ylabel(\u0026#39;depth\u0026#39;) plt.legend() \u0026lt;matplotlib.legend.Legend at 0x7fd80953ae20\u0026gt;  Gridding Profile Data If you have large amount of profile data you may want to average it into grid boxes to get, for example, mean error maps or climatologies. This can be done using ProfileAnalysis.average_into_grid_boxes().\nWe can create a gridded dataset shape (y_dim, x_dim) from all the data using:\ngrid_lon = np.arange(-15, 15, 0.5) grid_lat = np.arange(45, 65, 0.5) prof_gridded = analysis.average_into_grid_boxes(profile, grid_lon, grid_lat) # NB this method does not separately treat `z_dim`, see docstr lat = prof_gridded.dataset.latitude lon = prof_gridded.dataset.longitude temperature = prof_gridded.dataset.temperature plt.pcolormesh( lon, lat, temperature) plt.title(\u0026#39;gridded mean temperature\u0026#39;) plt.colorbar() \u0026lt;matplotlib.colorbar.Colorbar at 0x7fd809350dc0\u0026gt;  Alternatively, we can calculate averages for each season:\nprof_gridded_DJF = analysis.average_into_grid_boxes(profile, grid_lon, grid_lat, season=\u0026#34;DJF\u0026#34;, var_modifier=\u0026#34;_DJF\u0026#34;) prof_gridded_MAM = analysis.average_into_grid_boxes(profile, grid_lon, grid_lat, season=\u0026#34;MAM\u0026#34;, var_modifier=\u0026#34;_MAM\u0026#34;) prof_gridded_JJA = analysis.average_into_grid_boxes(profile, grid_lon, grid_lat, season=\u0026#34;JJA\u0026#34;, var_modifier=\u0026#34;_JJA\u0026#34;) prof_gridded_SON = analysis.average_into_grid_boxes(profile, grid_lon, grid_lat, season=\u0026#34;SON\u0026#34;, var_modifier=\u0026#34;_SON\u0026#34;) Here, season specifies which season to average over and var_modifier is added to the end of all variable names in the object\u0026rsquo;s dataset.\nNB with the example data only DJF has any data.\nThis function returns a new Gridded object. It also contains a new variable called grid_N, which stores how many profiles were averaged into each grid box. You may want to use this when using or extending the analysis. E.g. use it with plot symbol size\ntemperature = prof_gridded_DJF.dataset.temperature_DJF N = prof_gridded_DJF.dataset.grid_N_DJF plt.scatter( lon, lat, c=temperature, s=N) plt.title(\u0026#39;DJF gridded mean temperature\u0026#39;) plt.colorbar() \u0026lt;matplotlib.colorbar.Colorbar at 0x7fd809237c10\u0026gt;  ","excerpt":"Example useage of Profile object. Overview INDEXED type class for storing data from a CTD Profile …","ref":"/COAsT/docs/examples/notebooks/profile/0._profile_introduction/","title":"0. profile introduction"},{"body":"Script for processing raw EN4 data prior to analysis. See docstring of Profile.process_en4() for more specifics on what it does.\nThis script will just load modules, read in raw EN4 data, cut out a geographical box, call the processing routine and write the processed data to file.\nYou don\u0026rsquo;t have to do this for each EN4 file individually if you don\u0026rsquo;t want, you can read in multiple using multiple = True on the creation of the profile object. However, if analysing model data in parallel chunks, you may want to split up the processing into smaller files to make the analysis faster.\nimport sys # IF USING A DEVELOPMENT BRANCH OF COAST, ADD THE REPOSITORY TO PATH: # sys.path.append(\u0026#39;\u0026lt;PATH_TO_COAST_REPO\u0026#39;) import coast import pandas as pd from datetime import datetime from os import path print(\u0026#34;Modules loaded\u0026#34;) # File paths - input en4, output processed file and read config file \u0026#34;\u0026#34;\u0026#34; fn_prof = \u0026#34;\u0026lt;PATH_TO_RAW_EN4_DATA_FILE(S)\u0026gt;\u0026#34; fn_out = \u0026#34;\u0026lt;PATH_TO_OUTPUT_LOCATION_FOR_PROCESSED_PROFILES\u0026gt;\u0026#34; fn_cfg_prof = \u0026#34;\u0026lt;PATH_TO_COAST_PROFILE_CONFIG_FILE\u0026gt;\u0026#34; \u0026#34;\u0026#34;\u0026#34; fn_out = \u0026#34;./output.nc\u0026#34; fn_prof = path.join(\u0026#39;./example_files\u0026#39;, \u0026#34;coast_example_en4_201008.nc\u0026#34;) fn_prof = path.join(\u0026#39;/Users/jeff/Downloads/EN.4.2.2.profiles.g10.2022\u0026#39;, \u0026#34;EN.4.2.2.f.profiles.g10.2022*.nc\u0026#34;) fn_cfg_prof = path.join(\u0026#39;/Users/jeff/gitHub/COAsT/config\u0026#39;, \u0026#34;example_en4_profiles.json\u0026#34;) # Some important settings, easier to get at here longitude_bounds = [-15, 15] # Geo box to cut out from data (match to model) latitude_bounds = [40, 65] multiple = True # Reading multple files? Modules loaded  fn_prof = \u0026#34;/Users/jeff/GitHub/COAsT/example_files/coast_example_en4_201008.nc\u0026#34; # Some important settings, easier to get at here longitude_bounds = [0, 360] # Geo box to cut out from data (match to model) latitude_bounds = [-15, 15] multiple = True # Reading multple files? Create profile object containing data\nprofile = coast.Profile(config=fn_cfg_prof) profile.read_en4(fn_prof, multiple=multiple) /Users/jeff/gitHub/COAsT/config/example_en4_profiles.json --------------------------------------------------------------------------- FileNotFoundError Traceback (most recent call last) /tmp/ipykernel_3741/3838477898.py in \u0026lt;cell line: 1\u0026gt;() ----\u0026gt; 1 profile = coast.Profile(config=fn_cfg_prof) 2 profile.read_en4(fn_prof, multiple=multiple) /usr/share/miniconda/envs/coast/lib/python3.8/site-packages/coast/data/profile.py in __init__(self, dataset, config) 62 debug(f\u0026quot;Creating a new {get_slug(self)}\u0026quot;) 63 self.config = config ---\u0026gt; 64 super().__init__(self.config) 65 66 # If dataset is provided, put inside this object /usr/share/miniconda/envs/coast/lib/python3.8/site-packages/coast/data/index.py in __init__(self, config) 32 if config: 33 print(config) ---\u0026gt; 34 self.json_config = ConfigParser(config) 35 self.chunks = self.json_config.config.chunks 36 self.dim_mapping = self.json_config.config.dataset.dimension_map /usr/share/miniconda/envs/coast/lib/python3.8/site-packages/coast/data/config_parser.py in __init__(self, json_path) 16 json_path (Union[Path, str]): path to json config file. 17 \u0026quot;\u0026quot;\u0026quot; ---\u0026gt; 18 with open(json_path, \u0026quot;r\u0026quot;) as j: 19 json_content = json.loads(j.read()) 20 conf_type = ConfigTypes(json_content[ConfigKeys.TYPE]) FileNotFoundError: [Errno 2] No such file or directory: '/Users/jeff/gitHub/COAsT/config/example_en4_profiles.json'  profile.dataset --------------------------------------------------------------------------- NameError Traceback (most recent call last) /tmp/ipykernel_3741/2681786031.py in \u0026lt;cell line: 1\u0026gt;() ----\u0026gt; 1 profile.dataset NameError: name 'profile' is not defined  Get geographical indices to extract.\nprofile = profile.subset_indices_lonlat_box(longitude_bounds, latitude_bounds) --------------------------------------------------------------------------- NameError Traceback (most recent call last) /tmp/ipykernel_3741/2450699116.py in \u0026lt;cell line: 1\u0026gt;() ----\u0026gt; 1 profile = profile.subset_indices_lonlat_box(longitude_bounds, latitude_bounds) NameError: name 'profile' is not defined  profile.quick_plot() --------------------------------------------------------------------------- NameError Traceback (most recent call last) /tmp/ipykernel_3741/1711720463.py in \u0026lt;cell line: 1\u0026gt;() ----\u0026gt; 1 profile.quick_plot() NameError: name 'profile' is not defined  Cut out a time slice of the data.\nprofile = profile.time_slice(date0=datetime(2010, 1, 1), date1=datetime(2010, 1, 20)) --------------------------------------------------------------------------- NameError Traceback (most recent call last) /tmp/ipykernel_3741/567752478.py in \u0026lt;cell line: 1\u0026gt;() ----\u0026gt; 1 profile = profile.time_slice(date0=datetime(2010, 1, 1), date1=datetime(2010, 1, 20)) NameError: name 'profile' is not defined  Process the extracted data into new processed profile.\nprocessed_profile = profile.process_en4() --------------------------------------------------------------------------- NameError Traceback (most recent call last) /tmp/ipykernel_3741/2338363098.py in \u0026lt;cell line: 1\u0026gt;() ----\u0026gt; 1 processed_profile = profile.process_en4() NameError: name 'profile' is not defined  Sometimes the following line is needed to avoid an error::\nprocessed_profile.dataset[\u0026quot;time\u0026quot;] = (\u0026quot;id_dim\u0026quot;, pd.to_datetime(processed_profile.dataset.time.values))\nWrite processed profiles to file.\nprocessed_profile.dataset.to_netcdf(fn_out) --------------------------------------------------------------------------- NameError Traceback (most recent call last) /tmp/ipykernel_3741/1086284925.py in \u0026lt;cell line: 1\u0026gt;() ----\u0026gt; 1 processed_profile.dataset.to_netcdf(fn_out) NameError: name 'processed_profile' is not defined  ","excerpt":"Script for processing raw EN4 data prior to analysis. See docstring of Profile.process_en4() for …","ref":"/COAsT/docs/examples/notebooks/profile/1._analysis_preprocess_en4/","title":"1. analysis preprocess en4"},{"body":"This script demonstrates how to use the Profile and Gridded objects to extract model profiles and do some comparisons with observed profiles. It will do a nearest neighbour extraction of model data (with time interpolation of your choice). It will then calculate differences between the model and obs and averaged profiles and errors into surface and bottom layers.\nThis script will result in five new files being written:\n   extracted_profiles: Model data on model levels extracted at obs locs    interpolated_profiles: Model data on ref depth level    interpolated_obs: Obs data on ref depth levels    profile_errors: Differences between interpolated_profiles and _obs    surface_data: Surface data and errors    bottom_data: Bottom data and errors    If you are dealing with very large datasets, you should take a look at the script analysis_extract_and_compare_single_process_tutorial.ipynb. This script demonstrates a single process that can be used to build a parallel scheme.\nThis script can be used with COAsT example data. Please set:\nfn_dom = path.join('./example_files\u0026rsquo;, \u0026ldquo;coast_example_nemo_domain.nc\u0026rdquo;)\nfn_dat = path.join('./example_files\u0026rsquo;, \u0026ldquo;coast_example_nemo_data.nc\u0026rdquo;)\ndn_out = \u0026ldquo;./example_files\u0026rdquo;\nfn_prof = path.join('./example_files\u0026rsquo;, \u0026ldquo;coast_example_EN4_201008.nc\u0026rdquo;)\nfn_cfg_nemo = path.join('./config\u0026rsquo;, \u0026ldquo;example_nemo_grid_t.json\u0026rdquo;)\nfn_cfg_prof = path.join('./config\u0026rsquo;, \u0026ldquo;example_en4_profiles.json\u0026rdquo;)\nimport sys # IF USING A DEVELOPMENT BRANCH OF COAST, ADD THE REPOSITORY TO PATH: # sys.path.append(\u0026#39;\u0026lt;PATH_TO_COAST_REPO\u0026gt;\u0026#39;) import coast import xarray as xr import numpy as np import datetime from dateutil.relativedelta import relativedelta import os.path as path print(\u0026#34;Modules loaded\u0026#34;, flush=True) # Name of the run -- used mainly for naming output files run_name = \u0026#34;co7\u0026#34; Modules loaded  Figure out what the date range is for this analysis process.\nstart_date = datetime.datetime(2007, 1, 1) end_date = datetime.datetime(2010, 12, 1) print(\u0026#34;Analysis Range: {0} --\u0026gt;\u0026gt; {1}\u0026#34;.format(start_date.strftime(\u0026#34;%Y%m%d\u0026#34;), end_date.strftime(\u0026#34;%Y%m%d\u0026#34;)), flush=True) Analysis Range: 20070101 --\u0026gt;\u0026gt; 20101201  Depth averaging settings.\nref_depth = np.concatenate((np.arange(1, 100, 2), np.arange(100, 300, 5), np.arange(300, 1000, 50))) surface_def = 5 # in metres bottom_height = [10, 30, 100] # Use bottom heights of 10m, 30m and 100m for... bottom_thresh = [100, 500, np.inf] # ...depths less than 100m, 500m and infinite File paths (All) \u0026ndash; use format suggestions.\n\u0026#34;\u0026#34;\u0026#34; fn_dom = \u0026#34;\u0026lt;PATH_TO_NEMO_DOMAIN_FILE\u0026gt;\u0026#34; fn_dat = \u0026#34;\u0026lt;PATH_TO_NEMO_DATA_FILE(S)\u0026gt;\u0026#34; # .format(run_name, start_date.year) dn_out = \u0026#34;\u0026lt;PATH_TO_OUTPUT_DIRECTORY\u0026gt;\u0026#34; # .format(run_name) fn_prof = \u0026#34;\u0026lt;PATH_TO_PROCESSED_EN4_DATA\u0026gt;\u0026#34; fn_cfg_nemo = \u0026#34;\u0026lt;PATH_TO_COAST_GRIDDED_CONFIG_FILE\u0026gt;\u0026#34; fn_cfg_prof = \u0026#34;\u0026lt;PATH_TO_CODE_PROFILE_CONFIG_FILE\u0026gt;\u0026#34; \u0026#34;\u0026#34;\u0026#34; fn_dom = path.join(\u0026#39;./example_files\u0026#39;, \u0026#34;coast_example_nemo_domain.nc\u0026#34;) fn_dat = path.join(\u0026#39;./example_files\u0026#39;, \u0026#34;coast_example_nemo_data.nc\u0026#34;) dn_out = \u0026#34;./example_files\u0026#34; fn_prof = path.join(\u0026#39;./example_files\u0026#39;, \u0026#34;coast_example_en4_201008.nc\u0026#34;) fn_cfg_nemo = path.join(\u0026#39;./config\u0026#39;, \u0026#34;example_nemo_grid_t.json\u0026#34;) fn_cfg_prof = path.join(\u0026#39;./config\u0026#39;, \u0026#34;example_en4_profiles.json\u0026#34;) CREATE NEMO OBJECT and read in NEMO data. Extract latitude and longitude array.\nprint(\u0026#34;Reading model data..\u0026#34;, flush=True) nemo = coast.Gridded(fn_dat, fn_dom, multiple=True, config=fn_cfg_nemo) lon = nemo.dataset.longitude.values.squeeze() lat = nemo.dataset.latitude.values.squeeze() print(\u0026#34;NEMO object created\u0026#34;, flush=True) Reading model data.. NEMO object created  Extract time indices between start and end dates.\nnemo = nemo.time_slice(start_date, end_date) Create a landmask array \u0026ndash; important for obs_operator. We can get a landmask from bottom_level.\nnemo.dataset[\u0026#34;landmask\u0026#34;] = nemo.dataset.bottom_level == 0 nemo.dataset = nemo.dataset.rename({\u0026#34;depth_0\u0026#34;: \u0026#34;depth\u0026#34;}) print(\u0026#34;Landmask calculated\u0026#34;, flush=True) Landmask calculated  CREATE EN4 PROFILE OBJECT\nIf you have not already processed the data:\nprofile = coast.Profile(config=fn_cfg_prof) profile.read_en4(fn_prof) profile = profile.process_en4() ./config/example_en4_profiles.json  If you have already processed then uncomment:\nprofile = coast.Profile(dataset = xr.open_dataset(fn_prof, chunks={\u0026ldquo;id_dim\u0026rdquo;: 10000}))\nprint(\u0026#34;Profile object created\u0026#34;, flush=True) Profile object created  Slice out the Profile times.\nprofile = profile.time_slice(start_date, end_date) Extract only the variables that we want.\nnemo.dataset = nemo.dataset[[\u0026#34;temperature\u0026#34;, \u0026#34;bathymetry\u0026#34;, \u0026#34;bottom_level\u0026#34;, \u0026#34;landmask\u0026#34;]] profile.dataset = profile.dataset[[\u0026#34;potential_temperature\u0026#34;, \u0026#34;practical_salinity\u0026#34;, \u0026#34;depth\u0026#34;]] profile.dataset = profile.dataset.rename({\u0026#34;potential_temperature\u0026#34;: \u0026#34;temperature\u0026#34;, \u0026#34;practical_salinity\u0026#34;: \u0026#34;salinity\u0026#34;}) Create Profile analysis object.\nprofile_analysis = coast.ProfileAnalysis() Interpolate model to obs using obs_operator().\nmodel_profiles = profile.obs_operator(nemo) print(\u0026#34;Obs_operator successful.\u0026#34;, flush=True) Obs_operator successful.  Throw away profiles where the interpolation distance is larger than 5km.\nkeep_indices = model_profiles.dataset.interp_dist \u0026lt;= 5 model_profiles = model_profiles.isel(id_dim=keep_indices) profile = profile.isel(id_dim=keep_indices) Load the profiles (careful with memory).\nprofile.dataset.load() print(\u0026#34;Model interpolated to obs locations\u0026#34;, flush=True) Model interpolated to obs locations  Vertical Interpolation of model profiles to obs depths.\nmodel_profiles_interp = profile_analysis.interpolate_vertical(model_profiles, profile, interp_method=\u0026#34;linear\u0026#34;) print(\u0026#34;Model interpolated to obs depths\u0026#34;, flush=True) Model interpolated to obs depths  Vertical interpolation of model profiles to reference depths.\nmodel_profiles_interp = profile_analysis.interpolate_vertical(model_profiles_interp, ref_depth) model_profiles.dataset.to_netcdf( dn_out + \u0026#34;extracted_profiles_{0}_{1}_{2}.nc\u0026#34;.format(run_name, start_date.strftime(\u0026#34;%Y%m\u0026#34;), end_date.strftime(\u0026#34;%Y%m\u0026#34;)) ) model_profiles_interp.dataset.to_netcdf( dn_out + \u0026#34;interpolated_profiles_{0}_{1}_{2}.nc\u0026#34;.format(run_name, start_date.strftime(\u0026#34;%Y%m\u0026#34;), end_date.strftime(\u0026#34;%Y%m\u0026#34;)) ) print(\u0026#34;Model interpolated to ref depths\u0026#34;, flush=True) Model interpolated to ref depths  Interpolation of obs profiles to reference depths.\nprofile_interp = profile_analysis.interpolate_vertical(profile, ref_depth) profile_interp.dataset.to_netcdf( dn_out + \u0026#34;interpolated_obs_{0}_{1}_{2}.nc\u0026#34;.format(run_name, start_date.strftime(\u0026#34;%Y%m\u0026#34;), end_date.strftime(\u0026#34;%Y%m\u0026#34;)) ) print(\u0026#34;Obs interpolated to reference depths\u0026#34;, flush=True) Obs interpolated to reference depths  Difference between Model and Obs.\ndifferences = profile_analysis.difference(profile_interp, model_profiles_interp) differences.dataset.load() differences.dataset.to_netcdf( dn_out + \u0026#34;profile_errors_{0}_{1}_{2}.nc\u0026#34;.format(run_name, start_date.strftime(\u0026#34;%Y%m\u0026#34;), end_date.strftime(\u0026#34;%Y%m\u0026#34;)) ) print(\u0026#34;Calculated errors and written\u0026#34;, flush=True) Calculated errors and written  Surface Values and errors.\nmodel_profiles_surface = profile_analysis.depth_means(model_profiles, [0, surface_def]) obs_profiles_surface = profile_analysis.depth_means(profile, [0, surface_def]) surface_errors = profile_analysis.difference(obs_profiles_surface, model_profiles_surface) surface_data = xr.merge( (surface_errors.dataset, model_profiles_surface.dataset, obs_profiles_surface.dataset), compat=\u0026#34;override\u0026#34; ) surface_data.to_netcdf( dn_out + \u0026#34;surface_data_{0}_{1}_{2}.nc\u0026#34;.format(run_name, start_date.strftime(\u0026#34;%Y%m\u0026#34;), end_date.strftime(\u0026#34;%Y%m\u0026#34;)) ) Bottom values and errors.\nmodel_profiles_bottom = profile_analysis.bottom_means(model_profiles, bottom_height, bottom_thresh) obs_bathymetry = model_profiles.dataset[\u0026#34;bathymetry\u0026#34;].values profile.dataset[\u0026#34;bathymetry\u0026#34;] = ([\u0026#34;id_dim\u0026#34;], obs_bathymetry) obs_profiles_bottom = profile_analysis.bottom_means(profile, bottom_height, bottom_thresh) bottom_errors = profile_analysis.difference(model_profiles_bottom, obs_profiles_bottom) bottom_data = xr.merge( (bottom_errors.dataset, model_profiles_bottom.dataset, obs_profiles_bottom.dataset), compat=\u0026#34;override\u0026#34; ) bottom_data.to_netcdf( dn_out + \u0026#34;bottom_data_{0}_{1}_{2}.nc\u0026#34;.format(run_name, start_date.strftime(\u0026#34;%Y%m\u0026#34;), end_date.strftime(\u0026#34;%Y%m\u0026#34;)) ) print(\u0026#34;Bottom and surface data estimated and written\u0026#34;, flush=True) print(\u0026#34;DONE\u0026#34;, flush=True) Bottom and surface data estimated and written DONE  ","excerpt":"This script demonstrates how to use the Profile and Gridded objects to extract model profiles and do …","ref":"/COAsT/docs/examples/notebooks/profile/2._analysis_extract_and_compare/","title":"2. analysis extract and compare"},{"body":"This runs the same analysis as analysis_extract_and_compare.py however it does so in time blocks (multiples of months) to avoid memory problems, and can not be run with the example_files. At the top of this file are two variables called min_date and end_date and freq_monthgs. These are the dates that this script will run an analysis between and the monthly multiples to run per block. You must pass an index to this file at the command line, telling the script which month index to run.\nFor example\u0026hellip; If the dates are between 20040101 and 20050101, then an index of 0 will run for the period 20040101 -\u0026gt; 20040201. An index of 4 will run for the period 20040301 -\u0026gt; 20040401.\nThis script exists to be used as part of a parallel scheme on a platform like JASMIN. For example, using a command interface such as jug. This script should be run on each process, being passed just a single index.\nIf uneditted, this script will output five files PER PROCESS to the output directory specified by dn_out:\n extracted_profiles: Model data on model levels extracted at obs locs interpolated_profiles: Model data on ref depth level interpolated_obs: Obs data on ref depth levels profile_errors: Differences between interpolated_profiles and _obs surface_data: Surface data and errors bottom_data: Bottom data and errors  The files can then be concatenated and given to an averaging routine such as analysis_mask_means.py or analysis_average_into_grid.py.\nImport relevant packages import sys import coast import xarray as xr import numpy as np import datetime from dateutil.relativedelta import relativedelta Define settings index = 1 # Name of the run -- used mainly for naming output files run_name = \u0026quot;co7\u0026quot; # Start and end dates for the analysis. The script will cut down model # and EN4 data to be witin this range. min_date = datetime.datetime(2004, 1, 1) freq_months = 12 end_date = datetime.datetime(2004, 3, 1) Figure out what the date range is for this analysis process start_date = min_date + relativedelta(months=int(index * freq_months)) end_date = start_date + relativedelta(months=int(freq_months)) print(\u0026quot;Analysis Range: {0} --\u0026gt;\u0026gt; {1}\u0026quot;.format(start_date.strftime(\u0026quot;%Y%m%d\u0026quot;), end_date.strftime(\u0026quot;%Y%m%d\u0026quot;)), flush=True) Set depth averaging settings ref_depth = np.concatenate((np.arange(1, 100, 2), np.arange(100, 300, 5), np.arange(300, 1000, 50))) surface_def = 5 # in metres bottom_height = [10, 30, 100] # Use bottom heights of 10m, 30m and 100m for... bottom_thresh = [100, 500, np.inf] # ...depths less than 100m, 500m and infinite Set file paths # define some file paths fn_dom = \u0026quot;\u0026lt;PATH_TO_NEMO_DOMAIN_FILE\u0026gt;\u0026quot; fn_dat = \u0026quot;\u0026lt;PATH_TO_NEMO_DATA_FILE(S)\u0026gt;\u0026quot; # .format(run_name, start_date.year) dn_out = \u0026quot;\u0026lt;PATH_TO_OUTPUT_DIRECTORY\u0026gt;\u0026quot; # .format(run_name) fn_prof = \u0026quot;\u0026lt;PATH_TO_PROCESSED_EN4_DATA\u0026gt;\u0026quot; fn_cfg_nemo = \u0026quot;\u0026lt;PATH_TO_COAST_GRIDDED_CONFIG_FILE\u0026gt;\u0026quot; fn_cfg_prof = \u0026quot;\u0026lt;PATH_TO_CODE_PROFILE_CONFIG_FILE\u0026gt;\u0026quot; Create NEMO object and read in NEMO data. nemo = coast.Gridded(fn_dat, fn_dom, multiple=True, config=fn_cfg_nemo) Extract latitude and longitude lat = nemo.dataset.latitude.values.squeeze() lon = nemo.dataset.longitude.values.squeeze() Extract time indices between start and end dates nemo = nemo.time_slice(start_date, end_date) nemo.dataset.temperature.values Create a landmask array This is important for obs_operator We can get a landmask from bottom_level.\nnemo.dataset[\u0026quot;landmask\u0026quot;] = nemo.dataset.bottom_level == 0 nemo.dataset = nemo.dataset.rename({\u0026quot;depth_0\u0026quot;: \u0026quot;depth\u0026quot;}) print(\u0026quot;Landmask calculated\u0026quot;, flush=True) Create EN4 Profile object # CREATE EN4 PROFILE OBJECT containing processed data. We just need to # create a Profile object and place the data straight into its dataset profile = coast.Profile() profile.dataset = xr.open_dataset(fn_prof, chunks={\u0026quot;id_dim\u0026quot;: 10000}) profile = profile.time_slice(start_date, end_date) print(\u0026quot;Profile object created\u0026quot;, flush=True) Extract only the variables that we want nemo.dataset = nemo.dataset[[\u0026quot;temperature\u0026quot;, \u0026quot;salinity\u0026quot;, \u0026quot;bathymetry\u0026quot;, \u0026quot;bottom_level\u0026quot;, \u0026quot;landmask\u0026quot;]] profile.dataset = profile.dataset[[\u0026quot;potential_temperature\u0026quot;, \u0026quot;practical_salinity\u0026quot;, \u0026quot;depth\u0026quot;]] profile.dataset = profile.dataset.rename({\u0026quot;potential_temperature\u0026quot;: \u0026quot;temperature\u0026quot;, \u0026quot;practical_salinity\u0026quot;: \u0026quot;salinity\u0026quot;}) Create Profile analysis object profile_analysis = coast.ProfileAnalysis() Interpolate model to obs using obs_operator() model_profiles = profile.obs_operator(nemo) print(\u0026quot;Obs_operator successful.\u0026quot;, flush=True) Throw away profiles where the interpolation distance is larger than 5km. keep_indices = model_profiles.dataset.interp_dist \u0026lt;= 5 model_profiles = model_profiles.isel(profile=keep_indices) profile = profile.isel(profile=keep_indices) Load the profiles (careful with memory) profile.dataset.load() print(\u0026quot;Model interpolated to obs locations\u0026quot;, flush=True) Vertical Interpolation of model profiles to obs depths model_profiles_interp = profile_analysis.interpolate_vertical(model_profiles, profile, interp_method=\u0026quot;linear\u0026quot;) print(\u0026quot;Model interpolated to obs depths\u0026quot;, flush=True) Vertical interpolation of model profiles to reference depths model_profiles_interp = profile_analysis.interpolate_vertical(model_profiles_interp, ref_depth) model_profiles.dataset.to_netcdf( dn_out + \u0026quot;extracted_profiles_{0}_{1}_{2}.nc\u0026quot;.format(run_name, start_date.strftime(\u0026quot;%Y%m\u0026quot;), end_date.strftime(\u0026quot;%Y%m\u0026quot;)) ) model_profiles_interp.dataset.to_netcdf( dn_out + \u0026quot;interpolated_profiles_{0}_{1}_{2}.nc\u0026quot;.format(run_name, start_date.strftime(\u0026quot;%Y%m\u0026quot;), end_date.strftime(\u0026quot;%Y%m\u0026quot;)) ) print(\u0026quot;Model interpolated to ref depths\u0026quot;, flush=True) Interpolation of obs profiles to reference depths profile_interp = profile_analysis.interpolate_vertical(profile, ref_depth) profile_interp.dataset.to_netcdf( dn_out + \u0026quot;interpolated_obs_{0}_{1}_{2}.nc\u0026quot;.format(run_name, start_date.strftime(\u0026quot;%Y%m\u0026quot;), end_date.strftime(\u0026quot;%Y%m\u0026quot;)) ) print(\u0026quot;Obs interpolated to reference depths\u0026quot;, flush=True) Get difference between Model and Obs differences = profile_analysis.difference(profile_interp, model_profiles_interp) differences.dataset.load() differences.dataset.to_netcdf( dn_out + \u0026quot;profile_errors_{0}_{1}_{2}.nc\u0026quot;.format(run_name, start_date.strftime(\u0026quot;%Y%m\u0026quot;), end_date.strftime(\u0026quot;%Y%m\u0026quot;)) ) print(\u0026quot;Calculated errors and written\u0026quot;, flush=True) Get surface values and errors model_profiles_surface = profile_analysis.depth_means(model_profiles, [0, surface_def]) obs_profiles_surface = profile_analysis.depth_means(profile, [0, surface_def]) surface_errors = profile_analysis.difference(obs_profiles_surface, model_profiles_surface) surface_data = xr.merge( (surface_errors.dataset, model_profiles_surface.dataset, obs_profiles_surface.dataset), compat=\u0026quot;override\u0026quot; ) surface_data.to_netcdf( dn_out + \u0026quot;surface_data_{0}_{1}_{2}.nc\u0026quot;.format(run_name, start_date.strftime(\u0026quot;%Y%m\u0026quot;), end_date.strftime(\u0026quot;%Y%m\u0026quot;)) ) Get bottom values and errors model_profiles_bottom = profile_analysis.bottom_means(model_profiles, bottom_height, bottom_thresh) obs_bathymetry = model_profiles.dataset[\u0026quot;bathymetry\u0026quot;].values profile.dataset[\u0026quot;bathymetry\u0026quot;] = ([\u0026quot;id_dim\u0026quot;], obs_bathymetry) obs_profiles_bottom = profile_analysis.bottom_means(profile, bottom_height, bottom_thresh) bottom_errors = profile_analysis.difference(model_profiles_bottom, obs_profiles_bottom) bottom_data = xr.merge( (bottom_errors.dataset, model_profiles_bottom.dataset, obs_profiles_bottom.dataset), compat=\u0026quot;override\u0026quot; ) bottom_data.to_netcdf( dn_out + \u0026quot;bottom_data_{0}_{1}_{2}.nc\u0026quot;.format(run_name, start_date.strftime(\u0026quot;%Y%m\u0026quot;), end_date.strftime(\u0026quot;%Y%m\u0026quot;)) ) ","excerpt":"This runs the same analysis as analysis_extract_and_compare.py however it does so in time blocks …","ref":"/COAsT/docs/examples/notebooks/profile/3._analysis_extract_and_compare_single_process_tutorial/","title":"3. analysis extract and compare single process tutorial"},{"body":"Tutorial to calculate mask means (regional means) of variables within a Profile object.\nProvide paths to four files:\nfn_dom : NEMO domain file defining mask lon/lat. fn_cfg_nemo : NEMO config file. fn_profile : Path to netCDF containing profile data. fn_out : Path to netCDF output file.  You can use this script with example files by setting:\nfn_dom = path.join('./example_files', \u0026quot;coast_example_nemo_domain.nc\u0026quot;) fn_prof = path.join('./example_files', \u0026quot;coast_example_en4_201008.nc\u0026quot;) fn_cfg_nemo = path.join('./config', \u0026quot;example_nemo_grid_t.json\u0026quot;) fn_cfg_prof = path.join('./config', \u0026quot;example_en4_profiles.json\u0026quot;)  Import relevant packages import coast import numpy as np from os import path Set filepaths to data and configuration \u0026#34;\u0026#34;\u0026#34; fn_dom = \u0026#34;\u0026lt;PATH_TO_NEMO_DOMAIN_FILE\u0026gt;\u0026#34; fn_cfg_nemo = \u0026#34;\u0026lt;PATH_TO_COAST_GRIDDED_CONFIG_FILE\u0026gt;\u0026#34; fn_cfg_prof = \u0026#34;\u0026lt;PATH_TO_COAST_PROFILE_CONFIG_FILE\u0026gt;\u0026#34; fn_prof = \u0026#34;\u0026lt;PATH_TO_COAST_PROFILE_NETCDF\u0026gt;\u0026#34; fn_out = \u0026#34;\u0026lt;PATH_TO_OUTPUT_FILE\u0026gt;\u0026#34; \u0026#34;\u0026#34;\u0026#34; fn_out = \u0026#34;./output.nc\u0026#34; fn_dom = path.join(\u0026#39;./example_files\u0026#39;, \u0026#34;coast_example_nemo_domain.nc\u0026#34;) fn_prof = path.join(\u0026#39;./example_files\u0026#39;, \u0026#34;coast_example_en4_201008.nc\u0026#34;) fn_cfg_nemo = path.join(\u0026#39;./config\u0026#39;, \u0026#34;example_nemo_grid_t.json\u0026#34;) fn_cfg_prof = path.join(\u0026#39;./config\u0026#39;, \u0026#34;example_en4_profiles.json\u0026#34;) Create NEMO object and read in NEMO data nemo = coast.Gridded(fn_domain=fn_dom, multiple=True, config=fn_cfg_nemo) Extract latitude and longitude array lon = nemo.dataset.longitude.values.squeeze() lat = nemo.dataset.latitude.values.squeeze() Create analysis object and mask maker object profile_analysis = coast.ProfileAnalysis() Make Profile object and read data profile = coast.Profile(config=fn_cfg_prof) profile.read_en4(fn_prof) ./config/example_en4_profiles.json  Make MaskMaker object and define Regional Masks # Make MaskMaker object mm = coast.MaskMaker() # Define Regional Masks regional_masks = [] bath = nemo.dataset.bathymetry.values regional_masks.append(np.ones(lon.shape)) regional_masks.append(mm.region_def_nws_north_sea(lon, lat, bath)) regional_masks.append(mm.region_def_nws_outer_shelf(lon, lat, bath)) regional_masks.append(mm.region_def_nws_english_channel(lon, lat, bath)) regional_masks.append(mm.region_def_nws_norwegian_trench(lon, lat, bath)) regional_masks.append(mm.region_def_kattegat(lon, lat, bath)) regional_masks.append(mm.region_def_south_north_sea(lon, lat, bath)) off_shelf = mm.region_def_off_shelf(lon, lat, bath) off_shelf[regional_masks[3].astype(bool)] = 0 off_shelf[regional_masks[4].astype(bool)] = 0 regional_masks.append(off_shelf) regional_masks.append(mm.region_def_irish_sea(lon, lat, bath)) region_names = [ \u0026#34;whole_domain\u0026#34;, \u0026#34;north_sea\u0026#34;, \u0026#34;outer_shelf\u0026#34;, \u0026#34;eng_channel\u0026#34;, \u0026#34;nor_trench\u0026#34;, \u0026#34;kattegat\u0026#34;, \u0026#34;southern_north_sea\u0026#34;, \u0026#34;irish_sea\u0026#34;, \u0026#34;off_shelf\u0026#34;, ] mask_list = mm.make_mask_dataset(lon, lat, regional_masks) mask_indices = profile_analysis.determine_mask_indices(profile, mask_list) Do mask averaging mask_means = profile_analysis.mask_means(profile, mask_indices) Save mask dataset to file mask_means.to_netcdf(fn_out) ","excerpt":"Tutorial to calculate mask means (regional means) of variables within a Profile object.\nProvide …","ref":"/COAsT/docs/examples/notebooks/profile/4._analysis_mask_means_tutorial/","title":"4. analysis mask means tutorial"},{"body":"Script for showing use of Profile.average_into_grid_boxes(). This routines takes all data in a Profile obejct and averages it into lat/lon grid boxes.\nThis script can be used for comparing observed and modelled climatologies. It should be run AFTER the nearest profiles have been extracted from the model data, such as shown in analysis_extract_and_compare.py.\nInput and output files should be provided as a list. If you only have one input file, then just enclose the string in [].\nRelevant imports and filepath configuration import coast import numpy as np import xarray as xr import os from os import path # List of input files fn_prof = path.join(\u0026#39;./example_files\u0026#39;, \u0026#34;coast_example_en4_201008.nc\u0026#34;) fn_cfg_prof = path.join(\u0026#39;./config\u0026#39;, \u0026#34;example_en4_profiles.json\u0026#34;) # If needed fn_out = path.join(\u0026#39;./example_files\u0026#39;, \u0026#39;mask_mean.nc\u0026#39;) # Names of output files (coresponding to fn_in_list), include \u0026#34;.nc\u0026#34; Define longitude and latitude grid.\ngrid_lon = np.arange(-15, 15, 0.5) grid_lat = np.arange(45, 65, 0.5) Load the data Load in data for averaging (e.g. surface data).\nprof_data = coast.Profile(config=fn_cfg_prof) prof_data.read_en4(fn_prof) profile_analysis = coast.ProfileAnalysis() ./config/example_en4_profiles.json  Take just the data we want so it is faster\nprof_data.dataset = prof_data.dataset[[\u0026#34;temperature\u0026#34;, \u0026#34;practical_salinity\u0026#34;]] Process, merge and save Average all data across all seasons.\nprof_gridded = profile_analysis.average_into_grid_boxes(prof_data, grid_lon, grid_lat) Average data for each season.\nprof_gridded_DJF = profile_analysis.average_into_grid_boxes( prof_data, grid_lon, grid_lat, season=\u0026#34;DJF\u0026#34;, var_modifier=\u0026#34;_DJF\u0026#34; ) prof_gridded_MAM = profile_analysis.average_into_grid_boxes( prof_data, grid_lon, grid_lat, season=\u0026#34;MAM\u0026#34;, var_modifier=\u0026#34;_MAM\u0026#34; ) prof_gridded_JJA = profile_analysis.average_into_grid_boxes( prof_data, grid_lon, grid_lat, season=\u0026#34;JJA\u0026#34;, var_modifier=\u0026#34;_JJA\u0026#34; ) prof_gridded_SON = profile_analysis.average_into_grid_boxes( prof_data, grid_lon, grid_lat, season=\u0026#34;SON\u0026#34;, var_modifier=\u0026#34;_SON\u0026#34; ) Merge together.\nds_prof_gridded = xr.merge( ( prof_gridded.dataset, prof_gridded_DJF.dataset, prof_gridded_MAM.dataset, prof_gridded_JJA.dataset, prof_gridded_SON.dataset, ) ) Save to file.\nds_prof_gridded.to_netcdf(fn_out) ","excerpt":"Script for showing use of Profile.average_into_grid_boxes(). This routines takes all data in a …","ref":"/COAsT/docs/examples/notebooks/profile/5._analysis_average_into_grid_boxes/","title":"5. analysis average into grid boxes"},{"body":"This is a demonstration script for using the Altimetry object in the COAsT package. This object has strict data formatting requirements, which are outlined in altimetry.py.\nRelevant imports and filepath configuration # Begin by importing coast and other packages import coast root = \u0026#34;./\u0026#34; # And by defining some file paths dn_files = root + \u0026#34;./example_files/\u0026#34; fn_nemo_dat = dn_files + \u0026#34;coast_example_nemo_data.nc\u0026#34; fn_nemo_dom = dn_files + \u0026#34;coast_example_nemo_domain.nc\u0026#34; fn_nemo_config = root + \u0026#34;./config/example_nemo_grid_t.json\u0026#34; fn_altimetry = dn_files + \u0026#34;coast_example_altimetry_data.nc\u0026#34; fn_altimetry_config = root + \u0026#34;./config/example_altimetry.json\u0026#34; Load data # We need to load in a NEMO object for doing NEMO things. nemo = coast.Gridded(fn_nemo_dat, fn_nemo_dom, config=fn_nemo_config) # And now we can load in our Altimetry data. By default, Altimetry is set up # to read in CMEMS netCDF files. However, if no path is supplied, then the # object\u0026#39;s dataset will be initialised as None. Custom data can then be loaded # if desired, as long as it follows the data formatting for Altimetry. # altimetry = coast.Altimetry(fn_altimetry) altimetry = coast.Altimetry(fn_altimetry, config=fn_altimetry_config) ././config/example_altimetry.json Altimetry object at 0x5650c0a26fc0 initialised  Subsetting # Before going any further, lets just cut out the bit of the altimetry that # is over the model domain. This can be done using `subset_indices_lonlat_box` # to find relevant indices and then `isel` to extract them. The data here is then also # also thinned slightly. ind = altimetry.subset_indices_lonlat_box([-10, 10], [45, 60]) ind = ind[::4] altimetry = altimetry.isel(t_dim=ind) Subsetting Altimetry object at 0x5650c0a26fc0 indices in [-10, 10], [45, 60]  Model interpolation # Before comparing our observations to the model, we will interpolate a model # variable to the same time and geographical space as the altimetry. This is # done using the obs_operator() method: altimetry.obs_operator(nemo, mod_var_name=\u0026#34;ssh\u0026#34;, time_interp=\u0026#34;nearest\u0026#34;) # Doing this has created a new interpolated variable called interp_ssh and # saved it back into our Altimetry object. Take a look at altimetry.dataset # to see for yourself. Interpolating Gridded object at 0x5650c0a26fc0 \u0026quot;ssh\u0026quot; with time_interp \u0026quot;nearest\u0026quot;  #altimetry.dataset # uncomment to print data object summary Interpolated vs observed # Next we will compare this interpolated variable to an observed variable # using some basic metrics. The basic_stats() routine can be used for this, # which calculates some simple metrics including differences, RMSE and # correlations. NOTE: This may not be a wise choice of variables. stats = altimetry.basic_stats(\u0026#34;ocean_tide_standard_name\u0026#34;, \u0026#34;interp_ssh\u0026#34;) Altimetry object at 0x5650c0a26fc0 initialised /usr/share/miniconda/envs/coast/lib/python3.8/site-packages/coast/data/altimetry.py:352: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning. corr = pdvar.corr(method=method) /usr/share/miniconda/envs/coast/lib/python3.8/site-packages/coast/data/altimetry.py:366: FutureWarning: The default value of numeric_only in DataFrame.cov is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning. cov = pdvar.cov()  # Take a look inside stats.dataset to see all of the new variables. When using # basic stats, the returned object is also an Altimetry object, so all of the # same methods can be applied. Alternatively, if you want to save the new # metrics to the original altimetry object, set \u0026#39;create_new_object = False\u0026#39;. #stats.dataset # uncomment to print data object summary # Now we will do a more complex comparison using the Continuous Ranked # Probability Score (CRPS). For this, we need to hand over the model object, # a model variable and an observed variable. We also give it a neighbourhood # radius in km (nh_radius). crps = altimetry.crps(nemo, model_var_name=\u0026#34;ssh\u0026#34;, obs_var_name=\u0026#34;ocean_tide_standard_name\u0026#34;, nh_radius=20) # Again, take a look inside `crps.dataset` to see some new variables. Similarly # to basic_stats, `create_new_object` keyword arg can be set to `false` to save output to # the original altimetry object. #crps.dataset # uncomment to print data object summary Altimetry object at 0x5650c0a26fc0 initialised  Plotting data # Altimetry has a ready built quick_plot() routine for taking a look at any # of the observed or derived quantities above. So to take a look at the # \u0026#39;ocean_tide_standard_name\u0026#39; variable: fig, ax = altimetry.quick_plot(\u0026#34;ocean_tide_standard_name\u0026#34;) /usr/share/miniconda/envs/coast/lib/python3.8/site-packages/cartopy/io/__init__.py:241: DownloadWarning: Downloading: https://naturalearth.s3.amazonaws.com/50m_physical/ne_50m_coastline.zip warnings.warn(f'Downloading: {url}', DownloadWarning)  # As stats and crps are also `altimetry` objects, quick_plot() can also be used: fig, ax = crps.quick_plot(\u0026#34;crps\u0026#34;) # stats quick_plot: fig, ax = stats.quick_plot(\u0026#34;absolute_error\u0026#34;) ","excerpt":"This is a demonstration script for using the Altimetry object in the COAsT package. This object has …","ref":"/COAsT/docs/examples/notebooks/altimetry/altimetry_tutorial/","title":"Altimetry tutorial"},{"body":"","excerpt":"","ref":"/COAsT/docs/examples/notebooks/altimetry/","title":"Altimety"},{"body":"This demonstration has two parts:\n  Climatology.make_climatology(): This demonstration uses the COAsT package to calculate a climatological mean of an input dataset at a desired output frequency. Output can be written straight to file.\n  Climatology.make_multiyear_climatology(): This demonstrations uses the COAsT package to calculate a climatological mean of an input dataset at a desired output frequency, over multiple years, but will work with single year datasets too.\n  COAsT and xarray should preserve any lazy loading and chunking. If defined properly in the read function, memory issues can be avoided and parallel processes will automatically be used.\nimport coast Usage of coast.Climatology.make_climatology(). Calculates mean over a given period of time. This doesn\u0026rsquo;t take different years into account, unless using the \u0026lsquo;years\u0026rsquo; frequency.\nroot = \u0026#34;./\u0026#34; # Paths to a single or multiple data files. dn_files = root + \u0026#34;./example_files/\u0026#34; fn_nemo_dat = dn_files + \u0026#34;coast_example_nemo_data.nc\u0026#34; fn_nemo_config = root + \u0026#34;./config/example_nemo_grid_t.json\u0026#34; # Set path for domain file if required. fn_nemo_dom = dn_files + \u0026#34;coast_example_nemo_domain.nc\u0026#34; # Define output filepath (optional: None or str) fn_out = None # Read in multiyear data (This example uses NEMO data from a single file.) nemo_data = coast.Gridded(fn_data=fn_nemo_dat, fn_domain=fn_nemo_dom, config=fn_nemo_config, ).dataset Calculate the climatology for temperature and sea surface height (ssh) as an example:\n# Select specific data variables. data = nemo_data[[\u0026#34;temperature\u0026#34;, \u0026#34;ssh\u0026#34;]] # Define frequency -- Any xarray time string: season, month, etc climatology_frequency = \u0026#34;month\u0026#34; # Calculate the climatology and write to file. clim = coast.Climatology() clim_mean = clim.make_climatology(data, climatology_frequency, fn_out=fn_out) Below shows the structure of a dataset returned, containing 1 month worth of meaned temperature and sea surface height data:\n#clim_mean # uncomment to print data object summary Usage of coast.Climatology.multiyear_averages(). Calculates the mean over a specified period and groups the data by year-period. Here a fully working example is not available as multi-year example data is not in the example_files. However a working example using synthetic data is given in: tests/test_climatology.py. This method is designed to be compatible with multi-year datasets, but will work with single year datasets too.\n# Paths to a single or multiple data files. fn_nemo_data = \u0026quot;/path/to/nemo/*.nc\u0026quot; # Set path for domain file if required. fn_nemo_domain = None # Set path to configuration file fn_nemo_config = \u0026quot;/path/to/nemo/*.json\u0026quot; # Read in multiyear data (This example uses NEMO data from multiple datafiles.) nemo_data = coast.Gridded(fn_data=fn_nemo_data, fn_domain=fn_nemo_domain, config=fn_nemo_config, multiple=True).dataset Now calculate temperature and ssh means of each season across multiple years for specified data, using seasons module to specify time period.\nfrom coast._utils import seasons # Select specific data variables. data = nemo_data[[\u0026quot;temperature\u0026quot;, \u0026quot;ssh\u0026quot;]] clim = coast.Climatology() # SPRING, SUMMER, AUTUMN, WINTER, ALL are valid values for seasons. clim_multiyear = clim.multiyear_averages(data, seasons.ALL, time_var='time', time_dim='t_dim') # Or explicitly defining specific month periods. # A list of tuples defining start and end month integers. The start months should be in chronological order. # (you may need to read/load the data again if it gives an error) month_periods = [(1,2), (12,2)] # Specifies January -\u0026gt; February and December -\u0026gt; February for each year of data. clim_multiyear = clim.multiyear_averages(data, month_periods , time_var='time', time_dim='t_dim') ","excerpt":"This demonstration has two parts:\n  Climatology.make_climatology(): This demonstration uses the …","ref":"/COAsT/docs/examples/notebooks/general/climatology_tutorial/","title":"Climatology tutorial"},{"body":"Contour subsetting (a vertical slice of data along a contour).\nThis is a demonstration script for using the Contour class in the COAsT package. This object has strict data formatting requirements, which are outlined in contour.py.\nThe code is taken directly from unit_tesing/unit_test.py\nIn this tutorial we take a look the following Isobath Contour Methods:\na. Extract isbath contour between two points b. Plot contour on map c. Calculate pressure along contour d. Calculate flow across contour e. Calculate pressure gradient driven flow across contour  Load packages and define some file paths. import coast import matplotlib.pyplot as plt # Define some file paths root = \u0026#34;./\u0026#34; dn_files = root + \u0026#34;./example_files/\u0026#34; fn_nemo_dat_t = dn_files + \u0026#34;nemo_data_T_grid.nc\u0026#34; fn_nemo_dat_u = dn_files + \u0026#34;nemo_data_U_grid.nc\u0026#34; fn_nemo_dat_v = dn_files + \u0026#34;nemo_data_V_grid.nc\u0026#34; fn_nemo_dom = dn_files + \u0026#34;coast_example_nemo_domain.nc\u0026#34; # Configuration files describing the data files fn_config_t_grid = root + \u0026#34;./config/example_nemo_grid_t.json\u0026#34; fn_config_f_grid = root + \u0026#34;./config/example_nemo_grid_f.json\u0026#34; fn_config_u_grid = root + \u0026#34;./config/example_nemo_grid_u.json\u0026#34; fn_config_v_grid = root + \u0026#34;./config/example_nemo_grid_v.json\u0026#34; Extract isobath contour between two points and create contour object. Create a gridded object with the grid only.\nnemo_f = coast.Gridded(fn_domain=fn_nemo_dom, config=fn_config_f_grid) Then create a contour object on the 200m isobath.\ncontours, no_contours = coast.Contour.get_contours(nemo_f, 200) Extract the indices for the contour in a specified box.\ny_ind, x_ind, contour = coast.Contour.get_contour_segment(nemo_f, contours[0], [50, -10], [60, 3]) Extract the contour for the specified indices.\ncont_f = coast.ContourF(nemo_f, y_ind, x_ind, 200) Plot contour on map plt.figure() coast.Contour.plot_contour(nemo_f, contour) plt.show() \u0026lt;Figure size 640x480 with 0 Axes\u0026gt;  Calculate pressure along contour. Repeat the above procedure but on t-points.\nnemo_t = coast.Gridded(fn_data=fn_nemo_dat_t, fn_domain=fn_nemo_dom, config=fn_config_t_grid) contours, no_contours = coast.Contour.get_contours(nemo_t, 200) y_ind, x_ind, contour = coast.Contour.get_contour_segment(nemo_t, contours[0], [50, -10], [60, 3]) cont_t = coast.ContourT(nemo_t, y_ind, x_ind, 200) Now contruct pressure along this contour segment.\ncont_t.construct_pressure(1027) # This creates ``cont_t.data_contour.pressure_s`` and ``cont_t.data_contour.pressure_h_zlevels`` fields. Calculate flow across contour. Create the contour segement on f-points again.\nnemo_f = coast.Gridded(fn_domain=fn_nemo_dom, config=fn_config_f_grid) nemo_u = coast.Gridded(fn_data=fn_nemo_dat_u, fn_domain=fn_nemo_dom, config=fn_config_u_grid) nemo_v = coast.Gridded(fn_data=fn_nemo_dat_v, fn_domain=fn_nemo_dom, config=fn_config_v_grid) contours, no_contours = coast.Contour.get_contours(nemo_f, 200) y_ind, x_ind, contour = coast.Contour.get_contour_segment(nemo_f, contours[0], [50, -10], [60, 3]) cont_f = coast.ContourF(nemo_f, y_ind, x_ind, 200) Calculate the flow across the contour, pass u- and v- gridded velocity objects.\ncont_f.calc_cross_contour_flow(nemo_u, nemo_v) # This creates fields ``cont_f.data_cross_flow.normal_velocities`` and ## ``cont_f.data_cross_flow.depth_integrated_normal_transport`` Calculate pressure gradient driven flow across contour. The \u0026ldquo;calc_geostrophic_flow()\u0026rdquo; operates on f-grid objects and requires configuration files for the u- and v- grids.\ncont_f.calc_geostrophic_flow(nemo_t, config_u=fn_config_u_grid, config_v=fn_config_v_grid, ref_density=1027) \u0026#34;\u0026#34;\u0026#34; This constructs: cont_f.data_cross_flow.normal_velocity_hpg cont_f.data_cross_flow.normal_velocity_spg cont_f.data_cross_flow.transport_across_AB_hpg cont_f.data_cross_flow.transport_across_AB_spg \u0026#34;\u0026#34;\u0026#34; '\\n This constructs:\\n cont_f.data_cross_flow.normal_velocity_hpg\\n cont_f.data_cross_flow.normal_velocity_spg\\n cont_f.data_cross_flow.transport_across_AB_hpg\\n cont_f.data_cross_flow.transport_across_AB_spg\\n'  ","excerpt":"Contour subsetting (a vertical slice of data along a contour).\nThis is a demonstration script for …","ref":"/COAsT/docs/examples/notebooks/gridded/contour_tutorial/","title":"Contour tutorial"},{"body":"The notebook proves a template and some instruction on how to create a dask wrapper\nMotivation Start with an xarray.DataArray object called myDataArray, that we want to pass into a function. That function will perform eager evaluation and return a numpy array, but we want lazy evaluation with the possibility to allow dask parallelism. See worked example in Process_data.seasonal_decomposition.\nImport dependencies import dask.array as da from dask import delayed import xarray as xr import numpy as np Step 1. (optional: allows dask to distribute computation across multiple cores, if not interested see comment 2) Partition data in myDataArray by chunking it up as desired. Note that chunking dimensions need to make sense for your particular problem! Here we just chunk along dim_2\nmyDataArray = myDataArray.chunk({\u0026quot;dim_1\u0026quot;: myDataArray.dim_1.size, \u0026quot;dim_2\u0026quot;: chunksize}) # can be more dimensions Then create a list containing all the array chunks as dask.delayed objects (e.g. 4 chunks =\u0026gt; list contain 4 delayed objects)\nmyDataArray_partitioned = myDataArray.data.to_delayed().ravel() Comment 1 There are different ways to partition your data. For example, if you start off with a numpy array rather than an xarray DataArray you can just iterate over the array and partition it that way (the partitions do NOT need to be dask.delayed objects). For example see the very simple case here: https://docs.dask.org/en/stable/delayed.html\nThe method described in 1 is just very convenient for DataArrays where the multi-dimensional chunks may be the desired way to partition the data.\nStep 2. Call your eager evaluating function using dask.delayed and pass in your data. This returns a list containing the outputs from the function as dask.delayed objects. The list will have the same length as myDataArray_partitioned\ndelayed_myFunction_output = [ delayed(myFunction)(aChunk, other_args_for_myFunction) for aChunk in myDataArray_partitioned ] Step 3. Convert the lists of delayed objects to lists of dask arrays to allow array operations. It\u0026rsquo;s possible this step is not necessary!\ndask_array_list = [] for chunk_idx, aChunk in enumerate(delayed_myFunction_output): # When converting from dask.delayed to dask.array, you must know the shape of the # array. In this example we know this from the chunk sizes of the original DataArray chunk_shape = (myDataArray.chunks[0][0], myDataArray.chunks[1][chunk_idx]) dask_array_list.append(da.from_delayed(aChunk, shape=chunk_shape, dtype=float)) Step 4. Concatenate the array chunks together to get a single dask.array. This can be assigned to a new DataArray as desired.\nmyOutputArray = da.concatenate(dask_array_list, axis=1) Comment 2 If you skipped step 1., i.e. just want a lazy operation and no parallelism, you can just do this\nmyOutputArray = da.from_delayed( delayed(myFunction)(myDataArray, other_args_for_myFunction), shape=myDataArray.shape, dtype=float ) ","excerpt":"The notebook proves a template and some instruction on how to create a dask wrapper\nMotivation Start …","ref":"/COAsT/docs/examples/notebooks/general/dask_wrapper_template_tutorial/","title":"Dask wrapper template tutorial"},{"body":"Using COAsT to compute the Empirical Orthogonal Functions (EOFs) of your data\nRelevant imports and filepath configuration # Begin by importing coast and other packages import coast import xarray as xr import matplotlib.pyplot as plt # Define some file paths root = \u0026#34;./\u0026#34; dn_files = root + \u0026#34;./example_files/\u0026#34; fn_nemo_dat_t = dn_files + \u0026#34;nemo_data_T_grid.nc\u0026#34; fn_nemo_dom = dn_files + \u0026#34;coast_example_nemo_domain.nc\u0026#34; fn_nemo_config = root + \u0026#34;./config/example_nemo_grid_t.json\u0026#34; Loading data # Load data variables that are on the NEMO t-grid nemo_t = coast.Gridded( fn_data = fn_nemo_dat_t, fn_domain = fn_nemo_dom, config = fn_nemo_config ) Compute EOF For a variable (or subset of a variable) with two spatial dimensions and one temporal dimension, i.e. (x,y,t), the EOFs, temporal projections and variance explained can be computed by calling the ‘eofs’ method, and passing in the ssh DataArray as an argument. For example, for the sea surface height field, we can do\neof_data = coast.compute_eofs( nemo_t.dataset.ssh ) The method returns an xarray dataset that contains the EOFs, temporal projections and variance as DataArrays\n#eof_data # uncomment to print data object summary Inspect EOFs The variance explained of the first four modes is\n# eof_data.variance.sel(mode=[1,2,3,4]) ## uncomment Plotting And the EOFs and temporal projections can be quick plotted:\neof_data.EOF.sel(mode=[1,2,3,4]).plot.pcolormesh(col=\u0026#39;mode\u0026#39;,col_wrap=2,x=\u0026#39;longitude\u0026#39;,y=\u0026#39;latitude\u0026#39;) \u0026lt;xarray.plot.facetgrid.FacetGrid at 0x7f7aac54aee0\u0026gt;  eof_data.temporal_proj.sel(mode=[1,2,3,4]).plot(col=\u0026#39;mode\u0026#39;,col_wrap=2,x=\u0026#39;time\u0026#39;) \u0026lt;xarray.plot.facetgrid.FacetGrid at 0x7f7a63b2ad30\u0026gt;  Complex EOFs The more exotic hilbert complex EOFs can also be computed to investigate the propagation of variability, for example:\nheof_data = coast.compute_hilbert_eofs( nemo_t.dataset.ssh ) #heof_data # uncomment to print data object summary now with the modes expressed by their amplitude and phase, the spatial propagation of the variability can be examined through the EOF_phase.\n","excerpt":"Using COAsT to compute the Empirical Orthogonal Functions (EOFs) of your data\nRelevant imports and …","ref":"/COAsT/docs/examples/notebooks/general/eof_tutorial/","title":"Eof tutorial"},{"body":"This is a demonstration script for how to export intermediate data from COAsT to netCDF files for later analysis or storage. The tutorial showcases the xarray.to_netcdf() method. http://xarray.pydata.org/en/stable/generated/xarray.Dataset.to_netcdf.html\nBegin by importing COAsT and other packages import coast import xarray as xr Now define some file paths root = \u0026#34;./\u0026#34; # And by defining some file paths dn_files = root + \u0026#34;./example_files/\u0026#34; fn_nemo_dat = dn_files + \u0026#34;coast_example_nemo_data.nc\u0026#34; fn_nemo_dom = dn_files + \u0026#34;coast_example_nemo_domain.nc\u0026#34; config = root + \u0026#34;./config/example_nemo_grid_t.json\u0026#34; ofile = \u0026#34;example_export_output.nc\u0026#34; # The target filename for output We need to load in a NEMO object for doing NEMO things nemo = coast.Gridded(fn_nemo_dat, fn_nemo_dom, config=config) We can export the whole xr.DataSet to a netCDF file Other file formats are available. From the documentation:\n NETCDF4: Data is stored in an HDF5 file, using netCDF4 API features. NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only netCDF 3 compatible API features. NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format, which fully supports 2+ GB files, but is only compatible with clients linked against netCDF version 3.6.0 or later. NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not handle 2+ GB files very well.  Mode - \u0026lsquo;w\u0026rsquo; (write) is the default. Other options from the documentation:\n mode ({\u0026ldquo;w\u0026rdquo;, \u0026ldquo;a\u0026rdquo;}, default: \u0026ldquo;w\u0026rdquo;) – Write (‘w’) or append (‘a’) mode. If mode=’w’, any existing file at this location will be overwritten. If mode=’a’, existing variables will be overwritten.  Similarly xr.DataSets collections of variables or xr.DataArray variables can be exported to netCDF for objects in the TRANSECT, TIDEGAUGE, etc classes.\nnemo.dataset.to_netcdf(ofile, mode=\u0026#34;w\u0026#34;, format=\u0026#34;NETCDF4\u0026#34;) Alternatively a single variable (an xr.DataArray object) can be exported nemo.dataset[\u0026#34;temperature\u0026#34;].to_netcdf(ofile, format=\u0026#34;NETCDF4\u0026#34;) Check the exported file is as you expect Perhaps by using ncdump -h example_export_output.nc, or load the file and see that the xarray structure is preserved.\nobject = xr.open_dataset(ofile) object.close() # close file associated with this object ","excerpt":"This is a demonstration script for how to export intermediate data from COAsT to netCDF files for …","ref":"/COAsT/docs/examples/notebooks/general/export_to_netcdf_tutorial/","title":"Export to netcdf tutorial"},{"body":"This page will walk you though a simple setup for hugo extended - which is needed if want to view any changes you make to this site locally.\nFor more details please read this.\nInstallation Manual  Download hugo extended from GitHub Unzip into preferred location (I use C:\\hugo) Add to OS PATH  optional but makes usage easier    Via a Package Manager On Windows you can use Chocolately to install with:\nchoco install hugo-extended Or on macOS/Linux you can use Homebrew to install with:\nbrew install hugo Try it out! You should now be able to try the following in a terminal\n$ hugo --help if you have cloned the COAsT-site repo you should also now be able to;\n$ cd COAsT-site $ hugo server the above will start a local hugo powered version of the website. you can edit any of the files under /content and see your changes at http://localhost:1313/COAsT/\n","excerpt":"This page will walk you though a simple setup for hugo extended - which is needed if want to view …","ref":"/COAsT/docs/contributing-docs/hugo/","title":"setting up Hugo"},{"body":"An introduction to the Gridded class. Loading variables and grid information.\nThis is designed to be a brief introduction to the Gridded class including: 1. Creation of a Gridded object 2. Loading data into the Gridded object. 3. Combining Gridded output and Gridded domain data. 4. Interrogating the Gridded object. 5. Basic manipulation ans subsetting 6. Looking at the data with matplotlib\nLoading and Interrogating Begin by importing COAsT and define some file paths for NEMO output data and a NEMO domain, as an example of model data suitable for the Gridded object.\nimport coast import matplotlib.pyplot as plt import datetime import numpy as np # Define some file paths root = \u0026#34;./\u0026#34; dn_files = root + \u0026#34;./example_files/\u0026#34; fn_nemo_dat = dn_files + \u0026#34;coast_example_nemo_data.nc\u0026#34; fn_nemo_dom = dn_files + \u0026#34;coast_example_nemo_domain.nc\u0026#34; fn_config_t_grid = root + \u0026#34;./config/example_nemo_grid_t.json\u0026#34; We can create a new Gridded object by simple calling coast.Gridded(). By passing this a NEMO data file and a NEMO domain file, COAsT will combine the two into a single xarray dataset within the Gridded object. Each individual Gridded object should be for a specified NEMO grid type, which is specified in a configuration file which is also passed as an argument. The Dask library is switched on by default, chunking can be specified in the configuration file.\nnemo_t = coast.Gridded(fn_data = fn_nemo_dat, fn_domain = fn_nemo_dom, config=fn_config_t_grid) Our new Gridded object nemo_t contains a variable called dataset, which holds information on the two files we passed. Let’s have a look at this:\n#nemo_t.dataset # uncomment to print data object summary This is an xarray dataset, which has all the information on netCDF style structures. You can see dimensions, coordinates and data variables. At the moment, none of the actual data is loaded to memory and will remain that way until it needs to be accessed.\nAlong with temperature (which has been renamed from votemper) a number of other things have happen under the hood:\n The dimensions have been renamed to t_dim, x_dim, y_dim, z_dim The coordinates have been renamed to time, longitude, latitude and depth_0. These are the coordinates for this grid (the t-grid). Also depth_0 has been calculated as the 3D depth array at time zero. The variables e1, e2 and e3_0 have been created. These are the metrics for the t-grid in the x-dim, y-dim and z_dim (at time zero) directions.  So we see that the Gridded class has standardised some variable names and created an object based on this discretisation grid by combining the appropriate grid information with all the variables on that grid.\nWe can interact with this as an xarray Dataset object. So to extract a specific variable (say temperature):\nssh = nemo_t.dataset.ssh #ssh # uncomment to print data object summary Or as a numpy array:\nssh_np = ssh.values #ssh_np.shape # uncomment to print data object summary Then lets plot up a single time snapshot of ssh using matplotlib:\nplt.pcolormesh(nemo_t.dataset.longitude, nemo_t.dataset.latitude, nemo_t.dataset.ssh[0]) \u0026lt;matplotlib.collections.QuadMesh at 0x7f53a43580d0\u0026gt;  Some Manipulation There are currently some basic subsetting routines for Gridded objects, to cut out specified regions of data. Fundamentally, this can be done using xarray’s isel or sel routines to index the data. In this case, the Gridded object will pass arguments straight through to xarray.isel.\nLets get the indices of all model points within 111km km of (5W, 55N):\nind_y, ind_x = nemo_t.subset_indices_by_distance(centre_lon=-5, centre_lat=55, radius=111) #ind_x.shape # uncomment to print data object summary Now create a new, smaller subsetted Gridded object by passing those indices to isel.\nnemo_t_subset = nemo_t.isel(x_dim=ind_x, y_dim=ind_y) #nemo_t_subset.dataset # uncomment to print data object summary Alternatively, xarray.isel can be applied directly to the xarray.Dataset object. A longitude/latitude box of data can also be extracted using Gridded.subset_indices().\nPlotting example for NEMO-ERSEM biogechemical variables Import COAsT, define some file paths for NEMO-ERSEM output data and a NEMO domain, and read/load your NEMO-ERSEM data into a gridded object, example:\nimport coast import matplotlib.pyplot as plt # Define some file paths root = \u0026#34;./\u0026#34; dn_files = root + \u0026#34;./example_files/\u0026#34; fn_bgc_dat = dn_files + \u0026#34;coast_example_SEAsia_BGC_1990.nc\u0026#34; fn_bgc_dom = dn_files + \u0026#34;coast_example_domain_SEAsia.nc\u0026#34; fn_config_bgc_grid = root + \u0026#34;./config/example_nemo_bgc.json\u0026#34; nemo_bgc = coast.Gridded(fn_data = fn_bgc_dat, fn_domain = fn_bgc_dom, config=fn_config_bgc_grid) #nemo_bgc.dataset # uncomment to print data object summary As an example plot a snapshot of dissolved inorganic carbon at the sea surface\nfig = plt.figure() plt.pcolormesh( nemo_bgc.dataset.longitude, nemo_bgc.dataset.latitude, nemo_bgc.dataset.dic.isel(t_dim=0).isel(z_dim=0), cmap=\u0026#34;RdYlBu_r\u0026#34;, vmin=1600, vmax=2080, ) plt.colorbar() plt.title(\u0026#34;DIC, mmol/m^3\u0026#34;) plt.xlabel(\u0026#34;longitude\u0026#34;) plt.ylabel(\u0026#34;latitude\u0026#34;) plt.show() /tmp/ipykernel_3934/2498690501.py:2: UserWarning: The input coordinates to pcolormesh are interpreted as cell centers, but are not monotonically increasing or decreasing. This may lead to incorrectly calculated cell edges, in which case, please supply explicit cell edges to pcolormesh. plt.pcolormesh(  ","excerpt":"An introduction to the Gridded class. Loading variables and grid information.\nThis is designed to be …","ref":"/COAsT/docs/examples/notebooks/gridded/introduction_to_gridded_class/","title":"Introduction to gridded class"},{"body":"A demonstration to calculate the Potential Energy Anomaly.\nRelevant imports and filepath configuration import coast import numpy as np import os import matplotlib.pyplot as plt import matplotlib.colors as colors # colormap fiddling # set some paths root = \u0026#34;./\u0026#34; dn_files = root + \u0026#34;./example_files/\u0026#34; fn_nemo_grid_t_dat = dn_files + \u0026#34;nemo_data_T_grid_Aug2015.nc\u0026#34; fn_nemo_dom = dn_files + \u0026#34;coast_example_nemo_domain.nc\u0026#34; config_t = root + \u0026#34;./config/example_nemo_grid_t.json\u0026#34; Loading data # Create a Gridded object and load in the data: nemo_t = coast.Gridded(fn_nemo_grid_t_dat, fn_nemo_dom, config=config_t) Calculates Potential Energy Anomaly The density and depth averaged density can be supplied within gridded_t as density and density_bar DataArrays, respectively. If they are not supplied they will be calculated. density_bar is calcuated using depth averages of temperature and salinity.\n# Compute a vertical max to exclude depths below 200m Zd_mask, kmax, Ikmax = nemo_t.calculate_vertical_mask(200.) # Initiate a stratification diagnostics object strat = coast.GriddedStratification(nemo_t) # calculate PEA for unmasked depths strat.calc_pea(nemo_t, Zd_mask) make a plot strat.quick_plot(\u0026#39;PEA\u0026#39;) (\u0026lt;Figure size 1000x1000 with 2 Axes\u0026gt;, \u0026lt;AxesSubplot: title={'center': '01 Aug 2015: Potential Energy Anomaly (J / m^3)'}, xlabel='longitude', ylabel='latitude'\u0026gt;)  ","excerpt":"A demonstration to calculate the Potential Energy Anomaly.\nRelevant imports and filepath …","ref":"/COAsT/docs/examples/notebooks/gridded/potential_energy_tutorial/","title":"Potential energy tutorial"},{"body":"","excerpt":"","ref":"/COAsT/docs/examples/notebooks/profile/","title":"Profile"},{"body":"A demonstration of pycnocline depth and thickness diagnostics. The first and second depth moments of stratification are computed as proxies for pycnocline depth and thickness, suitable for a nearly two-layer fluid.\nNote that in the AMM7 example data the plots are not particularly spectacular as the internal tide is poorly resolved at 7km.\nRelevant imports and filepath configuration import coast import numpy as np import os import matplotlib.pyplot as plt import matplotlib.colors as colors # colormap fiddling # set some paths root = \u0026#34;./\u0026#34; dn_files = root + \u0026#34;./example_files/\u0026#34; fn_nemo_grid_t_dat = dn_files + \u0026#34;nemo_data_T_grid_Aug2015.nc\u0026#34; fn_nemo_dom = dn_files + \u0026#34;coast_example_nemo_domain.nc\u0026#34; config_t = root + \u0026#34;./config/example_nemo_grid_t.json\u0026#34; config_w = root + \u0026#34;./config/example_nemo_grid_w.json\u0026#34; Loading data # Create a Gridded object and load in the data: nemo_t = coast.Gridded(fn_nemo_grid_t_dat, fn_nemo_dom, config=config_t) #nemo_t.dataset # uncomment to print data object summary # The stratification variables are computed as centred differences of the t-grid variables.  # These will become w-grid variables. So, create an empty w-grid object, to store stratification.  # Note how we do not pass a NEMO data file for this load. nemo_w = coast.Gridded(fn_domain=fn_nemo_dom, config=config_w) Subset the domain We are not interested in the whole doman so it is computationally efficient to subset the data for the region of interest. Here we will look at the North Sea between (51N: 62N) and (-4E:15E). We will great subset objects for both the t- and w-grids:\nind_2d = nemo_t.subset_indices(start=[51,-4], end=[62,15]) nemo_nwes_t = nemo_t.isel(y_dim=ind_2d[0], x_dim=ind_2d[1]) #nwes = northwest european shelf ind_2d = nemo_w.subset_indices(start=[51,-4], end=[62,15]) nemo_nwes_w = nemo_w.isel(y_dim=ind_2d[0], x_dim=ind_2d[1]) #nwes = northwest european shelf #nemo_nwes_t.dataset # uncomment to print data object summary Diagnostic calculations and plotting We can use a COAsT method to construct the in-situ density:\nnemo_nwes_t.construct_density( eos=\u0026#39;EOS10\u0026#39; ) Then we construct stratification using a COAsT method to take the vertical derivative. Noting that the inputs are on t-pts and the outputs are on w-pt\nnemo_nwes_w = nemo_nwes_t.differentiate( \u0026#39;density\u0026#39;, dim=\u0026#39;z_dim\u0026#39;, out_var_str=\u0026#39;rho_dz\u0026#39;, out_obj=nemo_nwes_w ) # --\u0026gt; sci_nwes_w.rho_dz This has created a variable called nemo_nwes_w.rho_dz.\nCreate internal tide diagnostics We can now use the GriddedStratification class to construct the first and second moments (over depth) of density. In the limit of an idealised two-layer fluid these converge to the depth and thickness of the interface. I.e. the pycnocline depth and thickness respectively.\nstrat = coast.GriddedStratification(nemo_nwes_t) #%% Construct pycnocline variables: depth and thickness strat.construct_pycnocline_vars( nemo_nwes_t, nemo_nwes_w ) /usr/share/miniconda/envs/coast/lib/python3.8/site-packages/xarray/core/computation.py:727: RuntimeWarning: invalid value encountered in sqrt result_data = func(*input_data)  Plotting data Finally we plot pycnocline variables (depth and thickness) using an GriddedStratification method:\nstrat.quick_plot() (\u0026lt;Figure size 1000x1000 with 2 Axes\u0026gt;, \u0026lt;AxesSubplot: title={'center': '01 Aug 2015: masked pycnocline thickness (m)'}, xlabel='longitude', ylabel='latitude'\u0026gt;)  ","excerpt":"A demonstration of pycnocline depth and thickness diagnostics. The first and second depth moments of …","ref":"/COAsT/docs/examples/notebooks/gridded/pycnocline_tutorial/","title":"Pycnocline tutorial"},{"body":"","excerpt":"","ref":"/COAsT/docs/reference/","title":"Reference"},{"body":"This is a demonstration on regridding in COAsT. To do this, the COAsT package uses the already capable xesmf package, which will need to be installed independently (is not natively part of the COAsT package).\nIntroduction COAsT uses XESMF by providing a data class xesmf_convert which provides functions to prepare COAsT.Gridded objects, so they can be passed to XESMF for regridding to either a curvilinear or rectilienar grid.\nAll you need to do if provide a Gridded object and a grid type when creating a new instance of this class. It will then contain an appropriate input dataset. You may also provide a second COAsT gridded object if regridding between two objects.\nInstall XESMF See the package\u0026rsquo;s documentation website here:\nhttps://xesmf.readthedocs.io/en/latest/index.html  You can install XESMF using:\n conda install -c conda-forge xesmf.  The setup used by this class has been tested for xesmf v0.6.2 alongside esmpy v8.0.0. It was installed using:\n conda install -c conda-forge xesmf esmpy=8.0.0  Example useage If regridding a Gridded object to an arbitrarily defined rectilinear or curvilinear grid, you just need to do the following:\nimport xesmf as xe # Create your gridded object gridded = coast.Gridded(*args, **kwargs) # Pass the gridded object over to xesmf_convert xesmf_ready = coast.xesmf_convert(gridded, input_grid_type = 'curvilinear') # Now this object will contain a dataset called xesmf_input, which can # be passed over to xesmf. E.G: destination_grid = xesmf.util.grid_2d(-15, 15, 1, 45, 65, 1) regridder = xe.Regridder(xesmf_ready.input_grid, destination_grid, \u0026quot;bilinear\u0026quot;) regridded_dataset = regridder(xesmf_ready.input_data) XESMF contains a couple of difference functions for quickly creating output grids, such as xesmf.util.grid_2d and xesmf.util.grid_global(). See their website for more info.\nThe process is almost the same if regridding from one COAsT.Gridded object to another (gridded0 -\u0026gt; gridded1):\nxesmf_ready = coast.xesmf_convert(gridded0, gridded1, input_grid_type = \u0026quot;curvilinear\u0026quot;, output_grid_type = \u0026quot;curvilinear\u0026quot;) regridder = xe.Regridder(xesmf_ready.input_grid, xesmf_ready.output_grid, \u0026quot;bilinear\u0026quot;) regridded_dataset = regridder(xesmf_ready.input_data) Note that you can select which variables you want to regrid, either prior to using this tool or by indexing the input_data dataset. e.g.:\nregridded_dataset = regridder(xesmf_ready.input_data['temperature']) If your input datasets were lazy loaded, then so will the regridded dataset. At this point you can either load the data or (recomended) save the regridded data to file:\nregridded_dataset.to_netcdf(\u0026lt;filename_to_save\u0026gt;) Before saving back to file, call xesmf_ready.to_gridded() to convert the regridded xesmf object back to a gridded object\nCompatability Note (written 8 Sept 2022) xesmf is not included natively within COAsT as satisfying all the dependencies within COAsT gets increasingly challenging with more components in COAsT. So whilst valuable, xesmf is currently deemed not core. Here are some notes from a user on its installation with conda:\nA conda environemt with `esmpy=8.0.0` specified and `xesmf` version unspecified works suggests a downgrade of: netCDF4 1.5.6 scipy 1.5.3 lxml 4.8 A solution to avoid the downgrade maybe found in https://github.com/pangeo-data/pangeo-docker-images/issues/101 conda create … \u0026quot;mpi==openmpi\u0026quot; \u0026quot;esmpy==mpi_openmpi*\u0026quot; xesmf ","excerpt":"This is a demonstration on regridding in COAsT. To do this, the COAsT package uses the already …","ref":"/COAsT/docs/examples/notebooks/gridded/regridding_with_xesmf_tutorial/","title":"Regridding with xesmf tutorial"},{"body":"Tutorial to make a simple SEAsia 1/12 deg DIC plot.\nImport the relevant packages import coast import matplotlib.pyplot as plt Define file paths for data root = \u0026#34;./\u0026#34; dn_files = root + \u0026#34;./example_files/\u0026#34; path_config = root + \u0026#34;./config/\u0026#34; fn_seasia_domain = dn_files + \u0026#34;coast_example_domain_SEAsia.nc\u0026#34; fn_seasia_var = dn_files + \u0026#34;coast_example_SEAsia_BGC_1990.nc\u0026#34; fn_seasia_config_bgc = path_config + \u0026#34;example_nemo_bgc.json\u0026#34; Create a Gridded object seasia_bgc = coast.Gridded(fn_data=fn_seasia_var, fn_domain=fn_seasia_domain, config=fn_seasia_config_bgc) Plot DIC fig = plt.figure() plt.pcolormesh( seasia_bgc.dataset.longitude, seasia_bgc.dataset.latitude, seasia_bgc.dataset.dic.isel(t_dim=0).isel(z_dim=0), cmap=\u0026#34;RdYlBu_r\u0026#34;, vmin=1600, vmax=2080, ) plt.colorbar() plt.title(\u0026#34;DIC, mmol/m^3\u0026#34;) plt.xlabel(\u0026#34;longitude\u0026#34;) plt.ylabel(\u0026#34;latitude\u0026#34;) plt.show() /tmp/ipykernel_4049/1161426776.py:2: UserWarning: The input coordinates to pcolormesh are interpreted as cell centers, but are not monotonically increasing or decreasing. This may lead to incorrectly calculated cell edges, in which case, please supply explicit cell edges to pcolormesh. plt.pcolormesh(  ","excerpt":"Tutorial to make a simple SEAsia 1/12 deg DIC plot.\nImport the relevant packages import coast import …","ref":"/COAsT/docs/examples/notebooks/gridded/seasia_dic_example_plot_tutorial/","title":"Seasia dic example plot tutorial"},{"body":"Overview A function within the Process_data class that will decompose time series into trend, seasonal and residual components. The function is a wrapper that adds functionality to the seasonal_decompose function contained in the statsmodels package to make it more convenient for large geospatial datasets.\nSpecifically:\n Multiple time series spread across multiple dimensions, e.g. a gridded dataset, can be processed. The user simply passes in an xarray DataArray that has a \u0026ldquo;t_dim\u0026rdquo; dimension and 1 or more additional dimensions, for example gridded spatial dimensions Masked locations, such as land points, are handled A dask wrapper is applied to the function that a) supports lazy evaluation b) allows the dataset to be easily seperated into chunks so that processing can be carried out in parallel (rather than processing every time series sequentially) The decomposed time series are returned as xarray DataArrays within a single coast.Gridded object  An example Below is an example using the coast.Process_data.seasonal_decomposition function with the example data. Note that we will artifically extend the length of the example data time series for demonstrative purposes.\nBegin by importing coast, defining paths to the data, and loading the example data into a gridded object:\nimport coast import numpy as np import xarray as xr # Path to a data file root = \u0026#34;./\u0026#34; dn_files = root + \u0026#34;./example_files/\u0026#34; fn_nemo_dat = dn_files + \u0026#34;coast_example_nemo_data.nc\u0026#34; # Set path for domain file if required. fn_nemo_dom = dn_files + \u0026#34;coast_example_nemo_domain.nc\u0026#34; # Set path for model configuration file config = root + \u0026#34;./config/example_nemo_grid_t.json\u0026#34; # Read in data (This example uses NEMO data.) grd = coast.Gridded(fn_nemo_dat, fn_nemo_dom, config=config) The loaded example data only has 7 time stamps, the code below creates a new (fake) extended temperature variable with 48 monthly records. This code is not required to use the function, it is only included here to make a set of time series that are long enough to be interesting.\n# create a 4 yr monthly time coordinate array time_array = np.arange( np.datetime64(\u0026#34;2010-01-01\u0026#34;), np.datetime64(\u0026#34;2014-01-01\u0026#34;), np.timedelta64(1, \u0026#34;M\u0026#34;), dtype=\u0026#34;datetime64[M]\u0026#34; ).astype(\u0026#34;datetime64[s]\u0026#34;) # create 4 years of monthly temperature data based on the loaded data temperature_array = ( (np.arange(0, 48) * 0.05)[:, np.newaxis, np.newaxis, np.newaxis] + np.random.normal(0, 0.1, 48)[:, np.newaxis, np.newaxis, np.newaxis] + np.tile(grd.dataset.temperature[:-1, :2, :, :], (8, 1, 1, 1)) ) # create a new temperature DataArray temperature = xr.DataArray( temperature_array, coords={ \u0026#34;t_dim\u0026#34;: time_array, \u0026#34;depth_0\u0026#34;: grd.dataset.depth_0[:2, :, :], \u0026#34;longitude\u0026#34;: grd.dataset.longitude, \u0026#34;latitude\u0026#34;: grd.dataset.latitude, }, dims=[\u0026#34;t_dim\u0026#34;, \u0026#34;z_dim\u0026#34;, \u0026#34;y_dim\u0026#34;, \u0026#34;x_dim\u0026#34;], ) Check out the new data\n#temperature # uncomment to print data object summary temperature[0,0,:,:].plot() \u0026lt;matplotlib.collections.QuadMesh at 0x7f506c448940\u0026gt;  Check out time series at 2 different grid points\ntemperature[:,0,50,50].plot() temperature[:,0,200,200].plot() [\u0026lt;matplotlib.lines.Line2D at 0x7f506c28c8e0\u0026gt;]  Create a coast.Process_data object, and call the seasonal_decomposition function, passing in the required arguments. The first two arguments are:\n The input data, here the temperature data as an xarray DataArray The number of chuncks to split the data into. Here we split the data into 2 chunks so that the dask scheduler will try to run 4 processes in parallel  The remaining arguments are keyword arguments for the underlying statsmodels.tsa.seasonal.seasonal_decompose function, which are documented on the statsmodels documentation pages. Here we specify:\nthree The type of model, i.e. an additive model The period of the seasonal cycle, here it is 6 months Extrapolate the trend component to cover the entire range of the time series (this is required because the trend is calculated using a convolution filter)  proc_data = coast.Process_data() grd = proc_data.seasonal_decomposition(temperature, 2, model=\u0026#34;additive\u0026#34;, period=6, extrapolate_trend=\u0026#34;freq\u0026#34;) The returned xarray Dataset contains the decomposed time series (trend, seasonal, residual) as dask arrays\n#grd.dataset # uncomment to print data object summary Execute the computation\ngrd.dataset.compute()             /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { \u0026ndash;xr-font-color0: var(\u0026ndash;jp-content-font-color0, rgba(0, 0, 0, 1)); \u0026ndash;xr-font-color2: var(\u0026ndash;jp-content-font-color2, rgba(0, 0, 0, 0.54)); \u0026ndash;xr-font-color3: var(\u0026ndash;jp-content-font-color3, rgba(0, 0, 0, 0.38)); \u0026ndash;xr-border-color: var(\u0026ndash;jp-border-color2, #e0e0e0); \u0026ndash;xr-disabled-color: var(\u0026ndash;jp-layout-color3, #bdbdbd); \u0026ndash;xr-background-color: var(\u0026ndash;jp-layout-color0, white); \u0026ndash;xr-background-color-row-even: var(\u0026ndash;jp-layout-color1, white); \u0026ndash;xr-background-color-row-odd: var(\u0026ndash;jp-layout-color2, #eeeeee); }\nhtml[theme=dark], body.vscode-dark { \u0026ndash;xr-font-color0: rgba(255, 255, 255, 1); \u0026ndash;xr-font-color2: rgba(255, 255, 255, 0.54); \u0026ndash;xr-font-color3: rgba(255, 255, 255, 0.38); \u0026ndash;xr-border-color: #1F1F1F; \u0026ndash;xr-disabled-color: #515151; \u0026ndash;xr-background-color: #111111; \u0026ndash;xr-background-color-row-even: #111111; \u0026ndash;xr-background-color-row-odd: #313131; }\n.xr-wrap { display: block !important; min-width: 300px; max-width: 700px; }\n.xr-text-repr-fallback { /* fallback to plain text repr when CSS is not injected (untrusted notebook) */ display: none; }\n.xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(\u0026ndash;xr-border-color); }\n.xr-header \u0026gt; div, .xr-header \u0026gt; ul { display: inline; margin-top: 0; margin-bottom: 0; }\n.xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; }\n.xr-obj-type { color: var(\u0026ndash;xr-font-color2); }\n.xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; }\n.xr-section-item { display: contents; }\n.xr-section-item input { display: none; }\n.xr-section-item input + label { color: var(\u0026ndash;xr-disabled-color); }\n.xr-section-item input:enabled + label { cursor: pointer; color: var(\u0026ndash;xr-font-color2); }\n.xr-section-item input:enabled + label:hover { color: var(\u0026ndash;xr-font-color0); }\n.xr-section-summary { grid-column: 1; color: var(\u0026ndash;xr-font-color2); font-weight: 500; }\n.xr-section-summary \u0026gt; span { display: inline-block; padding-left: 0.5em; }\n.xr-section-summary-in:disabled + label { color: var(\u0026ndash;xr-font-color2); }\n.xr-section-summary-in + label:before { display: inline-block; content: \u0026lsquo;►\u0026rsquo;; font-size: 11px; width: 15px; text-align: center; }\n.xr-section-summary-in:disabled + label:before { color: var(\u0026ndash;xr-disabled-color); }\n.xr-section-summary-in:checked + label:before { content: \u0026lsquo;▼\u0026rsquo;; }\n.xr-section-summary-in:checked + label \u0026gt; span { display: none; }\n.xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; }\n.xr-section-inline-details { grid-column: 2 / -1; }\n.xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; }\n.xr-section-summary-in:checked ~ .xr-section-details { display: contents; }\n.xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; }\n.xr-array-wrap \u0026gt; label { grid-column: 1; vertical-align: top; }\n.xr-preview { color: var(\u0026ndash;xr-font-color3); }\n.xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; }\n.xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; }\n.xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; }\n.xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; }\n.xr-dim-list li { display: inline-block; padding: 0; margin: 0; }\n.xr-dim-list:before { content: \u0026lsquo;('; }\n.xr-dim-list:after { content: \u0026lsquo;)'; }\n.xr-dim-list li:not(:last-child):after { content: \u0026lsquo;,'; padding-right: 5px; }\n.xr-has-index { font-weight: bold; }\n.xr-var-list, .xr-var-item { display: contents; }\n.xr-var-item \u0026gt; div, .xr-var-item label, .xr-var-item \u0026gt; .xr-var-name span { background-color: var(\u0026ndash;xr-background-color-row-even); margin-bottom: 0; }\n.xr-var-item \u0026gt; .xr-var-name:hover span { padding-right: 5px; }\n.xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; div, .xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; label, .xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; .xr-var-name span { background-color: var(\u0026ndash;xr-background-color-row-odd); }\n.xr-var-name { grid-column: 1; }\n.xr-var-dims { grid-column: 2; }\n.xr-var-dtype { grid-column: 3; text-align: right; color: var(\u0026ndash;xr-font-color2); }\n.xr-var-preview { grid-column: 4; }\n.xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; }\n.xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; }\n.xr-var-attrs, .xr-var-data { display: none; background-color: var(\u0026ndash;xr-background-color) !important; padding-bottom: 5px !important; }\n.xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; }\n.xr-var-data \u0026gt; table { float: right; }\n.xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; }\n.xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; }\ndl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; }\n.xr-attrs dt, .xr-attrs dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; }\n.xr-attrs dt { font-weight: normal; grid-column: 1; }\n.xr-attrs dt:hover span { display: inline-block; background: var(\u0026ndash;xr-background-color); padding-right: 10px; }\n.xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; }\n.xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } \u0026lt;xarray.Dataset\u0026gt; Dimensions: (t_dim: 48, z_dim: 2, y_dim: 375, x_dim: 297) Coordinates:\n  t_dim (t_dim) datetime64[ns] 2010-01-01 2010-02-01 \u0026hellip; 2013-12-01 depth_0 (z_dim, y_dim, x_dim) float32 0.5 0.5 0.5 0.5 \u0026hellip; 1.5 1.5 1.5 1.5 longitude (y_dim, x_dim) float32 -19.89 -19.78 -19.67 \u0026hellip; 12.78 12.89 13.0 latitude (y_dim, x_dim) float32 40.07 40.07 40.07 40.07 \u0026hellip; 65.0 65.0 65.0 Dimensions without coordinates: z_dim, y_dim, x_dim Data variables: trend (t_dim, z_dim, y_dim, x_dim) float64 nan nan nan \u0026hellip; nan nan nan seasonal (t_dim, z_dim, y_dim, x_dim) float64 nan nan nan \u0026hellip; nan nan nan residual (t_dim, z_dim, y_dim, x_dim) float64 nan nan nan \u0026hellip; nan nan nanxarray.DatasetDimensions:t_dim: 48z_dim: 2y_dim: 375x_dim: 297Coordinates: (4)t_dim(t_dim)datetime64[ns]2010-01-01 \u0026hellip; 2013-12-01array(['2010-01-01T00:00:00.000000000', '2010-02-01T00:00:00.000000000', '2010-03-01T00:00:00.000000000', '2010-04-01T00:00:00.000000000', '2010-05-01T00:00:00.000000000', '2010-06-01T00:00:00.000000000', '2010-07-01T00:00:00.000000000', '2010-08-01T00:00:00.000000000', '2010-09-01T00:00:00.000000000', '2010-10-01T00:00:00.000000000', '2010-11-01T00:00:00.000000000', '2010-12-01T00:00:00.000000000', '2011-01-01T00:00:00.000000000', '2011-02-01T00:00:00.000000000', '2011-03-01T00:00:00.000000000', '2011-04-01T00:00:00.000000000', '2011-05-01T00:00:00.000000000', '2011-06-01T00:00:00.000000000', '2011-07-01T00:00:00.000000000', '2011-08-01T00:00:00.000000000', '2011-09-01T00:00:00.000000000', '2011-10-01T00:00:00.000000000', '2011-11-01T00:00:00.000000000', '2011-12-01T00:00:00.000000000', '2012-01-01T00:00:00.000000000', '2012-02-01T00:00:00.000000000', '2012-03-01T00:00:00.000000000', '2012-04-01T00:00:00.000000000', '2012-05-01T00:00:00.000000000', '2012-06-01T00:00:00.000000000', '2012-07-01T00:00:00.000000000', '2012-08-01T00:00:00.000000000', '2012-09-01T00:00:00.000000000', '2012-10-01T00:00:00.000000000', '2012-11-01T00:00:00.000000000', '2012-12-01T00:00:00.000000000', '2013-01-01T00:00:00.000000000', '2013-02-01T00:00:00.000000000', '2013-03-01T00:00:00.000000000', '2013-04-01T00:00:00.000000000', '2013-05-01T00:00:00.000000000', '2013-06-01T00:00:00.000000000', '2013-07-01T00:00:00.000000000', '2013-08-01T00:00:00.000000000', '2013-09-01T00:00:00.000000000', '2013-10-01T00:00:00.000000000', '2013-11-01T00:00:00.000000000', '2013-12-01T00:00:00.000000000'], dtype='datetime64[ns]')depth_0(z_dim, y_dim, x_dim)float320.5 0.5 0.5 0.5 \u0026hellip; 1.5 1.5 1.5 1.5units :mstandard_name :Depth at time zero on the t-gridarray([[[0.5 , 0.5 , 0.5 , \u0026hellip;, 0.5 , 0.5 , 0.5 ], [0.5 , 0.4975586 , 0.4975586 , \u0026hellip;, 0.10009766, 0.10009766, 0.5 ], [0.5 , 0.4975586 , 0.4975586 , \u0026hellip;, 0.10009766, 0.10009766, 0.5 ], \u0026hellip;, [0.5 , 0.10009766, 0.10009766, \u0026hellip;, 0.10009766, 0.10009766, 0.5 ], [0.5 , 0.10009766, 0.10009766, \u0026hellip;, 0.10009766, 0.10009766, 0.5 ], [0.5 , 0.5 , 0.5 , \u0026hellip;, 0.5 , 0.5 , 0.5 ]],\n[[1.5 , 1.5 , 1.5 , \u0026hellip;, 1.5 , 1.5 , 1.5 ], [1.5 , 1.5170898 , 1.5170898 , \u0026hellip;, 0.30029297, 0.30029297, 1.5 ], [1.5 , 1.5170898 , 1.5170898 , \u0026hellip;, 0.30029297, 0.30029297, 1.5 ], \u0026hellip;, [1.5 , 0.30029297, 0.30029297, \u0026hellip;, 0.30029297, 0.30029297, 1.5 ], [1.5 , 0.30029297, 0.30029297, \u0026hellip;, 0.30029297, 0.30029297, 1.5 ], [1.5 , 1.5 , 1.5 , \u0026hellip;, 1.5 , 1.5 , 1.5 ]]], dtype=float32)longitude(y_dim, x_dim)float32-19.89 -19.78 -19.67 \u0026hellip; 12.89 13.0array([[-19.888672, -19.777344, -19.666992, \u0026hellip;, 12.777344, 12.888672, 13. ], [-19.888672, -19.777344, -19.666992, \u0026hellip;, 12.777344, 12.888672, 13. ], [-19.888672, -19.777344, -19.666992, \u0026hellip;, 12.777344, 12.888672, 13. ], \u0026hellip;, [-19.888672, -19.777344, -19.666992, \u0026hellip;, 12.777344, 12.888672, 13. ], [-19.888672, -19.777344, -19.666992, \u0026hellip;, 12.777344, 12.888672, 13. ], [-19.888672, -19.777344, -19.666992, \u0026hellip;, 12.777344, 12.888672, 13. ]], dtype=float32)latitude(y_dim, x_dim)float3240.07 40.07 40.07 \u0026hellip; 65.0 65.0array([[40.066406, 40.066406, 40.066406, \u0026hellip;, 40.066406, 40.066406, 40.066406], [40.13379 , 40.13379 , 40.13379 , \u0026hellip;, 40.13379 , 40.13379 , 40.13379 ], [40.200195, 40.200195, 40.200195, \u0026hellip;, 40.200195, 40.200195, 40.200195], \u0026hellip;, [64.868164, 64.868164, 64.868164, \u0026hellip;, 64.868164, 64.868164, 64.868164], [64.93457 , 64.93457 , 64.93457 , \u0026hellip;, 64.93457 , 64.93457 , 64.93457 ], [65.00098 , 65.00098 , 65.00098 , \u0026hellip;, 65.00098 , 65.00098 , 65.00098 ]], dtype=float32)Data variables: (3)trend(t_dim, z_dim, y_dim, x_dim)float64nan nan nan nan \u0026hellip; nan nan nan nanarray([[[[ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, 15.32984915, 15.32659394, \u0026hellip;, nan, nan, nan], [ nan, 15.31878144, 15.48658743, \u0026hellip;, nan, nan, nan], \u0026hellip;, [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan]],\n[[ nan, nan, nan, ..., nan, nan, nan], [ nan, 15.32887259, 15.32578014, ..., nan, nan, nan], [ nan, 15.31764212, 15.49049368, ..., nan, nan, nan],    \u0026hellip; [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan]],\n [[ nan, nan, nan, ..., nan, nan, nan], [ nan, 17.44304166, 17.43994921, ..., nan, nan, nan], [ nan, 17.43181119, 17.60466275, ..., nan, nan, nan], ..., [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan]]]])\u0026lt;/pre\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li class='xr-var-item'\u0026gt;\u0026lt;div class='xr-var-name'\u0026gt;\u0026lt;span\u0026gt;seasonal\u0026lt;/span\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;div class='xr-var-dims'\u0026gt;(t_dim, z_dim, y_dim, x_dim)\u0026lt;/div\u0026gt;\u0026lt;div class='xr-var-dtype'\u0026gt;float64\u0026lt;/div\u0026gt;\u0026lt;div class='xr-var-preview xr-preview'\u0026gt;nan nan nan nan ... nan nan nan nan\u0026lt;/div\u0026gt;\u0026lt;input id='attrs-f8f444d0-0360-486f-9684-5b743155864b' class='xr-var-attrs-in' type='checkbox' disabled\u0026gt;\u0026lt;label for='attrs-f8f444d0-0360-486f-9684-5b743155864b' title='Show/Hide attributes'\u0026gt;\u0026lt;svg class='icon xr-icon-file-text2'\u0026gt;\u0026lt;use xlink:href='#icon-file-text2'\u0026gt;\u0026lt;/use\u0026gt;\u0026lt;/svg\u0026gt;\u0026lt;/label\u0026gt;\u0026lt;input id='data-0900c28f-dcde-421b-bdb7-c54d2ca509dd' class='xr-var-data-in' type='checkbox'\u0026gt;\u0026lt;label for='data-0900c28f-dcde-421b-bdb7-c54d2ca509dd' title='Show/Hide data repr'\u0026gt;\u0026lt;svg class='icon xr-icon-database'\u0026gt;\u0026lt;use xlink:href='#icon-database'\u0026gt;\u0026lt;/use\u0026gt;\u0026lt;/svg\u0026gt;\u0026lt;/label\u0026gt;\u0026lt;div class='xr-var-attrs'\u0026gt;\u0026lt;dl class='xr-attrs'\u0026gt;\u0026lt;/dl\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;div class='xr-var-data'\u0026gt;\u0026lt;pre\u0026gt;array([[[[ nan, nan, nan, ..., nan, nan, nan], [ nan, 0.07512087, -0.00853798, ..., nan, nan, nan], [ nan, 0.10083702, 0.33049197, ..., nan, nan, nan], ..., [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan]], [[ nan, nan, nan, ..., nan, nan, nan], [ nan, 0.06437869, -0.01944293, ..., nan, nan, nan], [ nan, 0.09025759, 0.32658572, ..., nan, nan, nan],  \u0026hellip; [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan]],\n [[ nan, nan, nan, ..., nan, nan, nan], [ nan, -0.33511949, -0.31835517, ..., nan, nan, nan], [ nan, -0.3453734 , -0.69400621, ..., nan, nan, nan], ..., [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan]]]])\u0026lt;/pre\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li class='xr-var-item'\u0026gt;\u0026lt;div class='xr-var-name'\u0026gt;\u0026lt;span\u0026gt;residual\u0026lt;/span\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;div class='xr-var-dims'\u0026gt;(t_dim, z_dim, y_dim, x_dim)\u0026lt;/div\u0026gt;\u0026lt;div class='xr-var-dtype'\u0026gt;float64\u0026lt;/div\u0026gt;\u0026lt;div class='xr-var-preview xr-preview'\u0026gt;nan nan nan nan ... nan nan nan nan\u0026lt;/div\u0026gt;\u0026lt;input id='attrs-a8e40b59-2876-4e70-b5fe-8c8f284cd446' class='xr-var-attrs-in' type='checkbox' disabled\u0026gt;\u0026lt;label for='attrs-a8e40b59-2876-4e70-b5fe-8c8f284cd446' title='Show/Hide attributes'\u0026gt;\u0026lt;svg class='icon xr-icon-file-text2'\u0026gt;\u0026lt;use xlink:href='#icon-file-text2'\u0026gt;\u0026lt;/use\u0026gt;\u0026lt;/svg\u0026gt;\u0026lt;/label\u0026gt;\u0026lt;input id='data-53f52dc3-9b89-4362-8a64-43bff3284e3f' class='xr-var-data-in' type='checkbox'\u0026gt;\u0026lt;label for='data-53f52dc3-9b89-4362-8a64-43bff3284e3f' title='Show/Hide data repr'\u0026gt;\u0026lt;svg class='icon xr-icon-database'\u0026gt;\u0026lt;use xlink:href='#icon-database'\u0026gt;\u0026lt;/use\u0026gt;\u0026lt;/svg\u0026gt;\u0026lt;/label\u0026gt;\u0026lt;div class='xr-var-attrs'\u0026gt;\u0026lt;dl class='xr-attrs'\u0026gt;\u0026lt;/dl\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;div class='xr-var-data'\u0026gt;\u0026lt;pre\u0026gt;array([[[[ nan, nan, nan, ..., nan, nan, nan], [ nan, -0.00601061, -0.00601061, ..., nan, nan, nan], [ nan, -0.00601061, -0.00601061, ..., nan, nan, nan], ..., [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan]], [[ nan, nan, nan, ..., nan, nan, nan], [ nan, -0.00601061, -0.00601061, ..., nan, nan, nan], [ nan, -0.00601061, -0.00601061, ..., nan, nan, nan],  \u0026hellip; [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan], [ nan, nan, nan, \u0026hellip;, nan, nan, nan]],\n [[ nan, nan, nan, ..., nan, nan, nan], [ nan, 0.13542462, 0.13542462, ..., nan, nan, nan], [ nan, 0.13542462, 0.13542462, ..., nan, nan, nan], ..., [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan], [ nan, nan, nan, ..., nan, nan, nan]]]])\u0026lt;/pre\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li class='xr-section-item'\u0026gt;\u0026lt;input id='section-08df3af8-fec2-41ed-baa0-5e7a69cfa3e0' class='xr-section-summary-in' type='checkbox' disabled \u0026gt;\u0026lt;label for='section-08df3af8-fec2-41ed-baa0-5e7a69cfa3e0' class='xr-section-summary' title='Expand/collapse section'\u0026gt;Attributes: \u0026lt;span\u0026gt;(0)\u0026lt;/span\u0026gt;\u0026lt;/label\u0026gt;\u0026lt;div class='xr-section-inline-details'\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;div class='xr-section-details'\u0026gt;\u0026lt;dl class='xr-attrs'\u0026gt;\u0026lt;/dl\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/div\u0026gt;  Plot the decomposed time series\ncomponent = xr.DataArray( [\u0026#34;trend\u0026#34;,\u0026#34;seasonal\u0026#34;,\u0026#34;residual\u0026#34;], dims=\u0026#34;component\u0026#34;, name=\u0026#34;component\u0026#34; ) temp_decomp = xr.concat( [grd.dataset.trend, grd.dataset.seasonal,grd.dataset.residual], dim=component ) temp_decomp.name = \u0026#34;temperature\u0026#34; temp_decomp[:,:,0,200,200].plot(hue=\u0026#34;component\u0026#34;) [\u0026lt;matplotlib.lines.Line2D at 0x7f506c139280\u0026gt;, \u0026lt;matplotlib.lines.Line2D at 0x7f506c139160\u0026gt;, \u0026lt;matplotlib.lines.Line2D at 0x7f506c139130\u0026gt;]  ","excerpt":"Overview A function within the Process_data class that will decompose time series into trend, …","ref":"/COAsT/docs/examples/notebooks/general/seasonal_decomp_example/","title":"Seasonal decomp example"},{"body":"","excerpt":"","ref":"/COAsT/docs/examples/notebooks/tidegauge/","title":"Tidegauge"},{"body":"This tutorial gives an overview of some of validation tools available when using the Tidegauge objects in COAsT.\nImport necessary libraries import xarray as xr import numpy as np import coast import datetime import matplotlib.pyplot as plt Define paths fn_dom = \u0026#34;\u0026lt;PATH_TO_NEMO_DOMAIN\u0026gt;\u0026#34; fn_dat = \u0026#34;\u0026lt;PATH_TO_NEMO_DATA\u0026gt;\u0026#34; fn_config = \u0026#34;\u0026lt;PATH_TO_CONFIG.json\u0026gt;\u0026#34; fn_tg = \u0026#34;\u0026lt;PATH_TO_TIDEGAUGE_NETCDF\u0026gt;\u0026#34; # This should already be processed, on the same time dimension # Change this to 0 to not use default files. if 1: #print(f\u0026#34;Use default files\u0026#34;) dir = \u0026#34;./example_files/\u0026#34; fn_dom = dir + \u0026#34;coast_example_nemo_domain.nc\u0026#34; fn_dat = dir + \u0026#34;coast_example_nemo_data.nc\u0026#34; fn_config = \u0026#34;./config/example_nemo_grid_t.json\u0026#34; fn_tidegauge = dir + \u0026#34;tide_gauges/lowestoft-p024-uk-bodc\u0026#34; fn_tg = dir + \u0026#34;tide_gauges/coast_example_tidegauges.nc\u0026#34; # These are a collection (xr.DataSet) of tidegauge observations. Created for this demonstration, they are synthetic. Reading and manipulation We can create our empty tidegauge object:\ntidegauge = coast.Tidegauge() Tidegauge object at 0x55fc4fb4cfc0 initialised  The Tidegauge class contains multiple methods for reading different typical tidegauge formats. This includes reading from the GESLA and BODC databases. To read a gesla file between two dates, we can use:\ndate0 = datetime.datetime(2007,1,10) date1 = datetime.datetime(2007,1,12) tidegauge.read_gesla_v3(fn_tidegauge, date_start = date0, date_end = date1) A Tidegauge object is a type of Timeseries object, so it has the form:\ntidegauge.dataset             /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { \u0026ndash;xr-font-color0: var(\u0026ndash;jp-content-font-color0, rgba(0, 0, 0, 1)); \u0026ndash;xr-font-color2: var(\u0026ndash;jp-content-font-color2, rgba(0, 0, 0, 0.54)); \u0026ndash;xr-font-color3: var(\u0026ndash;jp-content-font-color3, rgba(0, 0, 0, 0.38)); \u0026ndash;xr-border-color: var(\u0026ndash;jp-border-color2, #e0e0e0); \u0026ndash;xr-disabled-color: var(\u0026ndash;jp-layout-color3, #bdbdbd); \u0026ndash;xr-background-color: var(\u0026ndash;jp-layout-color0, white); \u0026ndash;xr-background-color-row-even: var(\u0026ndash;jp-layout-color1, white); \u0026ndash;xr-background-color-row-odd: var(\u0026ndash;jp-layout-color2, #eeeeee); }\nhtml[theme=dark], body.vscode-dark { \u0026ndash;xr-font-color0: rgba(255, 255, 255, 1); \u0026ndash;xr-font-color2: rgba(255, 255, 255, 0.54); \u0026ndash;xr-font-color3: rgba(255, 255, 255, 0.38); \u0026ndash;xr-border-color: #1F1F1F; \u0026ndash;xr-disabled-color: #515151; \u0026ndash;xr-background-color: #111111; \u0026ndash;xr-background-color-row-even: #111111; \u0026ndash;xr-background-color-row-odd: #313131; }\n.xr-wrap { display: block !important; min-width: 300px; max-width: 700px; }\n.xr-text-repr-fallback { /* fallback to plain text repr when CSS is not injected (untrusted notebook) */ display: none; }\n.xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(\u0026ndash;xr-border-color); }\n.xr-header \u0026gt; div, .xr-header \u0026gt; ul { display: inline; margin-top: 0; margin-bottom: 0; }\n.xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; }\n.xr-obj-type { color: var(\u0026ndash;xr-font-color2); }\n.xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; }\n.xr-section-item { display: contents; }\n.xr-section-item input { display: none; }\n.xr-section-item input + label { color: var(\u0026ndash;xr-disabled-color); }\n.xr-section-item input:enabled + label { cursor: pointer; color: var(\u0026ndash;xr-font-color2); }\n.xr-section-item input:enabled + label:hover { color: var(\u0026ndash;xr-font-color0); }\n.xr-section-summary { grid-column: 1; color: var(\u0026ndash;xr-font-color2); font-weight: 500; }\n.xr-section-summary \u0026gt; span { display: inline-block; padding-left: 0.5em; }\n.xr-section-summary-in:disabled + label { color: var(\u0026ndash;xr-font-color2); }\n.xr-section-summary-in + label:before { display: inline-block; content: \u0026lsquo;►\u0026rsquo;; font-size: 11px; width: 15px; text-align: center; }\n.xr-section-summary-in:disabled + label:before { color: var(\u0026ndash;xr-disabled-color); }\n.xr-section-summary-in:checked + label:before { content: \u0026lsquo;▼\u0026rsquo;; }\n.xr-section-summary-in:checked + label \u0026gt; span { display: none; }\n.xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; }\n.xr-section-inline-details { grid-column: 2 / -1; }\n.xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; }\n.xr-section-summary-in:checked ~ .xr-section-details { display: contents; }\n.xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; }\n.xr-array-wrap \u0026gt; label { grid-column: 1; vertical-align: top; }\n.xr-preview { color: var(\u0026ndash;xr-font-color3); }\n.xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; }\n.xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; }\n.xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; }\n.xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; }\n.xr-dim-list li { display: inline-block; padding: 0; margin: 0; }\n.xr-dim-list:before { content: \u0026lsquo;('; }\n.xr-dim-list:after { content: \u0026lsquo;)'; }\n.xr-dim-list li:not(:last-child):after { content: \u0026lsquo;,'; padding-right: 5px; }\n.xr-has-index { font-weight: bold; }\n.xr-var-list, .xr-var-item { display: contents; }\n.xr-var-item \u0026gt; div, .xr-var-item label, .xr-var-item \u0026gt; .xr-var-name span { background-color: var(\u0026ndash;xr-background-color-row-even); margin-bottom: 0; }\n.xr-var-item \u0026gt; .xr-var-name:hover span { padding-right: 5px; }\n.xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; div, .xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; label, .xr-var-list \u0026gt; li:nth-child(odd) \u0026gt; .xr-var-name span { background-color: var(\u0026ndash;xr-background-color-row-odd); }\n.xr-var-name { grid-column: 1; }\n.xr-var-dims { grid-column: 2; }\n.xr-var-dtype { grid-column: 3; text-align: right; color: var(\u0026ndash;xr-font-color2); }\n.xr-var-preview { grid-column: 4; }\n.xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; }\n.xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; }\n.xr-var-attrs, .xr-var-data { display: none; background-color: var(\u0026ndash;xr-background-color) !important; padding-bottom: 5px !important; }\n.xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; }\n.xr-var-data \u0026gt; table { float: right; }\n.xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; }\n.xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; }\ndl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; }\n.xr-attrs dt, .xr-attrs dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; }\n.xr-attrs dt { font-weight: normal; grid-column: 1; }\n.xr-attrs dt:hover span { display: inline-block; background: var(\u0026ndash;xr-background-color); padding-right: 10px; }\n.xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; }\n.xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } \u0026lt;xarray.Dataset\u0026gt; Dimensions: (id_dim: 1, t_dim: 193) Coordinates: time (t_dim) datetime64[ns] 2007-01-10 \u0026hellip; 2007-01-12 longitude (id_dim) float64 1.751 latitude (id_dim) float64 52.47 id_name (id_dim) \u0026lt;U9 'Lowestoft' Dimensions without coordinates: id_dim, t_dim Data variables: ssh (id_dim, t_dim) float64 2.818 2.823 2.871 \u0026hellip; 3.214 3.257 3.371 qc_flags (id_dim, t_dim) int64 1 1 1 1 1 1 1 1 1 1 \u0026hellip; 1 1 1 1 1 1 1 1 1 1xarray.DatasetDimensions:id_dim: 1t_dim: 193Coordinates: (4)time(t_dim)datetime64[ns]2007-01-10 \u0026hellip; 2007-01-12array(['2007-01-10T00:00:00.000000000', '2007-01-10T00:15:00.000000000', '2007-01-10T00:30:00.000000000', '2007-01-10T00:45:00.000000000', '2007-01-10T01:00:00.000000000', '2007-01-10T01:15:00.000000000', '2007-01-10T01:30:00.000000000', '2007-01-10T01:45:00.000000000', '2007-01-10T02:00:00.000000000', '2007-01-10T02:15:00.000000000', '2007-01-10T02:30:00.000000000', '2007-01-10T02:45:00.000000000', '2007-01-10T03:00:00.000000000', '2007-01-10T03:15:00.000000000', '2007-01-10T03:30:00.000000000', '2007-01-10T03:45:00.000000000', '2007-01-10T04:00:00.000000000', '2007-01-10T04:15:00.000000000', '2007-01-10T04:30:00.000000000', '2007-01-10T04:45:00.000000000', '2007-01-10T05:00:00.000000000', '2007-01-10T05:15:00.000000000', '2007-01-10T05:30:00.000000000', '2007-01-10T05:45:00.000000000', '2007-01-10T06:00:00.000000000', '2007-01-10T06:15:00.000000000', '2007-01-10T06:30:00.000000000', '2007-01-10T06:45:00.000000000', '2007-01-10T07:00:00.000000000', '2007-01-10T07:15:00.000000000', '2007-01-10T07:30:00.000000000', '2007-01-10T07:45:00.000000000', '2007-01-10T08:00:00.000000000', '2007-01-10T08:15:00.000000000', '2007-01-10T08:30:00.000000000', '2007-01-10T08:45:00.000000000', '2007-01-10T09:00:00.000000000', '2007-01-10T09:15:00.000000000', '2007-01-10T09:30:00.000000000', '2007-01-10T09:45:00.000000000', \u0026hellip; '2007-01-11T14:30:00.000000000', '2007-01-11T14:45:00.000000000', '2007-01-11T15:00:00.000000000', '2007-01-11T15:15:00.000000000', '2007-01-11T15:30:00.000000000', '2007-01-11T15:45:00.000000000', '2007-01-11T16:00:00.000000000', '2007-01-11T16:15:00.000000000', '2007-01-11T16:30:00.000000000', '2007-01-11T16:45:00.000000000', '2007-01-11T17:00:00.000000000', '2007-01-11T17:15:00.000000000', '2007-01-11T17:30:00.000000000', '2007-01-11T17:45:00.000000000', '2007-01-11T18:00:00.000000000', '2007-01-11T18:15:00.000000000', '2007-01-11T18:30:00.000000000', '2007-01-11T18:45:00.000000000', '2007-01-11T19:00:00.000000000', '2007-01-11T19:15:00.000000000', '2007-01-11T19:30:00.000000000', '2007-01-11T19:45:00.000000000', '2007-01-11T20:00:00.000000000', '2007-01-11T20:15:00.000000000', '2007-01-11T20:30:00.000000000', '2007-01-11T20:45:00.000000000', '2007-01-11T21:00:00.000000000', '2007-01-11T21:15:00.000000000', '2007-01-11T21:30:00.000000000', '2007-01-11T21:45:00.000000000', '2007-01-11T22:00:00.000000000', '2007-01-11T22:15:00.000000000', '2007-01-11T22:30:00.000000000', '2007-01-11T22:45:00.000000000', '2007-01-11T23:00:00.000000000', '2007-01-11T23:15:00.000000000', '2007-01-11T23:30:00.000000000', '2007-01-11T23:45:00.000000000', '2007-01-12T00:00:00.000000000'], dtype='datetime64[ns]')longitude(id_dim)float641.751array([1.75083])latitude(id_dim)float6452.47array([52.473])id_name(id_dim)\u0026lt;U9'Lowestoft'array(['Lowestoft'], dtype='\u0026lt;U9')Data variables: (2)ssh(id_dim, t_dim)float642.818 2.823 2.871 \u0026hellip; 3.257 3.371array([[ 2.818, 2.823, 2.871, 2.931, 2.961, 2.979, 2.953, 2.913, 2.864, 2.806, 2.723, 2.664, 2.606, 2.511, 2.43 , 2.379, 2.296, 2.201, 2.105, 2.006, 1.908, 1.801, 1.684, 1.579, 1.494, 1.402, 1.306, 1.233, 1.171, 1.102, 1.054, 1.028, 0.989, 0.97 , 0.983, 1.004, 1.026, 1.068, 1.153, 1.225, 1.296, 1.362, 1.436, 1.481, 1.536, 1.615, 1.695, 1.726, 1.802, 1.842, 1.86 , 1.872, 1.897, 1.912, 1.946, 1.994, 2.006, 2.028, 2.067, 2.081, 2.098, 2.137, 2.113, 2.068, 2.053, 1.985, 1.917, 1.869, 1.803, 1.695, 1.642, 1.545, 1.463, 1.463, 1.466, 1.462, 1.476, 1.524, 1.574, 1.633, 1.661, 1.717, 1.818, 1.918, 2.018, 2.093, 2.14 , 2.223, 2.278, 2.303, 2.372, 2.375, 2.395, 2.468, 2.481, 2.487, 2.535, 2.543, 2.578, 2.621, 2.627, 2.626, 2.585, 2.563, 2.529, 2.451, 2.335, 2.207, 2.086, 1.982, 1.855, 1.741, 1.618, 1.531, 1.429, 1.342, 1.246, 1.141, 1.031, 0.902, 0.784, 0.673, 0.571, 0.457, 0.323, 0.203, 0.13 , 0.056, -0.028, -0.077, -0.093, -0.143, -0.181, -0.211, -0.217, -0.182, -0.1 , -0.046, 0.02 , 0.121, 0.247, 0.358, 0.468, 0.65 , 0.845, 0.987, 1.059, 1.199, 1.322, 1.38 , 1.465, 1.519, 1.559, 1.691, 1.775, 1.844, 2.019, 2.113, 2.159, 2.266, 2.311, 2.406, 2.512, 2.533, 2.43 , 2.309, 2.185, 2.136, 2.086, 2.066, 2.114, 2.114, 2.051, 2.033, 2.055, 2.1 , 2.192, 2.278, 2.334, 2.421, 2.497, 2.548, 2.603, 2.679, 2.803, 2.859, 2.875, 3.001, 3.075, 3.135, 3.214, 3.257, 3.371]])qc_flags(id_dim, t_dim)int641 1 1 1 1 1 1 1 \u0026hellip; 1 1 1 1 1 1 1 1array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])Attributes: (0)\nAn example data variable could be ssh, or ntr (non-tidal residual). This object can also be used for other instrument types, not just tide gauges. For example moorings.\nEvery id index for this object should use the same time coordinates. Therefore, timeseries need to be aligned before being placed into the object. If there is any padding needed, then NaNs should be used. NaNs should also be used for quality control/data rejection.\nFor the rest of our examples, we will use data from multiple tide gauges on the same time dimension. It will be read in from a simple netCDF file:\n# Create the object and then inset the netcdf dataset tt = xr.open_dataset(fn_tg) obs = coast.Tidegauge(dataset=tt) obs.dataset = obs.dataset.set_coords(\u0026#34;time\u0026#34;) Tidegauge object at 0x55fc4fb4cfc0 initialised  Tidegauge has ready made quick plotting routines for viewing time series and tide gauge location. To look at the tide gauge location:\nfig, ax = obs.plot_on_map() /usr/share/miniconda/envs/coast/lib/python3.8/site-packages/cartopy/io/__init__.py:241: DownloadWarning: Downloading: https://naturalearth.s3.amazonaws.com/50m_physical/ne_50m_coastline.zip warnings.warn(f'Downloading: {url}', DownloadWarning)  id=1 obs.dataset.ssh[id].rename({\u0026#39;t_dim\u0026#39;:\u0026#39;time\u0026#39;}).plot() # rename time dimension to enable automatic x-axis labelling plt.show() Note that start and end dates can also be specified for the Tidegauge.plot_timeseries() method.\nWe can do some simple spatial and temporal manipulations of this data:\n# Cut out a geographical box obs_cut = obs.subset_indices_lonlat_box(lonbounds = [-5, 0], latbounds = [50, 55]) fig, ax = obs_cut.plot_on_map() Tidegauge object at 0x55fc4fb4cfc0 initialised  # Cut out a time window obs_cut = obs.time_slice( date0 = datetime.datetime(2007, 1, 1), date1 = datetime.datetime(2007,1,31)) Tidegauge object at 0x55fc4fb4cfc0 initialised  Gridded model comparison To compare our observations to the model, we will interpolate a model variable to the same time and geographical space as the tidegauge. This is done using the obs_operator() method.\nBut first load some gridded data and manipulate some variable names for convenience\nnemo = coast.Gridded(fn_dat, fn_dom, multiple=True, config=fn_config) # Rename depth_0 to be depth nemo.dataset = nemo.dataset.rename({\u0026#34;depth_0\u0026#34;: \u0026#34;depth\u0026#34;}) #nemo.dataset = nemo.dataset[[\u0026#34;ssh\u0026#34;, \u0026#34;landmask\u0026#34;]] interpolate model onto obs locations\ntidegauge_from_model = obs.obs_operator(nemo, time_interp='nearest') Doing this would create a new interpolated tidegauge called tidegauge_from_model Take a look at tidegauge_from_model.dataset to see for yourself. If a landmask variable is present in the Gridded dataset then the nearest wet points will be taken. Otherwise, just the nearest point is taken. If landmask is required but not present you will need to insert it into the dataset yourself. For nemo data, you could use the bottom_level or mbathy variables to do this. E.g:\n# Create a landmask array and put it into the nemo object. # Here, using the bottom_level == 0 variable from the domain file is enough. nemo.dataset[\u0026#34;landmask\u0026#34;] = nemo.dataset.bottom_level == 0 # Then do the interpolation tidegauge_from_model = obs.obs_operator(nemo, time_interp=\u0026#39;nearest\u0026#39;) Calculating spatial indices. Calculating time indices. Indexing model data at tide gauge locations.. Calculating interpolation distances. Interpolating in time... Tidegauge object at 0x55fc4fb4cfc0 initialised  However, the new tidegauge_from_model will the same number of time entries as the obs data (rather than the model data). So, for a more useful demonstration we trim the observed gauge data so it better matches the model data.\n# Shift the timestamp so it overlaps with the tidegauge data - Not ideal but this is only a demo #obs.dataset.coords[\u0026#34;time\u0026#34;] = obs.dataset.coords[\u0026#34;time\u0026#34;] + 1000000000 * 3600 * 24 * 365 * 3 # Cut down data to be only in 2007 to match model data. start_date = datetime.datetime(2007, 1, 1) end_date = datetime.datetime(2007, 1, 31) obs = obs.time_slice(start_date, end_date) Tidegauge object at 0x55fc4fb4cfc0 initialised  Interpolate model data onto obs locations model_timeseries = obs.obs_operator(nemo) # In this case, transpose the interpolated dataset model_timeseries.dataset = model_timeseries.dataset.transpose() Calculating spatial indices. Calculating time indices. Indexing model data at tide gauge locations.. Calculating interpolation distances. Interpolating in time... Tidegauge object at 0x55fc4fb4cfc0 initialised  For a good comparison, we would like to make sure that both the observed and modelled Tidegauge objects contain the same missing values. TidegaugeAnalysis contains a routine for ensuring this. First create our analysis object:\n# This routine searches for missing values in each dataset and applies them # equally to each corresponding dataset tganalysis = coast.TidegaugeAnalysis() obs_new, model_new = tganalysis.match_missing_values(obs.dataset.ssh, model_timeseries.dataset.ssh) Tidegauge object at 0x55fc4fb4cfc0 initialised Tidegauge object at 0x55fc4fb4cfc0 initialised  Although we input data arrays to the above routine, it returns two new Tidegauge objects. Now you have equivalent and comparable sets of time series that can be easily compared.\nHarmonic Analysis \u0026amp; Non tidal-Residuals The Tidegauge object contains some routines which make harmonic analysis and the calculation/comparison of non-tidal residuals easier. Harmonic analysis is done using the utide package. Please see here for more info.\nFirst we can use the TidegaugeAnalysis class to do a harmonic analysis. Suppose we have two Tidegauge objects called obs and model_timeseries generated from tidegauge observations and model simulations respectively.\nWe can subtract means from all time series:\nThen subtract means from all the time series\n# Subtract means from all time series obs_new = tganalysis.demean_timeseries(obs_new.dataset) model_new = tganalysis.demean_timeseries(model_new.dataset) # Now you have equivalent and comparable sets of time series that can be # easily compared. Tidegauge object at 0x55fc4fb4cfc0 initialised Tidegauge object at 0x55fc4fb4cfc0 initialised  Then we can apply the harmonic analysis (though the example data is too short for this example to be that meaningful):\nCalculate non tidal residuals First, do a harmonic analysis. This routine uses utide\nha_mod = tganalysis.harmonic_analysis_utide(model_new.dataset.ssh, min_datapoints=1) ha_obs = tganalysis.harmonic_analysis_utide(obs_new.dataset.ssh, min_datapoints=1) solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done. solve: matrix prep ... solution ... done.  The harmonic_analysis_utide routine returns a list of utide structure object, one for each id_dim in the Tidegauge object. It can be passed any of the arguments that go to utide. It also has an additional argument min_datapoints which determines a minimum number of data points for the harmonics analysis. If a tidegauge id_dim has less than this number, it will not return an analysis.\nNow, create new TidegaugeMultiple objects containing reconstructed tides:\ntide_mod = tganalysis.reconstruct_tide_utide(model_new.dataset.time, ha_mod) tide_obs = tganalysis.reconstruct_tide_utide(obs_new.dataset.time, ha_obs) prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. Tidegauge object at 0x55fc4fb4cfc0 initialised prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. prep/calcs ... done. Tidegauge object at 0x55fc4fb4cfc0 initialised  Get new TidegaugeMultiple objects containing non tidal residuals:\nntr_mod = tganalysis.calculate_non_tidal_residuals(model_new.dataset.ssh, tide_mod.dataset.reconstructed) ntr_obs = tganalysis.calculate_non_tidal_residuals(obs_new.dataset.ssh, tide_obs.dataset.reconstructed) Tidegauge object at 0x55fc4fb4cfc0 initialised Tidegauge object at 0x55fc4fb4cfc0 initialised  By default, this routines will apply scipy.signal.savgol_filter to the non-tidal residuals to remove some noise. This can be switched off using apply_filter = False.\nThe Doodson X0 filter is an alternative way of estimating non-tidal residuals:\ndx0 = tganalysis.doodson_x0_filter(obs.dataset, \u0026#34;ssh\u0026#34;) Tidegauge object at 0x55fc4fb4cfc0 initialised  This will return a new Tidegauge() object containing filtered ssh data.\nOther TidegaugeAnalysis methods Calculate errors The difference() routine will calculate differences, absolute_differences and squared differenced for all variables:\nntr_diff = tganalysis.difference(ntr_obs.dataset, ntr_mod.dataset) ssh_diff = tganalysis.difference(obs_new.dataset, model_new.dataset) Tidegauge object at 0x55fc4fb4cfc0 initialised Tidegauge object at 0x55fc4fb4cfc0 initialised  We can then easily get mean errors, MAE and MSE\nmean_stats = ntr_diff.dataset.mean(dim=\u0026#34;t_dim\u0026#34;, skipna=True) Threshold Statistics for non-tidal residuals This is a simple extreme value analysis of whatever data you use. It will count the number of peaks and the total time spent over each threshold provided. It will also count the numbers of daily and monthly maxima over each threshold. To this, a Tidegauge object and an array of thresholds (in metres) should be passed:\nthresh_mod = tganalysis.threshold_statistics(ntr_mod.dataset, thresholds=np.arange(0, 2, 0.2)) thresh_obs = tganalysis.threshold_statistics(ntr_obs.dataset, thresholds=np.arange(0, 2, 0.2)) ","excerpt":"This tutorial gives an overview of some of validation tools available when using the Tidegauge …","ref":"/COAsT/docs/examples/notebooks/tidegauge/tidegauge_validation_tutorial/","title":"Tidegauge validation tutorial"},{"body":"Tutorial for processing tabulated tide gauge data. Tidal highs and lows can be scraped from a website such as:\nhttps://www.ntslf.org/tides/tidepred?port=Liverpool\nand format them into a csv file:\nLIVERPOOL (GLADSTONE DOCK) TZ: UT(GMT)/BST Units: METRES Datum: Chart Datum\n01/10/2020 06:29 1.65\n01/10/2020 11:54 9.01\n01/10/2020 18:36 1.87\nThe data can be used in the following demonstration.\nimport coast import numpy as np Load and plot High and Low Water data.\nprint(\u0026#34;load and plot HLW data\u0026#34;) filnam = \u0026#34;./example_files/Gladstone_2020-10_HLW.txt\u0026#34; load and plot HLW data  Set the start and end dates.\ndate_start = np.datetime64(\u0026#34;2020-10-12 23:59\u0026#34;) date_end = np.datetime64(\u0026#34;2020-10-14 00:01\u0026#34;) Initiate a TideGauge object, if a filename is passed it assumes it is a GESLA type object.\ntg = coast.Tidegauge() Tidegauge object at 0x55d21cc47fc0 initialised  Specify the data read as a High Low Water dataset.\ntg.read_hlw(filnam, date_start, date_end) Show dataset. If timezone is specified then it is presented as requested, otherwise uses UTC.\nprint(\u0026#34;Try the TideGauge.show() method:\u0026#34;) tg.show(timezone=\u0026#34;Europe/London\u0026#34;) Try the TideGauge.show() method:  Do a basic plot of these points.\ntg.dataset.plot.scatter(x=\u0026#34;time\u0026#34;, y=\u0026#34;ssh\u0026#34;) \u0026lt;matplotlib.collections.PathCollection at 0x7fd45352c850\u0026gt;  There is a method to locate HLW events around an approximate date and time. First state the time of interest.\ntime_guess = np.datetime64(\u0026#34;2020-10-13 12:48\u0026#34;) Then recover all the HLW events in a +/- window, of specified size (iteger hrs). The default winsize = 2 (hrs).\nHLW = tg.get_tide_table_times(np.datetime64(\u0026#34;2020-10-13 12:48\u0026#34;), method=\u0026#34;window\u0026#34;, winsize=24) Alternatively recover the closest HLW event to the input timestamp.\nHLW = tg.get_tide_table_times(np.datetime64(\u0026#34;2020-10-13 12:48\u0026#34;), method=\u0026#34;nearest_1\u0026#34;) Or the nearest two events to the input timestamp.\nHLW = tg.get_tide_table_times(np.datetime64(\u0026#34;2020-10-13 12:48\u0026#34;), method=\u0026#34;nearest_2\u0026#34;) Extract the Low Tide value.\nprint(\u0026#34;Try the TideGauge.get_tidetabletimes() methods:\u0026#34;) print(\u0026#34;LT:\u0026#34;, HLW[np.argmin(HLW.data)].values, \u0026#34;m at\u0026#34;, HLW[np.argmin(HLW.data)].time.values) Try the TideGauge.get_tidetabletimes() methods: LT: 2.83 m at 2020-10-13T14:36:00.000000000  Extract the High Tide value.\nprint(\u0026#34;HT:\u0026#34;, HLW[np.argmax(HLW.data)].values, \u0026#34;m at\u0026#34;, HLW[np.argmax(HLW.data)].time.values) HT: 8.01 m at 2020-10-13T07:59:00.000000000  Or use the the nearest High Tide method to get High Tide.\nHT = tg.get_tide_table_times(np.datetime64(\u0026#34;2020-10-13 12:48\u0026#34;), method=\u0026#34;nearest_HW\u0026#34;) print(\u0026#34;HT:\u0026#34;, HT.values, \u0026#34;m at\u0026#34;, HT.time.values) HT: [8.01] m at 2020-10-13T07:59:00.000000000  The get_tidetabletimes() method can take extra paremeters such as a window size, an integer number of hours to seek either side of the guess.\nHLW = tg.get_tide_table_times(np.datetime64(\u0026#34;2020-10-13 12:48\u0026#34;), winsize=2, method=\u0026#34;nearest_1\u0026#34;) HLW = tg.get_tide_table_times(np.datetime64(\u0026#34;2020-10-13 12:48\u0026#34;), winsize=1, method=\u0026#34;nearest_1\u0026#34;) ","excerpt":"Tutorial for processing tabulated tide gauge data. Tidal highs and lows can be scraped from a …","ref":"/COAsT/docs/examples/notebooks/tidegauge/tidetable_tutorial/","title":"Tidetable tutorial"},{"body":"This is a demonstration script for using the Transect class in the COAsT package. This object has strict data formatting requirements, which are outlined in tranect.py.\nTransect subsetting (a vertical slice of data between two coordinates): Creating them and performing some custom diagnostics with them.\nIn this tutorial we take a look at subsetting the model data along a transect (a custom straight line) and creating some bespoke diagnostics along it. We look at:\n1. Creating a TRANSECT object, defined between two points. 2. Plotting data along a transect. 3. Calculating flow normal to the transect  Import relevant packages import coast import matplotlib.pyplot as plt Define filepaths for data and configuration root = \u0026#34;./\u0026#34; # And by defining some file paths dn_files = root + \u0026#34;./example_files/\u0026#34; fn_nemo_dat_t = dn_files + \u0026#34;nemo_data_T_grid.nc\u0026#34; fn_nemo_dat_u = dn_files + \u0026#34;nemo_data_U_grid.nc\u0026#34; fn_nemo_dat_v = dn_files + \u0026#34;nemo_data_V_grid.nc\u0026#34; fn_nemo_dom = dn_files + \u0026#34;coast_example_nemo_domain.nc\u0026#34; # Configuration files describing the data files fn_config_t_grid = root + \u0026#34;./config/example_nemo_grid_t.json\u0026#34; fn_config_f_grid = root + \u0026#34;./config/example_nemo_grid_f.json\u0026#34; fn_config_u_grid = root + \u0026#34;./config/example_nemo_grid_u.json\u0026#34; fn_config_v_grid = root + \u0026#34;./config/example_nemo_grid_v.json\u0026#34; Load data variables that are on the NEMO t-grid nemo_t = coast.Gridded(fn_data=fn_nemo_dat_t, fn_domain=fn_nemo_dom, config=fn_config_t_grid) Now create a transect using the coast.TransectT object. The transect is between the points (54 N 15 W) and (56 N, 12 W). This needs to be passed the corresponding NEMO object and transect end points. The model points closest to these coordinates will be selected as the transect end points.\ntran_t = coast.TransectT(nemo_t, (54, -15), (56, -12)) # Inspect the data #tran_t.data # uncomment to print data object summary Plot the data # It is simple to plot a scalar such as temperature along the transect: temp_mean = tran_t.data.temperature.mean(dim=\u0026#34;t_dim\u0026#34;) plt.figure() temp_mean.plot.pcolormesh(y=\u0026#34;depth_0\u0026#34;, yincrease=False) plt.show() Create a nemo f-grid object With NEMO’s staggered grid, the first step is to define the transect on the f-grid so that the velocity components are between f-points. We do not need any model data on the f-grid, just the grid information, so create a nemo f-grid object\nnemo_f = coast.Gridded(fn_domain=fn_nemo_dom, config=fn_config_f_grid) Transect on the f-grid tran_f = coast.TransectF(nemo_f, (54, -15), (56, -12)) # Inspect the data #tran_f.data # uncomment to print data object summary Load model data on the u- and v- grids nemo_u = coast.Gridded(fn_data=fn_nemo_dat_u, fn_domain=fn_nemo_dom, config=fn_config_u_grid) nemo_v = coast.Gridded(fn_data=fn_nemo_dat_v, fn_domain=fn_nemo_dom, config=fn_config_v_grid) Calculate the flow across the transect tran_f.calc_flow_across_transect(nemo_u, nemo_v) # The flow across the transect is stored in a new dataset where the variables are all defined at the points between f-points. #tran_f.data_cross_tran_flow # uncomment to print data object summary Plot the time averaged velocity across the transect # To do this we can plot the ‘normal_velocities’ variable. cross_velocity_mean = tran_f.data_cross_tran_flow.normal_velocities.mean(dim=\u0026#34;t_dim\u0026#34;) plt.figure() cross_velocity_mean.rolling(r_dim=2).mean().plot.pcolormesh(yincrease=False, y=\u0026#34;depth_0\u0026#34;, cbar_kwargs={\u0026#34;label\u0026#34;: \u0026#34;m/s\u0026#34;}) plt.show() Plot volume transport across the transect # To do this we can plot the ‘normal_transports’ variable. plt.figure() cross_transport_mean = tran_f.data_cross_tran_flow.normal_transports.mean(dim=\u0026#34;t_dim\u0026#34;) cross_transport_mean.rolling(r_dim=2).mean().plot() plt.ylabel(\u0026#34;Sv\u0026#34;) plt.show() ","excerpt":"This is a demonstration script for using the Transect class in the COAsT package. This object has …","ref":"/COAsT/docs/examples/notebooks/gridded/transect_tutorial/","title":"Transect tutorial"},{"body":"An example of using COAsT to analysis observational profile data alongside gridded NEMO data.\nLoad modules import coast import glob # For getting file paths import gsw import matplotlib.pyplot as plt import datetime import numpy as np import xarray as xr import coast._utils.general_utils as general_utils import scipy as sp # ====================== UNIV PARAMS =========================== path_examples = \u0026#34;./example_files/\u0026#34; path_config = \u0026#34;./config/\u0026#34; load and preprocess profile and model data fn_wod_var = path_examples + \u0026#34;WOD_example_ragged_standard_level.nc\u0026#34; fn_wod_config = path_config + \u0026#34;example_wod_profiles.json\u0026#34; wod_profile_1d = coast.Profile(config=fn_wod_config) wod_profile_1d.read_wod(fn_wod_var) ./config/example_wod_profiles.json  Reshape into 2D. Choose which observed variables you want.\nvar_user_want = [\u0026#34;salinity\u0026#34;, \u0026#34;temperature\u0026#34;, \u0026#34;nitrate\u0026#34;, \u0026#34;oxygen\u0026#34;, \u0026#34;dic\u0026#34;, \u0026#34;phosphate\u0026#34;, \u0026#34;alkalinity\u0026#34;] wod_profile = coast.Profile.reshape_2d(wod_profile_1d, var_user_want) Depth OK reshape successful salinity observed variable exist OK reshape successful temperature observed variable exist OK reshape successful nitrate variable not in observations oxygen observed variable exist OK reshape successful dic observed variable exist OK reshape successful phosphate observed variable exist OK reshape successful alkalinity variable not in observations  Keep subset.\nwod_profile_sub = wod_profile.subset_indices_lonlat_box(lonbounds=[90, 120], latbounds=[-5, 5]) #wod_profile_sub.dataset # uncomment to print data object summary SEAsia read BGC. Note in this simple test nemo data are only for 3 months from 1990 so the comparisons are not going to be correct but just as a demo.\nfn_seasia_domain = path_examples + \u0026#34;coast_example_domain_SEAsia.nc\u0026#34; fn_seasia_config_bgc = path_config + \u0026#34;example_nemo_bgc.json\u0026#34; fn_seasia_var = path_examples + \u0026#34;coast_example_SEAsia_BGC_1990.nc\u0026#34; seasia_bgc = coast.Gridded( fn_data=fn_seasia_var, fn_domain=fn_seasia_domain, config=fn_seasia_config_bgc, multiple=True ) Domain file does not have mask so this is just a trick.\nseasia_bgc.dataset[\u0026#34;landmask\u0026#34;] = seasia_bgc.dataset.bottom_level == 0 seasia_bgc.dataset = seasia_bgc.dataset.rename({\u0026#34;depth_0\u0026#34;: \u0026#34;depth\u0026#34;}) model_profiles = wod_profile_sub.obs_operator(seasia_bgc) #model_profiles.dataset # uncomment to print data object summary Remove any points that are far from model.\ntoo_far = 5 keep_indices = model_profiles.dataset.interp_dist \u0026lt;= too_far model_profiles = model_profiles.isel(id_dim=keep_indices) wod_profile = wod_profile_sub.isel(id_dim=keep_indices) #wod_profile.dataset # uncomment to print data object summary Plot profiles Transform observed DIC from mmol/l to mmol C/ m^3 that the model has.\nfig = plt.figure() plt.plot(1000 * wod_profile.dataset.dic[8, :], wod_profile.dataset.depth[8, :], linestyle=\u0026#34;\u0026#34;, marker=\u0026#34;o\u0026#34;) plt.plot(model_profiles.dataset.dic[8, :], model_profiles.dataset.depth[:, 8], linestyle=\u0026#34;\u0026#34;, marker=\u0026#34;o\u0026#34;) plt.ylim([2500, 0]) plt.title(\u0026#34;DIC vs depth\u0026#34;) plt.show() fig = plt.figure() plt.plot(wod_profile.dataset.oxygen[8, :], wod_profile.dataset.depth[8, :], linestyle=\u0026#34;\u0026#34;, marker=\u0026#34;o\u0026#34;) plt.plot(model_profiles.dataset.oxygen[8, :], model_profiles.dataset.depth[:, 8], linestyle=\u0026#34;\u0026#34;, marker=\u0026#34;o\u0026#34;) plt.ylim([2500, 0]) plt.title(\u0026#34;Oxygen vs depth\u0026#34;) plt.show() Perform profile analysis to evaluate differences Interpolate seasia to profile depths, using ProfileAnalysis class.\nreference_depths = wod_profile.dataset.depth[20, :].values model_profiles.dataset = model_profiles.dataset[[\u0026#34;dic\u0026#34;]] / 1000 pa = coast.ProfileAnalysis() model_interpolated = pa.interpolate_vertical(model_profiles, wod_profile) Calculate differences.\ndifferences = pa.difference(model_interpolated, wod_profile) #differences.dataset.load() # uncomment to print data object summary ","excerpt":"An example of using COAsT to analysis observational profile data alongside gridded NEMO data.\nLoad …","ref":"/COAsT/docs/examples/notebooks/profile/wod_bgc_ragged_example/","title":"Wod bgc ragged example"},{"body":"Intro Remote access to Copernicus Marine Environment Monitoring Service CMEMS datasets is enabled via OPeNDAP and Pydap.\nOPeNDAP allows COAsT to stream data from Copernicus without downloading specific subsets or the dataset as a whole.\nIn order to access CMEMS data, you must first create an account.\nAfter you have created your account, or if you already have one, a product ID can be selected from the product catalogue.\nExample import coast # Authenticate with Copernicus and select a database. database = coast.Copernicus(USERNAME, PASSWORD, \u0026#34;nrt\u0026#34;) # Instantiate a product with its ID forecast = database.get_product(\u0026#34;global-analysis-forecast-phy-001-024\u0026#34;) # Create a COAsT object with the relevant config file nemo_t = coast.Gridded(fn_data=forecast, config=\u0026#34;./config/example_cmems_grid_t.json\u0026#34;) Look inside the COAsT gridded object: nemo_t.dataset\n\u0026lt;xarray.Dataset\u0026gt; Dimensions: (x_dim: 4320, y_dim: 2041, z_dim: 50, t_dim: 912) Coordinates: longitude (x_dim) float32 -180.0 -179.9 -179.8 ... 179.8 179.8 179.9 latitude (y_dim) float32 -80.0 -79.92 -79.83 -79.75 ... 89.83 89.92 90.0 * z_dim (z_dim) float32 0.494 1.541 2.646 ... 5.275e+03 5.728e+03 time (t_dim) datetime64[ns] 2020-01-01T12:00:00 ... 2022-06-30T12... Dimensions without coordinates: x_dim, y_dim, t_dim Data variables: mlotst (t_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 2041, 4320), meta=np.ndarray\u0026gt; ssh (t_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 2041, 4320), meta=np.ndarray\u0026gt; bottomT (t_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 2041, 4320), meta=np.ndarray\u0026gt; sithick (t_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 2041, 4320), meta=np.ndarray\u0026gt; siconc (t_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 2041, 4320), meta=np.ndarray\u0026gt; usi (t_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 2041, 4320), meta=np.ndarray\u0026gt; vsi (t_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 2041, 4320), meta=np.ndarray\u0026gt; temperature (t_dim, z_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 50, 2041, 4320), meta=np.ndarray\u0026gt; salinity (t_dim, z_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 50, 2041, 4320), meta=np.ndarray\u0026gt; uo (t_dim, z_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 50, 2041, 4320), meta=np.ndarray\u0026gt; vo (t_dim, z_dim, y_dim, x_dim) float32 dask.array\u0026lt;chunksize=(1, 50, 2041, 4320), meta=np.ndarray\u0026gt; Attributes: (12/24) title: daily mean fields from Global Ocean Physics Analysis ... easting: longitude northing: latitude history: 2022/06/21 00:05:41 MERCATOR OCEAN Netcdf creation source: MERCATOR PSY4QV3R1 institution: MERCATOR OCEAN ... longitude_min: -180.0 longitude_max: 179.91667 latitude_min: -80.0 latitude_max: 90.0 z_min: 0.494025 z_max: 5727.917 Or plot a snapshot of surface temperature. (This lazy loaded so may take time to render)\nimport matplotlib.pyplot as plt plt.pcolormesh( nemo_t.dataset.temperature.isel(t_dim=1,z_dim=1)) plt.show() ","excerpt":"Intro Remote access to Copernicus Marine Environment Monitoring Service CMEMS datasets is enabled …","ref":"/COAsT/docs/examples/remote-datasets/copernicus/","title":"Copernicus"},{"body":"","excerpt":"","ref":"/COAsT/docs/examples/remote-datasets/","title":"Remote Datasets"},{"body":"A short script to install COAsT in a conda environment, download and run some build tests.\n# Fresh build module load anaconda/3-5.1.0 # or whatever it takes to activate conda yes | conda env remove --name test_env yes | conda create -n test_env python=3.8 # create a new environment conda activate test_env yes | conda install -c conda-forge -c bodc coast yes | conda install -c conda-forge cartopy=0.18.0 # used for some of the map plotting # Download bits and bobs rm -rf coast_test mkdir coast_test cd coast_test git clone https://github.com/British-Oceanographic-Data-Centre/COAsT.git wget -c https://linkedsystems.uk/erddap/files/COAsT_example_files/COAsT_example_files.zip \u0026amp;\u0026amp; unzip COAsT_example_files.zip ln -s COAsT/unit_testing/ . ln -s COAsT_example_files example_files # Run unit tests python COAsT/unit_testing/unit_test.py \u0026gt; coast_test.txt ## If OK then clean up cd .. rm -rf coast_test Or, trialling a new (Oct 2022) workflow which seems to dig deeper with useful feedback\n# create a new conda env with: conda env update --prune --file environment.yml # run the unit tests with: pip install . \u0026amp;\u0026amp; pytest unit_testing/unit_test.py -s ","excerpt":"A short script to install COAsT in a conda environment, download and run some build tests.\n# Fresh …","ref":"/COAsT/docs/contributing_package/build_test/","title":"Build test"},{"body":"To date the workflow has been to unit test anything and everything that goes into the develop branch and then periodically push to master less frequently and issue a new github release.\nWith the push to master Git Actions build the conda and pip packages and the package receives a zenodo update (https://zenodo.org/account/settings/github/repository/British-Oceanographic-Data-Centre/COAsT) and DOI.\n1. Push to master Any push to master initiates the Git Actions to build and release the package. It is advisable then to prepare the release in develop and only ever pull into master from develop. (Pulling from master to develop could bring unexpected Git Actions to develop). In order for the package builds to work the version of the package must be unique. The version of the package is set in file setup.py. E.g. shown as 2.0.1 below:\n# setup.py ... PACKAGE = SimpleNamespace(**{ \u0026#34;name\u0026#34;: \u0026#34;COAsT\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;2.0.1\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;This is the Coast Ocean Assessment Tool\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://www.bodc.ac.uk\u0026#34;, \u0026#34;download_url\u0026#34;: \u0026#34;https://github.com/British-Oceanographic-Data-Centre/COAsT/\u0026#34;, .... Package version also appears in CITATION.cff file, which therefore also needs updating. E.g.:\n... title: British-Oceanographic-Data-Centre/COAsT: v2.0.1 version: v2.0.1 date-released: 2022-04-07 Version numbering follows the semantic versioning convention. Briefly, given a version number MAJOR.MINOR.PATCH, increment the:\n MAJOR version when you make incompatible API changes, MINOR version when you add functionality in a backwards compatible manner, and PATCH version when you make backwards compatible bug fixes. Additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format.  2. Issue new release Then issue a new release, with the new version label, and annotate the major changes.\n","excerpt":"To date the workflow has been to unit test anything and everything that goes into the develop branch …","ref":"/COAsT/docs/contributing_package/push_to_master/","title":"Push to master"},{"body":"AMM15 - 1.5km resolution Atlantic Margin Model \u0026#34;\u0026#34;\u0026#34; AMM15_example_plot.py Make simple AMM15 SST plot. \u0026#34;\u0026#34;\u0026#34; #%% import coast import numpy as np import xarray as xr import matplotlib.pyplot as plt import matplotlib.colors as colors # colormap fiddling ################################################# #%% Loading data ################################################# config = \u0026#39;AMM15\u0026#39; dir_nam = \u0026#34;/projectsa/NEMO/gmaya/2013p2/\u0026#34; fil_nam = \u0026#34;20130415_25hourm_grid_T.nc\u0026#34; dom_nam = \u0026#34;/projectsa/NEMO/gmaya/AMM15_GRID/amm15.mesh_mask.cs3x.nc\u0026#34; config = \u0026#34;/work/jelt/GitHub/COAsT/config/example_nemo_grid_t.json\u0026#34; sci_t = coast.Gridded(dir_nam + fil_nam, dom_nam, config=config) # , chunks=chunks) chunks = { \u0026#34;x_dim\u0026#34;: 10, \u0026#34;y_dim\u0026#34;: 10, \u0026#34;t_dim\u0026#34;: 10, } # Chunks are prescribed in the config json file, but can be adjusted while the data is lazy loaded. sci_t.dataset.chunk(chunks) # create an empty w-grid object, to store stratification sci_w = coast.Gridded(fn_domain=dom_nam, config=config.replace(\u0026#34;t_nemo\u0026#34;, \u0026#34;w_nemo\u0026#34;)) sci_w.dataset.chunk({\u0026#34;x_dim\u0026#34;: 10, \u0026#34;y_dim\u0026#34;: 10}) # Can reset after loading config json print(\u0026#39;* Loaded \u0026#39;,config, \u0026#39; data\u0026#39;) ################################################# #%% subset of data and domain ## ################################################# # Pick out a North Sea subdomain print(\u0026#39;* Extract North Sea subdomain\u0026#39;) ind_sci = sci_t.subset_indices([51,-4], [62,15]) sci_nwes_t = sci_t.isel(y_dim=ind_sci[0], x_dim=ind_sci[1]) #nwes = northwest europe shelf ind_sci = sci_w.subset_indices([51,-4], [62,15]) sci_nwes_w = sci_w.isel(y_dim=ind_sci[0], x_dim=ind_sci[1]) #nwes = northwest europe shelf #%% Apply masks to temperature and salinity if config == \u0026#39;AMM15\u0026#39;: sci_nwes_t.dataset[\u0026#39;temperature_m\u0026#39;] = sci_nwes_t.dataset.temperature.where( sci_nwes_t.dataset.mask.expand_dims(dim=sci_nwes_t.dataset[\u0026#39;t_dim\u0026#39;].sizes) \u0026gt; 0) sci_nwes_t.dataset[\u0026#39;salinity_m\u0026#39;] = sci_nwes_t.dataset.salinity.where( sci_nwes_t.dataset.mask.expand_dims(dim=sci_nwes_t.dataset[\u0026#39;t_dim\u0026#39;].sizes) \u0026gt; 0) else: # Apply fake masks to temperature and salinity sci_nwes_t.dataset[\u0026#39;temperature_m\u0026#39;] = sci_nwes_t.dataset.temperature sci_nwes_t.dataset[\u0026#39;salinity_m\u0026#39;] = sci_nwes_t.dataset.salinity #%% Plots fig = plt.figure() plt.pcolormesh( sci_t.dataset.longitude, sci_t.dataset.latitude, sci_t.dataset.temperature.isel(z_dim=0).squeeze()) #plt.xlabel(\u0026#39;longitude\u0026#39;) #plt.ylabel(\u0026#39;latitude\u0026#39;) #plt.colorbar() plt.axis(\u0026#39;off\u0026#39;) plt.show() fig.savefig(\u0026#39;AMM15_SST_nocolorbar.png\u0026#39;, dpi=120)    India subcontinent maritime domain. WCSSP India configuration #%% import coast import numpy as np import xarray as xr import dask import matplotlib.pyplot as plt import matplotlib.colors as colors # colormap fiddling ################################################# #%% Loading data ################################################# dir_nam = \u0026#34;/projectsa/COAsT/NEMO_example_data/MO_INDIA/\u0026#34; fil_nam = \u0026#34;ind_1d_cat_20180101_20180105_25hourm_grid_T.nc\u0026#34; dom_nam = \u0026#34;domain_cfg_wcssp.nc\u0026#34; config_t = \u0026#34;/work/jelt/GitHub/COAsT/config/example_nemo_grid_t.json\u0026#34; sci_t = coast.Gridded(dir_nam + fil_nam, dir_nam + dom_nam, config=config_t) #%% Plot fig = plt.figure() plt.pcolormesh( sci_t.dataset.longitude, sci_t.dataset.latitude, sci_t.dataset.temperature.isel(t_dim=0).isel(z_dim=0)) plt.xlabel(\u0026#39;longitude\u0026#39;) plt.ylabel(\u0026#39;latitude\u0026#39;) plt.title(\u0026#39;WCSSP India SST\u0026#39;) plt.colorbar() plt.show() fig.savefig(\u0026#39;WCSSP_India_SST.png\u0026#39;, dpi=120)    South East Asia, 1/12 deg configuration (ACCORD: SEAsia_R12) #%% import coast import numpy as np import xarray as xr import dask import matplotlib.pyplot as plt import matplotlib.colors as colors # colormap fiddling ################################################# #%% Loading data ################################################# dir_nam = \u0026#34;/projectsa/COAsT/NEMO_example_data/SEAsia_R12/\u0026#34; fil_nam = \u0026#34;SEAsia_R12_5d_20120101_20121231_gridT.nc\u0026#34; dom_nam = \u0026#34;domain_cfg_ORCA12_adj.nc\u0026#34; config_t = \u0026#34;/work/jelt/GitHub/COAsT/config/example_nemo_grid_t.json\u0026#34; sci_t = coast.Gridded(dir_nam + fil_nam, dir_nam + dom_nam, config=config_t) #%% Plot fig = plt.figure() plt.pcolormesh( sci_t.dataset.longitude, sci_t.dataset.latitude, sci_t.dataset.soce.isel(t_dim=0).isel(z_dim=0)) plt.xlabel(\u0026#39;longitude\u0026#39;) plt.ylabel(\u0026#39;latitude\u0026#39;) plt.title(\u0026#39;SE Asia, surface salinity (psu)\u0026#39;) plt.colorbar() plt.show() fig.savefig(\u0026#39;SEAsia_R12_SSS.png\u0026#39;, dpi=120)    ","excerpt":"AMM15 - 1.5km resolution Atlantic Margin Model \u0026#34;\u0026#34;\u0026#34; AMM15_example_plot.py Make simple …","ref":"/COAsT/docs/examples/configs_gallery/","title":"Configuration Gallery"},{"body":"__________________________________________________________________________________________ ______ ___ _ _________ .' ___ | .' `. / \\ | _ _ | / .' \\_|/ .-. \\ / _ \\ .--.|_/ | | \\_| | | | | | | / ___ \\ ( (`\\] | | \\ `.___.'\\\\ `-' /_/ / \\ \\_ `'.'. _| |_ `.____ .' `.___.'|____| |____|[\\__) )|_____| Coastal Ocean Assessment Toolbox __________________________________________________________________________________________ COAsT is a Python package for managing and analysing high resolution NEMO output. Here you can find information on obtaining, installing and using COAsT as well as guidelines for contributing to the project.\nThis documentation site is still under construction but you can still find guidelines for contributing to the package and this website. See below for description of each section.\n","excerpt":"__________________________________________________________________________________________ ______ …","ref":"/COAsT/docs/","title":"Documentation"},{"body":"Objects Altimetry()\nAltimetry.read_cmems()\nAltimetry.load()\nAltimetry.load_single()\nAltimetry.load_multiple()\nAltimetry.subset_indices_lonlat_box()\nAltimetry.quick_plot()\nAltimetry.obs_operator()\nAltimetry.crps()\nAltimetry.difference()\nAltimetry.absolute_error()\nAltimetry.mean_absolute_error()\nAltimetry.root_mean_square_error()\nAltimetry.time_mean()\nAltimetry.time_std()\nAltimetry.time_correlation()\nAltimetry.time_covariance()\nAltimetry.basic_stats()\nAltimetry class\nAltimetry() class Altimetry(Track): An object for reading, storing and manipulating altimetry data. Currently the object contains functionality for reading altimetry netCDF data from the CMEMS database. This is the default for initialisation. Data should be stored in an xarray.Dataset, in the form: * Date Format Overview * 1. A single dimension (time). 2. Three coordinates: time, latitude, longitude. All lie on the 'time' dimension. 3. Observed variable DataArrays on the time dimension. There are currently no naming conventions for the variables however examples from the CMEMS database include sla_filtered, sla_unfiltered and mdt (mean dynamic topography). * Methods Overview * *Initialisation and File Reading* -\u0026gt; __init__(): Initialises an ALTIMETRY object. -\u0026gt; read_cmems(): Reads data from a CMEMS netCDF file. *Plotting* -\u0026gt; quick_plot(): Makes a quick along-track plot of specified data. *Model Comparison* -\u0026gt; obs_operator(): For interpolating model data to this object. -\u0026gt; cprs(): Calculates the CRPS between a model and obs variable. -\u0026gt; difference(): Differences two specified variables -\u0026gt; absolute_error(): Absolute difference, two variables -\u0026gt; mean_absolute_error(): MAE between two variables -\u0026gt; root_mean_square_error(): RMSE between two variables -\u0026gt; time_mean(): Mean of a variable in time -\u0026gt; time_std(): St. Dev of a variable in time -\u0026gt; time_correlation(): Correlation between two variables -\u0026gt; time_covariance(): Covariance between two variables -\u0026gt; basic_stats(): Calculates multiple of the above metrics. Altimetry.read_cmems() def Altimetry.read_cmems(self, file_path, multiple):  \nRead file.\nArgs:\nfile_path (str): path to data file\nmultiple (boolean): True if reading multiple files otherwise False\n Altimetry.load() def Altimetry.load(self, file_or_dir, chunks=None, multiple=False):  \nLoads a file into a object\u0026rsquo;s dataset variable using xarray\nArgs:\nfile_or_dir (str) : file name or directory to multiple files.\nchunks (dict) : Chunks to use in Dask [default None]\nmultiple (bool) : If true, load in multiple files from directory.\nIf false load a single file [default False]\n Altimetry.load_single() def Altimetry.load_single(self, file, chunks=None):  \nLoads a single file into object\u0026rsquo;s dataset variable.\nArgs:\nfile (str) : file name or directory to multiple files.\nchunks (dict) : Chunks to use in Dask [default None]\n Altimetry.load_multiple() def Altimetry.load_multiple(self, directory_to_files, chunks=None):  \nLoads multiple files from directory into dataset variable.\nArgs:\ndirectory_to_files (str) : directory path to multiple files.\nchunks (dict) : Chunks to use in Dask [default None]\n Altimetry.subset_indices_lonlat_box() def Altimetry.subset_indices_lonlat_box(self, lonbounds, latbounds):  \nGenerates array indices for data which lies in a given lon/lat box.\nKeyword arguments:\nlon \u0026ndash; Longitudes, 1D or 2D.\nlat \u0026ndash; Latitudes, 1D or 2D\nlonbounds \u0026ndash; Array of form [min_longitude=-180, max_longitude=180]\nlatbounds \u0026ndash; Array of form [min_latitude, max_latitude]\nreturn: Indices corresponding to datapoints inside specified box\n Altimetry.quick_plot() def Altimetry.quick_plot(self, color_var_str=None):  \nQuick plot\n Altimetry.obs_operator() def Altimetry.obs_operator(self, model, mod_var_name, time_interp=nearest, model_mask=None):  \nFor interpolating a model dataarray onto altimetry locations and times.\nFor ALTIMETRY, the interpolation is done independently in two steps:\n1. Horizontal space\n2. Time\nModel data is taken at the surface if necessary (0 index).\nExample usage:\n\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;\naltimetry.obs_operator(nemo_obj, \u0026lsquo;sossheig\u0026rsquo;)\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nmodel : model object (e.g. NEMO)\nmod_var: variable name string to use from model object\ntime_interp: time interpolation method (optional, default: \u0026lsquo;nearest\u0026rsquo;)\nThis can take any string scipy.interpolate would take. e.g.\n\u0026lsquo;nearest\u0026rsquo;, \u0026lsquo;linear\u0026rsquo; or \u0026lsquo;cubic'\nmodel_mask : Mask to apply to model data in geographical interpolation\nof model. For example, use to ignore land points.\nIf None, no mask is applied. If \u0026lsquo;bathy\u0026rsquo;, model variable\n(bathymetry==0) is used. Custom 2D mask arrays can be\nsupplied.\nReturns\n\u0026mdash;\u0026mdash;-\nAdds a DataArray to self.dataset, containing interpolated values.\n Altimetry.crps() def Altimetry.crps(self, model_object, model_var_name, obs_var_name, nh_radius=20, time_interp=linear, create_new_object=True):  \nComparison of observed variable to modelled using the Continuous\nRanked Probability Score. This is done using this ALTIMETRY object.\nThis method specifically performs a single-observation neighbourhood-\nforecast method.\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nmodel_object (model) : Model object (NEMO) containing model data\nmodel_var_name (str) : Name of model variable to compare.\nobs_var_name (str) : Name of observed variable to compare.\nnh_radius (float) : Neighbourhood radius (km)\ntime_interp (str) : Type of time interpolation to use (s)\ncreate_new_obj (bool): If True, save output to new ALTIMETRY obj.\nOtherwise, save to this obj.\nReturns\n\u0026mdash;\u0026mdash;-\nxarray.Dataset containing times, sealevel and quality control flags\nExample Usage\n\u0026mdash;\u0026mdash;-\nCompare modelled \u0026lsquo;sossheig\u0026rsquo; with \u0026lsquo;sla_filtered\u0026rsquo; using CRPS\n crps = altimetry.crps(nemo, \u0026lsquo;sossheig\u0026rsquo;, \u0026lsquo;sla_filtered\u0026rsquo;)\n Altimetry.difference() def Altimetry.difference(self, var_str0, var_str1, date0=None, date1=None):  \nDifference two variables defined by var_str0 and var_str1 between\ntwo dates date0 and date1. Returns xr.DataArray\n Altimetry.absolute_error() def Altimetry.absolute_error(self, var_str0, var_str1, date0=None, date1=None):  \nAbsolute difference two variables defined by var_str0 and var_str1\nbetween two dates date0 and date1. Return xr.DataArray\n Altimetry.mean_absolute_error() def Altimetry.mean_absolute_error(self, var_str0, var_str1, date0=None, date1=None):  \nMean absolute difference two variables defined by var_str0 and\nvar_str1 between two dates date0 and date1. Return xr.DataArray\n Altimetry.root_mean_square_error() def Altimetry.root_mean_square_error(self, var_str0, var_str1, date0=None, date1=None):  \nRoot mean square difference two variables defined by var_str0 and\nvar_str1 between two dates date0 and date1. Return xr.DataArray\n Altimetry.time_mean() def Altimetry.time_mean(self, var_str, date0=None, date1=None):  \nTime mean of variable var_str between dates date0, date1\n Altimetry.time_std() def Altimetry.time_std(self, var_str, date0=None, date1=None):  \nTime st. dev of variable var_str between dates date0 and date1\n Altimetry.time_correlation() def Altimetry.time_correlation(self, var_str0, var_str1, date0=None, date1=None, method=pearson):  \nTime correlation between two variables defined by var_str0,\nvar_str1 between dates date0 and date1. Uses Pandas corr().\n Altimetry.time_covariance() def Altimetry.time_covariance(self, var_str0, var_str1, date0=None, date1=None):  \nTime covariance between two variables defined by var_str0,\nvar_str1 between dates date0 and date1. Uses Pandas corr().\n Altimetry.basic_stats() def Altimetry.basic_stats(self, var_str0, var_str1, date0=None, date1=None, create_new_object=True):  \nCalculates a selection of statistics for two variables defined by\nvar_str0 and var_str1, between dates date0 and date1. This will return\ntheir difference, absolute difference, mean absolute error, root mean\nsquare error, correlation and covariance. If create_new_object is True\nthen this method returns a new ALTIMETRY object containing statistics,\notherwise variables are saved to the dateset inside this object.\n ","excerpt":"Objects Altimetry()\nAltimetry.read_cmems()\nAltimetry.load()\nAltimetry.load_single() …","ref":"/COAsT/docs/reference/altimetry/","title":"Altimetry"},{"body":"Objects Argos()\nArgos.read_data()\nArgos class\nArgos() class Argos(Indexed): Class for reading Argos CSV formatted data files into an xarray object Argos.read_data() def Argos.read_data(self, file_path):  \nRead the data file\nExpected format and variable names are\nDATIM,LAT,LON,SST,SST_F,PSST,PSST_F,PPRES,PPRES_F,BEN\nxarray dataset to have dimension as time and coordinates as time, latitude and longitude\nArgs:\nfile_path (str) : path of data file\n ","excerpt":"Objects Argos()\nArgos.read_data()\nArgos class\nArgos() class Argos(Indexed): Class for reading Argos …","ref":"/COAsT/docs/reference/argos/","title":"Argos"},{"body":"Objects Climatology()\nClimatology.make_climatology()\nClimatology._get_date_ranges()\nClimatology.multiyear_averages()\nClimatology class\nClimatology() class Climatology(Coast): A Python class containing methods for lazily creating climatologies of NEMO data (or any xarray datasets) and writing to file. Also for resampling methods. Climatology.make_climatology() @staticmethod def Climatology.make_climatology(ds, output_frequency, monthly_weights=False, time_var_name=time, time_dim_name=t_dim, fn_out=None):  \nCalculates a climatology for all variables in a supplied dataset.\nThe resulting xarray dataset will NOT be loaded to RAM. Instead,\nit is a set of dask operations. To load to RAM use, e.g. .compute().\nHowever, if the original data was large, this may take a long time and\na lot of memory. Make sure you have the available RAM or chunking\nand parallel processes are specified correctly.\nOtherwise, it is recommended that you access the climatology data\nin an indexed way. I.E. compute only at specific parts of the data\nare once.\nThe resulting cliamtology dataset can be written to disk using\n.to_netcdf(). Again, this may take a while for larger datasets.\nds :: xarray dataset object from a Coast object.\noutput_frequency :: any xarray groupby string. i.e:\n\u0026lsquo;month'\n\u0026lsquo;season'\ntime_var_name :: the string name of the time variable in dataset\ntime_dim_name :: the string name of the time dimension variable in dataset\nfn_out :: string defining full output netcdf file path and name.\n Climatology._get_date_ranges() @staticmethod def Climatology._get_date_ranges(years, month_periods):  \nCalculates a list of datetime date ranges for a given list of years and a specified start/end month.\nArgs:\nyears (list): A list of years to calculate date ranges for.\nmonth_periods (list): A list containing tuples of start and end month integers.\n(i.e. [(3,5),(12, 2)] is Mar -\u0026gt; May, Dec -\u0026gt; Feb). Must be in chronological order.\n Returns:\ndate_ranges (list): A list of tuples, each containing a start and end datetime.date object.\n Climatology.multiyear_averages() @classmethod def Climatology.multiyear_averages(cls, ds, month_periods, time_var=time, time_dim=t_dim):  \nCalculate multiyear means for all Data variables in a dataset between a given start and end month.\nArgs:\nds (xr.Dataset): xarray dataset containing data.\nmonth_periods (list): A list containing tuples of start and end month integers.\n(i.e. [(3,5),(12, 2)] is Mar -\u0026gt; May, Dec -\u0026gt; Feb). Must be in chronological order.\nThe seasons module can be used for convenience (e.g. seasons.WINTER, seasons.ALL etc. )\ntime_var (str): String representing the time variable name within the dataset.\ntime_dim (str): String representing the time dimension name within the dataset.\nreturns:\nds_mean (xr.Dataset): A new dataset containing mean averages for each data variable across all years and\ndate periods. Indexed by the multi-index \u0026lsquo;year_period\u0026rsquo; (i.e. (2000, \u0026lsquo;Dec-Feb\u0026rsquo;)).\n ","excerpt":"Objects Climatology()\nClimatology.make_climatology()\nClimatology._get_date_ranges() …","ref":"/COAsT/docs/reference/climatology/","title":"Climatology"},{"body":"Objects setup_dask_client()\nCoast()\nCoast.load()\nCoast.load_single()\nCoast.load_multiple()\nCoast.load_dataset()\nCoast.set_dimension_mapping()\nCoast.set_variable_mapping()\nCoast.set_grid_ref_attribute()\nCoast.set_dimension_names()\nCoast.set_variable_names()\nCoast.set_variable_grid_ref_attribute()\nCoast.copy()\nCoast.isel()\nCoast.sel()\nCoast.rename()\nCoast.subset()\nCoast.subset_as_copy()\nCoast.distance_between_two_points()\nCoast.subset_indices_by_distance()\nCoast.subset_indices_lonlat_box()\nCoast.calculate_haversine_distance()\nCoast.get_subset_as_xarray()\nCoast.get_2d_subset_as_xarray()\nCoast.plot_simple_2d()\nCoast.plot_cartopy()\nCoast.plot_movie()\nThe coast class is the main access point into this package.\nsetup_dask_client() def setup_dask_client(workers=2, threads=2, memory_limit_per_worker=2GB):  \nNone\n Coast() class Coast(): None Coast.load() def Coast.load(self, file_or_dir, chunks=None, multiple=False):  \nLoads a file into a COAsT object\u0026rsquo;s dataset variable using xarray.\nArgs:\nfile_or_dir (str): file name, OPeNDAP accessor, or directory to multiple files.\nchunks (dict): Chunks to use in Dask [default None].\nmultiple (bool): If true, load in multiple files from directory. If false load a single file [default False].\n Coast.load_single() def Coast.load_single(self, file, chunks=None):  \nLoads a single file into COAsT object\u0026rsquo;s dataset variable.\nArgs:\nfile (str): Input file.\nchunks (Dict): Chunks to use in Dask [default None].\n Coast.load_multiple() def Coast.load_multiple(self, directory_to_files, chunks=None):  \nLoads multiple files from directory into dataset variable.\nArgs:\n directory_to_files (str):\nchunks (Dict): Chunks to use in Dask [default None].\n Coast.load_dataset() def Coast.load_dataset(self, dataset):  \nLoads a dataset.\nArgs:\ndataset (xr.Dataset): Dataset to load.\n Coast.set_dimension_mapping() def Coast.set_dimension_mapping(self):  \nSet mapping of dimensions.\n Coast.set_variable_mapping() def Coast.set_variable_mapping(self):  \nSet mapping of variable.\n Coast.set_grid_ref_attribute() def Coast.set_grid_ref_attribute(self):  \nSet grid reference attribute.\n Coast.set_dimension_names() def Coast.set_dimension_names(self, dim_mapping):  \nRelabel dimensions in COAsT object xarray.dataset to ensure consistent naming throughout the COAsT package.\nArgs:\ndim_mapping (Dict): keys are dimension names to change and values new dimension names.\n Coast.set_variable_names() def Coast.set_variable_names(self, var_mapping):  \nRelabel variables in COAsT object xarray.dataset to ensure consistent naming throughout the COAsT package.\nArgs:\nvar_mapping (Dict): keys are variable names to change and values are new variable names\n Coast.set_variable_grid_ref_attribute() def Coast.set_variable_grid_ref_attribute(self, grid_ref_attr_mapping):  \nSet attributes for variables to access within package and set grid attributes to identify which grid variable is associated with.\nArgs:\ngrid_ref_attr_mapping (Dict): Dict containing mappings.\n Coast.copy() def Coast.copy(self):  \nMethod to copy self.\n Coast.isel() def Coast.isel(self, indexers=None, drop=False, **kwargs):  \nIndexes COAsT object along specified dimensions using xarray isel.\nInput is of same form as xarray.isel. Basic use, hand in either:\n dictionary with keys = dimensions, values = indices\n **kwargs of form dimension = indices.\nArgs:\nindexers (Dict): A dict with keys matching dimensions and values given by integers, slice objects or arrays.\ndrop (bool): If drop=True, drop coordinates variables indexed by integers instead of making them scalar.\n**kwargs (Any): The keyword arguments form of indexers. One of indexers or indexers_kwargs must be provided.\n   Coast.sel() def Coast.sel(self, indexers=None, drop=False, **kwargs):  \nIndexes COAsT object along specified dimensions using xarray sel.\nInput is of same form as xarray.sel. Basic use, hand in either:\n1. Dictionary with keys = dimensions, values = indices\n2. **kwargs of form dimension = indices\nArgs:\nindexers (Dict): A dict with keys matching dimensions and values given by scalars, slices or arrays of tick labels.\ndrop (bool): If drop=True, drop coordinates variables in indexers instead of making them scalar.\n**kwargs (Any): The keyword arguments form of indexers. One of indexers or indexers_kwargs must be provided.\n Coast.rename() def Coast.rename(self, rename_dict, **kwargs):  \nRename dataset.\nArgs:\nrename_dict (Dict): Dictionary whose keys are current variable or dimension names and whose values are the desired names.\n**kwargs (Any): Keyword form of name_dict. One of name_dict or names must be provided.\n Coast.subset() def Coast.subset(self, **kwargs):  \nSubsets all variables within the dataset inside self (a COAsT object).\nInput is a set of keyword argument pairs of the form: dimension_name = indices.\nThe entire object is then subsetted along this dimension at indices\nArgs:\n**kwargs (Any): The keyword arguments form of indexers. One of indexers or indexers_kwargs must be provided.\n Coast.subset_as_copy() def Coast.subset_as_copy(self, **kwargs):  \nSimilar to COAsT.subset() however applies the subsetting to a copy of the original COAsT object.\nThis subsetted copy is then returned.Useful for preserving the original object whilst creating smaller subsetted object copies.\nArgs:\n**kwargs (Any): The keyword arguments form of indexers. One of indexers or indexers_kwargs must be provided.\n Coast.distance_between_two_points() def Coast.distance_between_two_points(self):  \nNone\n Coast.subset_indices_by_distance() def Coast.subset_indices_by_distance(self, centre_lon, centre_lat, radius):  \nThis method returns a tuple of indices within the radius of the lon/lat point given by the user.\nDistance is calculated as haversine - see self.calculate_haversine_distance.\nArgs:\ncentre_lon (float): The longitude of the users central point.\ncentre_lat (float): The latitude of the users central point.\nradius (float): The haversine distance (in km) from the central point.\nReturn:\nTuple[xr.DataArray, xr.DataArray]: All indices in a tuple with the haversine distance of the central point.\n Coast.subset_indices_lonlat_box() def Coast.subset_indices_lonlat_box(self, lonbounds, latbounds):  \nGenerates array indices for data which lies in a given lon/lat box.\nArgs:\nlonbounds: Longitude boundaries. List of form [min_longitude=-180, max_longitude=180].\nlatbounds: Latitude boundaries. List of form [min_latitude, max_latitude].\nReturns:\nnp.ndarray: Indices corresponding to datapoints inside specified box.\n Coast.calculate_haversine_distance() @staticmethod def Coast.calculate_haversine_distance(lon1, lat1, lon2, lat2):  \nEstimation of geographical distance using the Haversine function.\nInput can be single values or 1D arrays of locations. This does NOT create a distance matrix but outputs another 1D array.\nThis works for either location vectors of equal length OR a single location and an arbitrary length location vector.\nArgs:\nlon1 (Any): Angles in degrees.\nlat1 (Any): Angles in degrees.\nlon2 (Any): Angles in degrees.\nlat2 (Any): Angles in degrees.\nReturns:\nfloat: Haversine distance between points.\n Coast.get_subset_as_xarray() def Coast.get_subset_as_xarray(self, var, points_x, points_y, line_length=None, time_counter=0):  \nThis method gets a subset of the data across the x/y indices given for the chosen variable.\nSetting time_counter to None will treat var as only having 3 dimensions depth, y, x\nthere is a check on var to see the size of the time_counter, if 1 then time_counter is fixed to index 0.\nArgs:\nvar (str): The name of the variable to get data from.\npoints_x (slice): A list/array of indices for the x dimension.\npoints_y (slice): A list/array of indices for the y dimension.\nline_length (int): The length of your subset (assuming simple line transect). TODO This is unused.\ntime_counter (int): Which time slice to get data from, if None and the variable only has one a time\nchannel of length 1 then time_counter is fixed too an index of 0.\nReturns:\nxr.DataArray: Data across all depths for the chosen variable along the given indices.\n Coast.get_2d_subset_as_xarray() def Coast.get_2d_subset_as_xarray(self, var, points_x, points_y, line_length=None, time_counter=0):  \nGet 2d subset as an xarray.\nArgs:\nvar (str): Member of dataset.\npoints_x (slice): Keys matching dimensions.\npoints_y (slice): Keys matching dimensions.\nline_length (int): Unused.\ntime_counter (int): Time counter.\nReturn:\nxr.Dataset: Subset.\n Coast.plot_simple_2d() def Coast.plot_simple_2d(self, x, y, data, cmap, plot_info):  \nThis is a simple method that will plot data in a 2d. It is a wrapper for matplotlib\u0026rsquo;s \u0026lsquo;pcolormesh\u0026rsquo; method.\ncmap and plot_info are required to run this method, cmap is passed directly to pcolormesh.\nplot_info contains all the required information for setting the figure;\n- ylim\n- xlim\n- clim\n- title\n- fig_size\n- ylabel\nArgs:\nx (xr.Variable): The variable contain the x axis information.\ny (xr.Variable): The variable contain the y axis information.\ndata (xr.DataArray): the DataArray a user wishes to plot.\ncmap (matplotlib.cm): Matplotlib color map.\nplot_info (Dict): Dict containing all the required information for setting the figure.\n Coast.plot_cartopy() def Coast.plot_cartopy(self, var, plot_var, params, time_counter=0):  \nPlot cartopy.\n Coast.plot_movie() def Coast.plot_movie(self):  \nPlot movie.\n ","excerpt":"Objects setup_dask_client()\nCoast()\nCoast.load()\nCoast.load_single()\nCoast.load_multiple() …","ref":"/COAsT/docs/reference/coast/","title":"Coast"},{"body":"Objects ConfigParser()\nConfigParser._parse_gridded()\nConfigParser._parse_indexed()\nConfigParser._get_code_processing_object()\nConfigParser._get_datafile_object()\nConfig parser.\nConfigParser() class ConfigParser(): Class for parsing gridded and indexed configuration files. ConfigParser._parse_gridded() @staticmethod def ConfigParser._parse_gridded(json_content):  \nStatic method to parse Gridded config files.\nArgs:\njson_content (dict): Config file json.\n ConfigParser._parse_indexed() @staticmethod def ConfigParser._parse_indexed(json_content):  \nStatic method to parse Indexed config files.\nArgs:\njson_content (dict): Config file json.\n ConfigParser._get_code_processing_object() @staticmethod def ConfigParser._get_code_processing_object(json_content):  \nStatic method to convert static_variables configs into objects.\nArgs:\njson_content (dict): Config file json.\n ConfigParser._get_datafile_object() @staticmethod def ConfigParser._get_datafile_object(json_content, data_file_type):  \nStatic method to convert dataset and domain configs into objects.\nArgs:\njson_content (dict): Config file json.\ndata_file_type (str): key of datafile type (dataset or domain).\n ","excerpt":"Objects ConfigParser()\nConfigParser._parse_gridded()\nConfigParser._parse_indexed() …","ref":"/COAsT/docs/reference/config_parser/","title":"Config_parser"},{"body":"Objects ConfigTypes()\nConfigKeys()\nDataFile()\nCodeProcessing()\nDataset()\nDomain()\nConfig()\nGriddedConfig()\nIndexedConfig()\nClasses defining config structure.\nConfigTypes() class ConfigTypes(Enum): Enum class containing the valid types for config files. ConfigKeys() class ConfigKeys(): Class of constants representing valid keys within configuriation json. DataFile() class DataFile(): General parent dataclass for holding common config attributes of datafiles. Args: variable_map (dict): dict containing mapping for variable names. dimension_map (dict): dict containing mapping for dimension names. keep_all_vars (boolean): True if xarray is to retain all data file variables otherwise False i.e keep only those in the json config file variable mappings. CodeProcessing() class CodeProcessing(): Dataclass holding config attributes for static variables that might not need changing between model runs Args: not_grid_variables (list): A list of variables not belonging to the grid. delete_variables (list): A list of variables to drop from the dataset. Dataset() class Dataset(DataFile): Dataclass holding config attributes for Dataset datafiles. Extends DataFile. Args: coord_var (list): list of dataset coordinate variables to apply once dataset is loaded Domain() class Domain(DataFile): Dataclass holding config attributes for Domain datafiles. Extends DataFile. Config() class Config(): General dataclass for holding common config file attributes. Args: dataset (Dataset): Dataset object representing 'dataset' config. processing_flags (list): List of processing flags. chunks (dict): dict for dask chunking config. (i.e. {\u0026quot;dim1\u0026quot;:100, \u0026quot;dim2\u0026quot;:100, \u0026quot;dim3\u0026quot;:100}). type (ConfigTypes): Type of config. Must be a valid ConfigType. GriddedConfig() class GriddedConfig(Config): Dataclass for holding gridded-config specific attributes. Extends Config. Args: type (ConfigTypes): Type of config. Set to ConfigTypes.GRIDDED. grid_ref (dict): dict containing key:value of grid_ref:[list of grid variables]. domain (Domain): Domain object representing 'domain' config. IndexedConfig() class IndexedConfig(Config): Dataclass for holding indexed-config specific attributes. Extends Config. Args: type (ConfigTypes): Type of config. Set to ConfigTypes.INDEXED. ","excerpt":"Objects ConfigTypes()\nConfigKeys()\nDataFile()\nCodeProcessing()\nDataset()\nDomain()\nConfig() …","ref":"/COAsT/docs/reference/config_structure/","title":"Config_structure"},{"body":"Objects Contour()\nContour.get_contours()\nContour.plot_contour()\nContour.get_contour_segment()\nContour.process_contour()\nContour.gen_z_levels()\nContourF()\nContourF.calc_cross_contour_flow()\nContourF._update_cross_flow_vars()\nContourF._update_cross_flow_latlon()\nContourF._pressure_gradient_fpoint2()\nContourF.calc_geostrophic_flow()\nContourT()\nContourT.construct_pressure()\nContourT.calc_along_contour_flow()\nContourT.calc_along_contour_flow_2d()\nContourT._update_flow_vars()\nContourT._update_along_flow_latlon()\nContour classes\nContour() class Contour(): None Contour.get_contours() @staticmethod def Contour.get_contours(gridded, contour_depth):  \nA method to obtain the continuous isbobath contours within a supplied\ngridded domain as a set of y indices and x indices for the model grid.\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\ngridded : Coast\nThe gridded object containing the dataset with the \u0026lsquo;bathymetry\u0026rsquo; variable\ncontour_depth : int\nDepth of desired contours\nReturns\n\u0026mdash;\u0026mdash;-\nList of 2d ndarrays\nEach item of the list contains a different continuous isobath contour as a\n 2d ndarray of indicies, i.e. for each list item:\ncontour[:,0] contains the y indices for the contour on the model grid\ncontour[:,1] contains the x indices for the contour on the model grid\n Contour.plot_contour() @staticmethod def Contour.plot_contour(gridded, contour):  \nQuick plot method to plot a contour over a pcolormesh of the\nmodel bathymetry\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\ngridded : Coast\nThe gridded object containing the dataset with the \u0026lsquo;bathymetry\u0026rsquo; variable\ncontour : 2d array\ncontour[:,0] contains the y indices for the contour on the model grid\ncontour[:,1] contains the x indices for the contour on the model grid\ni.e. contour = np.vstack((y_indices,x_indices)).T\nReturns\n\u0026mdash;\u0026mdash;-\nNone\n Contour.get_contour_segment() @staticmethod def Contour.get_contour_segment(gridded, contour, start_coords, end_coords):  \nMethod that will take a contour from the list of contours generated by\ncoast.Contour.get_contours() and trim it to start at supplied (lat,lon)\ncoordinates and end at supplied (lat, lon) coordinates.\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\ngridded : Coast\nThe gridded object containing the dataset with the \u0026lsquo;bathymetry\u0026rsquo; variable\ncontour : numpy.ndarray\ncontour[:,0] contains the y indices for the contour on the model grid\ncontour[:,1] contains the x indices for the contour on the model grid\nstart_coords : numpy.ndarray\n1d array containing [latitude,longitude] of the start point of the contour\nend_coords : numpy.ndarray\n1d array containing [latitude,longitude] of the end point of the contour\nReturns\n\u0026mdash;\u0026mdash;-\ny_ind : numpy.ndarray\ny indices of the contour on the model grid\nx_ind : numpy.ndarray\nx indices of the contour on the model grid\ncontour : numpy.ndarray\nFor the convenience of plotting using coast.Contour.plot_contour()\ncontour[:,0] = y_ind\ncontour[:,1] = x_ind\n Contour.process_contour() def Contour.process_contour(self, dataset, y_ind, x_ind):  \nRedefine contour so that each point on the contour defined by\ny_ind and x_ind is seperated from its neighbours by a single index change\nin y or x, but not both.\nexample: convert y_ind = [10,11], x_ind = [1,2] to y_ind = [10,10], x_ind = [1,2]\nor y_ind = [10,11], x_ind = [1,1]\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\ndataset : xarray.Dataset\nxarray Dataset from supplied gridded object\ny_ind : numpy.ndarray\n1d array of y indices defining the contour on the model grid\nx_ind : numpy.ndarray\n1d array of x indices defining the contour on the model grid\nReturns\n\u0026mdash;\u0026mdash;-\ny_ind : numpy.ndarray\nprocessed y indices of the contour on the model grid\nx_ind : numpy.ndarray\nprocessed x indices of the contour on the model grid\n Contour.gen_z_levels() @staticmethod def Contour.gen_z_levels(max_depth):  \nGenerates a pre-defined 1d vertical depth coordinates,\ni.e. horizontal z-level vertical coordinates up to a supplied\nmaximum depth, \u0026lsquo;max_depth'\n ContourF() class ContourF(Contour): Class defining a Contour type on the f-grid, which is a 3d dataset of points between a point A and a point B defining an isobath contour. The dataset has a time, depth and contour dimension. The contour dimension defines the points along the contour. The supplied model f-grid Data is subsetted in its entirety along these dimensions within Contour_f.data_contour of type xarray.Dataset Parameters ---------- gridded_f : Coast f-grid gridded object containing the model dataset. y_ind : numpy.ndarray 1d array of y indices defining the contour on the model grid x_ind : numpy.ndarray 1d array of x indices defining the contour on the model grid depth : int Depth of contour isobath ContourF.calc_cross_contour_flow() def ContourF.calc_cross_contour_flow(self, gridded_u, gridded_v):  \nMethod that will calculate the flow across the contour and store this data\nwithin Contour_f.data_cross_flow, which is an xarray.Dataset. Specifically\nContour_f.normal_velocities are the velocities across the contour\n(time, depth, position along contour) in m/s\nContour_f.depth_integrated_normal_transport are the depth integrated\nvolume transports across the contour (time, position along contour) in Sv\nIf the time dependent cell thicknesses (e3) on the u and v grids are\npresent in the gridded_u and gridded_v datasets they will be used, if they\nare not then the initial cell thicknesses (e3_0) will be used.\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\ngridded_u : Coast\nThe gridded object containing the model data on the u-grid.\ngridded_v : Coast\nThe gridded object containing the model data on the v-grid.\nReturns\n\u0026mdash;\u0026mdash;-\nNone.\n ContourF._update_cross_flow_vars() def ContourF._update_cross_flow_vars(self, var, u_var, v_var, dr_n, dr_s, dr_e, dr_w, pos):  \nThis method will pull variable data at specific points along the contour\nfrom the u and v grid datasets and put them into the self.data_cross_flow dataset\n ContourF._update_cross_flow_latlon() def ContourF._update_cross_flow_latlon(self, ds_u, ds_v, dr_n, dr_s, dr_e, dr_w):  \nThis method will pull the latitude and longitude data at specific points along the\ncontour from the u and v grid datasets and put them into the self.data_cross_flow dataset\n ContourF._pressure_gradient_fpoint2() @staticmethod def ContourF._pressure_gradient_fpoint2(ds_t, ds_t_j1, ds_t_i1, ds_t_j1i1, r_ind, velocity_component):  \nCalculates the hydrostatic and surface pressure gradients at a set of f-points\nalong the contour, i.e. at a set of specific values of r_dim (but for all time and depth).\nThe caller must supply four datasets that contain the variables which define\nthe hydrostatic and surface pressure at all vertical z_levels and all time\non the t-points around the contour i.e. for a set of f-points on the contour\ndefined each defined at (j+1/2, i+1/2), we want t-points at (j,i), (j+1,i), (j,i+1), (j+1,i+1),\ncorresponding to ds_t, ds_t_j1, ds_t_i1, ds_t_j1i1, respectively.\nds_t, ds_t_j1, ds_t_i1, ds_t_j1i1 will have dimensions in time and depth.\nThe velocity_component defines whether u or v is normal to the contour\nfor the segments of the contour. A segment of contour is\ndefined as being r_dim to r_dim+1 where r_dim is the along contour dimension.\nReturns\n\u0026mdash;\u0026mdash;-\nhpg_f : DataArray with dimensions in time and depth and along contour\nhydrostatic pressure gradient at a set of f-points along the contour\nfor all time and depth\nspg_f : DataArray with dimensions in time and depth and along contour\nsurface pressure gradient at a set of f-points along the contour\n ContourF.calc_geostrophic_flow() def ContourF.calc_geostrophic_flow(self, gridded_t, ref_density=None, config_u=config/example_nemo_grid_u.json, config_v=config/example_nemo_grid_v.json):  \nThis method will calculate the geostrophic velocity and volume transport\n(due to the geostrophic current) across the contour.\nFour variables are added to the Contour.data_cross_flow dataset:\n normal_velocity_hpg (t_dim, depth_z_levels, r_dim)\nThis is the velocity due to the hydrostatic pressure gradient\n normal_velocity_spg (t_dim, r_dim)\nThis is the velocity due to the surface pressure gradient\n transport_across_AB_hpg (t_dim, r_dim)\nThis is the volume transport due to the hydrostatic pressure gradient\n transport_across_AB_spg (t_dim, r_dim\nThis is the volume transport due to the surface pressure gradient\nThis implementation works by regridding vertically onto horizontal z_levels in order\nto perform the horizontal gradients. Currently s_level depths are\nassumed fixed at their initial depths, i.e. at time zero.\nRequirements: The gridded t-grid dataset, gridded_t, must contain the sea surface height,\nPractical Salinity and the Potential Temperature variables. The depth_0\nfield must also be supplied. The GSW package is used to calculate\nThe Absolute Pressure, Absolute Salinity and Conservate Temperature.\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\ngridded_t : Coast\nThis is the gridded model data on the t-grid for the entire domain.\nref_density : TYPE, optional\nreference density value. If not supplied a mean in time, depth and\nalong the contour will be used as the mean reference value.\nconfig_u : file\nconfiguration file for u-grid object\nconfig_v : file\nconfiguration file for v-grid object\nReturns\n\u0026mdash;\u0026mdash;-\nNone.\n   ContourT() class ContourT(Contour): Class defining a Contour type on the t-grid, which is a 3d dataset of points between a point A and a point B defining an isobath contour. The dataset has a time, depth and contour dimension. The contour dimension defines the points along the contour. The supplied model t-grid Data is subsetted in its entirety along these dimensions and calculations can be performed on this dataset. Parameters ---------- gridded_t : Coast t-grid gridded object containing the model dataset. y_ind : numpy.ndarray 1d array of y indices defining the contour on the model grid x_ind : numpy.ndarray 1d array of x indices defining the contour on the model grid depth : int Depth of contour isobath ContourT.construct_pressure() def ContourT.construct_pressure(self, ref_density=None, z_levels=None, extrapolate=False):  \nThis method is for calculating the hydrostatic and surface pressure fields\non horizontal levels in the vertical (z-levels). The motivation\nis to enable the calculation of horizontal gradients; however,\nthe variables can quite easily be interpolated onto the original\nvertical grid.\nRequirements: The object\u0026rsquo;s t-grid dataset must contain the sea surface height,\nPractical Salinity and the Potential Temperature variables.\nThe GSW package is used to calculate the Absolute Pressure,\nAbsolute Salinity and Conservate Temperature.\nThree new variables (density, hydrostatic pressure, surface pressure)\n are created and added to the Contour_t.data_contour dataset:\ndensity_zlevels (t_dim, depth_z_levels, r_dim)\npressure_h_zlevels (t_dim, depth_z_levels, r_dim)\npressure_s (t_dim, r_dim)\nNote that density is constructed using the EOS10\nequation of state.\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nref_density: float\nreference density value, if None, then the Contour mean across time,\ndepth and along contour will be used.\nz_levels : (optional) numpy array\n1d array that defines the depths to interpolate the density and pressure\non to.\nextrapolate : boolean, default False\nIf true the variables are extrapolated to the deepest z_level, if false,\nvalues below the bathymetry are set to NaN\nReturns\n\u0026mdash;\u0026mdash;-\nNone.\n ContourT.calc_along_contour_flow() def ContourT.calc_along_contour_flow(self, gridded_u, gridded_v):  \nFunction that will calculate the flow along the contour and store this data\nwithin Contour_t.data_along_flow, which is an xarray.Dataset. Specifically\nContour_t.data_along_flow.velocities are the velocities along the contour with dimensions\n(t_dim, z_dim, r_dim), where r_dim is the dimension along the contour.\nContour_t.data_along_flow.transport are the velocities along the contour multiplied by the\nthickness of the cell (velocity * e3) with dimensions\n(t_dim, z_dim, r_dim).\nIf the time dependent cell thicknesses (e3) on the u and v grids are\npresent in the gridded_u and gridded_v datasets they will be used, if they\nare not then the initial cell thicknesses (e3_0) will be used.\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\ngridded_u : Coast\nThe nemo object containing the model data on the u-grid.\ngridded_v : Coast\nThe nemo object containing the model data on the v-grid.\nReturns\n\u0026mdash;\u0026mdash;-\nNone.\n ContourT.calc_along_contour_flow_2d() def ContourT.calc_along_contour_flow_2d(self, gridded_u, gridded_v):  \nFunction that will calculate the 2d flow (no vertical dimension\nalong the contour and store this data within Contour_t.data_along_flow,\nwhich is an xarray.Dataset. Contour_t.data_along_flow.velocities are\nthe velocities along the contour with dimensions (t_dim, r_dim),\nwhere r_dim is the dimension along the contour. e3 and\ne3_0 are interpreted to be the water column thicknesses and\nare included in the dataset as Contour_t.data_along_flow.e3 and\nContour_t.data_along_flow.e3_0\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\ngridded_u : Coast\nThe nemo object containing the model data on the u-grid.\ngridded_v : Coast\nThe nemo object containing the model data on the v-grid.\nReturns\n\u0026mdash;\u0026mdash;-\nNone.\n ContourT._update_flow_vars() def ContourT._update_flow_vars(self, var, u_var, v_var, dr_n, dr_s, dr_e, dr_w, pos):  \nThis method will pull variable data at specific points along the contour\nfrom the u and v grid datasets and put them into the self.data_along_flow dataset\n ContourT._update_along_flow_latlon() def ContourT._update_along_flow_latlon(self, ds_u, ds_v, dr_n, dr_s, dr_e, dr_w):  \nThis method will pull latitude and longitude data at specific points along the\ncontour from the u and v grid datasets and put them into the self.data_along_flow dataset\n ","excerpt":"Objects Contour()\nContour.get_contours()\nContour.plot_contour()\nContour.get_contour_segment() …","ref":"/COAsT/docs/reference/contour/","title":"Contour"},{"body":"Objects CopernicusBase()\nCopernicusBase.get_url()\nProduct()\nProduct.from_copernicus()\nCopernicus()\nCopernicus.get_product()\nFunctionality for accessing Copernicus datasets via OPeNDAP.\nCopernicusBase() class CopernicusBase(): Information required for accessing Copernicus datasets via OPeNDAP. CopernicusBase.get_url() def CopernicusBase.get_url(self, product_id):  \nGet the URL required to access a Copernicus OPeNDAP dataset.\nArgs:\nproduct_id: The product ID belonging to the chosen dataset.\nReturns:\nThe constructed URL.\n Product() class Product(OpendapInfo): Information required to access and stream data from a Copernicus product. Product.from_copernicus() @classmethod def Product.from_copernicus(cls, product_id, copernicus):  \nInstantiate a Product using Copernicus information and a specific product ID.\nArgs:\nproduct_id: The product ID of the chosen Copernicus OPeNDAP dataset.\ncopernicus: A previously instantiated Copernicus info object.\nReturns:\nAn instantiated Product accessor.\n Copernicus() class Copernicus(CopernicusBase): An object for accessing Copernicus products via OPeNDAP. Copernicus.get_product() def Copernicus.get_product(self, product_id):  \nInstantiate a Product related to a specific product ID.\nArgs:\nproduct_id: The product ID of the chosen Copernicus OPeNDAP dataset.\nReturns:\nThe instantiated Product accessor.\n ","excerpt":"Objects CopernicusBase()\nCopernicusBase.get_url()\nProduct()\nProduct.from_copernicus()\nCopernicus() …","ref":"/COAsT/docs/reference/copernicus/","title":"Copernicus"},{"body":"Objects crps_empirical()\ncrps_empirical.calc()\ncrps_empirical_loop()\ncrps_empirical_loop.calc()\ncrps_sonf_fixed()\ncrps_sonf_moving()\nPython definitions used to aid in the calculation of Continuous Ranked Probability Score. Methods Overview -\u0026gt; crps_sonf_fixed(): Single obs neighbourhood forecast CRPS for fixed obs -\u0026gt; crps_song_moving(): Same as above for moving obs\ncrps_empirical() def crps_empirical(sample, obs):  \nCalculates CRPS for a single observations against a sample of values.\nThis sample of values may be an ensemble of model forecasts or a model\nneighbourhood. This is a comparison of a Heaviside function defined by\nthe observation value and an Empirical Distribution Function (EDF)\ndefined by the sample of values. This sample is sorted to create the\nEDF.The calculation method is that outlined by Hersbach et al. (2000).\nEach member of a supplied sample is weighted equally.\nArgs:\nsample (array): Array of points (ensemble or neighbourhood)\nobs (float): A single \u0026lsquo;observation\u0026rsquo; value which to compare against\nsample CDF.\nReturns:\nnp.ndarray: A single CRPS value.\n crps_empirical.calc() def crps_empirical.calc(alpha, beta, p):  \nNone\n crps_empirical_loop() def crps_empirical_loop(sample, obs):  \nLike crps_empirical, however a loop is used instead of numpy\nboolean indexing. For large samples, will be slower but consume less\nmemory.\nArgs:\nsample (array): Array of points (ensemble or neighbourhood)\nobs (float): A single \u0026lsquo;observation\u0026rsquo; value which to compare against\nsample CDF.\nReturns:\nfloat: A single CRPS integral value.\n crps_empirical_loop.calc() def crps_empirical_loop.calc(alpha, beta, p):  \nNone\n crps_sonf_fixed() def crps_sonf_fixed(mod_array, obs_lon, obs_lat, obs_var, obs_time, nh_radius, time_interp):  \nSingle-observation neighbourhood forecast CRPS for a time series at a fixed observation location.\nHandles the calculation of single-observation neighbourhood forecast CRPS for a time series at a fixed observation location.\nDiffers from crps_sonf_moving in that it only need calculate a model neighbourhood once.\nArgs:\nmod_array (xr.DataArray): DataArray from a Model Dataset.\nobs_lon (float): Longitude of fixed observation point.\nobs_lat (float): Latitude of fixed observation point.\nobs_var (np.ndarray): of floatArray of variable values, e.g time series.\nobs_time (np.ndarray): of datetimeArray of times, corresponding to obs_var.\nnh_radius (float): Neighbourhood radius in km.\ntime_interp (str): Type of time interpolation to use.\nReturns:\nTuple[np.ndarray, np.ndarray, np.ndarray]: Array of CRPS values, array containing the number of model points used for\neach CRPS value and an array of bools indicating where a model neighbourhood\ncontained land.\n crps_sonf_moving() def crps_sonf_moving(mod_array, obs_lon, obs_lat, obs_var, obs_time, nh_radius, time_interp):  \nHandles the calculation of single-observation neighbourhood forecast CRPS for a moving observation instrument.\nDiffers from crps_sonf_fixed in that latitude and longitude are arrays of locations. Mod_array must contain\ndimensions x_dim, y_dim and t_dim and coordinates longitude, latitude, time.\nArgs:\nmod_array (xr.DataArray): DataArray from a Model Dataset.\nobs_lon (np.ndarray): Longitudes of fixed observation point.\nobs_lat (np.ndarray): Latitudes of fixed observation point.\nobs_var (np.ndarray): of floatArray of variable values, e.g time series.\nobs_time: (np.ndarray): of datetimeArray of times, corresponding to obs_var.\nnh_radius (float): Neighbourhood radius in km.\ntime_interp (str): Type of time interpolation to use.\nReturns:\nTuple[np.ndarray, np.ndarray, np.ndarray]: Array of CRPS values,\nArray containing the number of model points used for each CRPS value,\nArray of bools indicating where a model neighbourhood contained land.\n ","excerpt":"Objects crps_empirical()\ncrps_empirical.calc()\ncrps_empirical_loop()\ncrps_empirical_loop.calc() …","ref":"/COAsT/docs/reference/crps_util/","title":"Crps_util"},{"body":"Objects DocsyTools()\nDocsyTools.write_class_to_markdown()\nDocsyTools._method_to_str()\nDocsyTools._get_list_of_methods()\nA class to help with writting markdown.\nDocsyTools() class DocsyTools(): DocsyTools Class DocsyTools.write_class_to_markdown() @classmethod def DocsyTools.write_class_to_markdown(cls, class_to_write, fn_out, method_to_omit=unknown, omit_private_methods=True, omit_parent_methods=True):  \nNone\n DocsyTools._method_to_str() @classmethod def DocsyTools._method_to_str(cls, method_name):  \nNone\n DocsyTools._get_list_of_methods() @classmethod def DocsyTools._get_list_of_methods(cls, class_to_search, methods_to_omit=unknown, omit_private_methods=True, omit_parent_methods=True):  \nMethod get a list of methods inside a provided COAsT class, with some other options.\nArgs:\nclass_to_search (Type): Class imported from COAsT (e.g. from coast import Profile)\nmethods_to_omit (List): List of method strings to omit from the output. The default is [].\nomit_private_methods (bool): If true, omit methods beginning with \u0026ldquo;_\u0026quot;. The default is True.\nomit_parent_methods (bool): If true, omit methods in any parent/ancestor class. The default is True.\nReturns:\nList[str]: List of strings denoting method names.\n ","excerpt":"Objects DocsyTools()\nDocsyTools.write_class_to_markdown()\nDocsyTools._method_to_str() …","ref":"/COAsT/docs/reference/docsy_tools/","title":"Docsy_tools"},{"body":"Objects compute_eofs()\ncompute_hilbert_eofs()\n_compute()\nThis is file deals with empirical orthogonal functions.\ncompute_eofs() def compute_eofs(variable, full_matrices=False, time_dim_name=t_dim):  \nCompute some numbers is a helper method.\nComputes the Empirical Orthogonal Functions (EOFs) of a variable (time series)\nthat has 3 dimensions where one is time, i.e. (x,y,time)\nReturns the set of EOF modes, the associated temporal projections and the\nvariance explained by each mode as DataArrays within an xarray Dataset.\nAll-NaN time series, such as those at land points, are handled and ignored;\nhowever, isolated NaNs within a time series, i.e. missing data point, must\nbe filled before calling the function.\nThe variable will be de-meaned in time before the EOFs are computed, normalisation\nshould be carried out before calling the function if desired. The returned EOFs and\ntemporal projections are not scaled or normalised.\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nvariable : (xarray.DataArray), 3-dimensional variable of size (I,J,T),\ncontaining IJ time series\nfull_matrices : (boolean, default False) if false computes only first K EOFs\nwhere K=min(IJ,T), where T is total number of time points. Setting to True\ncould demand significant memory.\ntime_dim_name : (string, default \u0026lsquo;t_dim\u0026rsquo;) the name of the time dimension.\nReturns\n\u0026mdash;\u0026mdash;-\ndataset : xarray Dataset, containing the EOFs, temporal projections and\nvariance explained as xarray DataArrays. The relevent coordinates\nfrom the original data variable are also included\n compute_hilbert_eofs() def compute_hilbert_eofs(variable, full_matrices=False, time_dim_name=t_dim):  \nCompute with hilbert is a helper method.\nComputes the complex Hilbert Empirical Orthogonal Functions (HEOFs) of a\nvariable (time series) that has 3 dimensions where one is time, i.e. (x,y,time).\nSee https://doi.org/10.1002/joc.1499\nReturns the set of HEOF amplitude and phase modes, the associated temporal\nprojection amplitudes and phases and the variance explained by each mode\nas DataArrays within an xarray Dataset.\nAll-NaN time series, such as those at land points, are handled and ignored;\nhowever, isolated NaNs within a time series, i.e. missing data point, must\nbe filled before calling the function.\nThe variable will be de-meaned in time before the EOFs are computed, normalisation\nshould be carried out before calling the function if desired. The returned EOFs and\ntemporal projections are not scaled or normalised.\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nvariable : (xarray.DataArray), 3-dimensional variable of size (I,J,T),\ncontaining IJ time series\nfull_matrices : (boolean, default False) if false computes only first K EOFs\nwhere K=min(IJ,T), where T is total number of time points.\ntime_dim_name : (string, default \u0026lsquo;t_dim\u0026rsquo;) the name of the time dimension.\nReturns\n\u0026mdash;\u0026mdash;-\ndataset : xarray Dataset, containing the EOF amplitudes and phases,\ntemporal projection amplitude and phases and the variance explained\nas xarray DataArrays. The relevent coordinates\nfrom the original data variable are also in the dataset.\n _compute() def _compute(signal, full_matrices, active_ind, number_points):  \nPrivate compute method is a helper method.\nCompute eofs, projections and variance explained using a Singular Value Decomposition\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nsignal : (array) the signal\nfull_matrices : (boolean) whether to return a full or abbreviated SVD\nactive_ind : (array) indices of points with non-null signal\nnumber_points : (int) number of points in original data set\nReturns\n\u0026mdash;\u0026mdash;-\nEOFs : (array) the EOFs in 2d form\nprojections : (array) the projection of the EOFs\nvariance_explained : (array) variance explained by each mode\nmode_count : (int) number of modes computed\n ","excerpt":"Objects compute_eofs()\ncompute_hilbert_eofs()\n_compute()\nThis is file deals with empirical …","ref":"/COAsT/docs/reference/eof/","title":"Eof"},{"body":"Objects experiments()\nnemo_filename_maker()\nSet of functions to control basic experiment file handling\nexperiments() def experiments(experiments=experiments.json):  \nReads a json formatted files, default name is experiments.json\nfor lists of:\nexperiment names (exp_names)\ndirectory names (dir names)\ndomain file names (domains)\nfile names (file_names)\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nexperiments : TYPE, optional\nDESCRIPTION. The default is \u0026lsquo;experiments.json\u0026rsquo;.\nReturns\n\u0026mdash;\u0026mdash;-\nexp_names,dirs,domains,file_names\n nemo_filename_maker() def nemo_filename_maker(directory, year_start, year_stop, grid=T):  \nCreates a list of NEMO file names from a set of standard templates.\nArgs:\ndirectory: path to the files'\nyear_start: start year\nyear_stop: stop year\ngrid: NEMO grid type defaults to T\nReturns: a list of possible nemo file names\n ","excerpt":"Objects experiments()\nnemo_filename_maker()\nSet of functions to control basic experiment file …","ref":"/COAsT/docs/reference/experiments_file_handling/","title":"Experiments_file_handling"},{"body":"Objects determine_season()\nsubset_indices_by_distance_balltree()\nsubset_indices_by_distance()\ncompare_angles()\ncartesian_to_polar()\npolar_to_cartesian()\nsubset_indices_lonlat_box()\ncalculate_haversine_distance()\nremove_indices_by_mask()\nreinstate_indices_by_mask()\nnearest_indices_2d()\ndata_array_time_slice()\nday_of_week()\nA general utility file.\ndetermine_season() def determine_season(t):  \nDetermine season (or array of seasons) from a time (Datetime or xarray)\nobject. Put in an array of times, get out an array of seasons.\n subset_indices_by_distance_balltree() def subset_indices_by_distance_balltree(longitude, latitude, centre_lon, centre_lat, radius, mask=None):  \nReturns the indices of points that lie within a specified radius (km) of\ncentral latitude and longitudes. This makes use of BallTree.query_radius.\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nlongitude : (numpy.ndarray) longitudes in degrees\nlatitude : (numpy.ndarray) latitudes in degrees\ncentre_lon : Central longitude. Can be single value or array of values\ncentre_lat : Central latitude. Can be single value or array of values\nradius : (float) Radius in km within which to find indices\nmask : (numpy.ndarray) of same dimension as longitude and latitude.\nIf specified, will mask out points from the routine.\nReturns\n\u0026mdash;\u0026mdash;-\nReturns an array of indices corresponding to points within radius.\nIf more than one central location is specified, this will be a list\nof index arrays. Each element of which corresponds to one centre.\nIf longitude is 1D:\nReturns one array of indices per central location\nIf longitude is 2D:\nReturns arrays of x and y indices per central location.\nind_y corresponds to row indices of the original input arrays.\n subset_indices_by_distance() def subset_indices_by_distance(longitude, latitude, centre_lon, centre_lat, radius):  \nThis method returns a tuple of indices within the radius of the\nlon/lat point given by the user.\nScikit-learn BallTree is used to obtain indices.\n:param longitude: The longitude of the users central point\n:param latitude: The latitude of the users central point\n:param radius: The haversine distance (in km) from the central point\n:return: All indices in a tuple with the haversine distance of the\ncentral point\n compare_angles() def compare_angles(a1, a2, degrees=True):  \nCompares the difference between two angles. e.g. it is 2 degrees between\n 359 and 1 degree. If degrees = False then will treat angles as radians.\n \n cartesian_to_polar() def cartesian_to_polar(x, y, degrees=True):  \nConversion of cartesian to polar coordinate system\n Output theta is in radians\n \n polar_to_cartesian() def polar_to_cartesian(r, theta, degrees=True):  \nConversion of polar to cartesian coordinate system\n Input theta must be in radians\n \n subset_indices_lonlat_box() def subset_indices_lonlat_box(array_lon, array_lat, lon_min, lon_max, lat_min, lat_max):  \nNone\n calculate_haversine_distance() def calculate_haversine_distance(lon1, lat1, lon2, lat2):  \nEstimation of geographical distance using the Haversine function.\n Input can be single values or 1D arrays of locations. This\n does NOT create a distance matrix but outputs another 1D array.\n This works for either location vectors of equal length OR a single loc\n and an arbitrary length location vector.\n #\nlon1, lat1 :: Location(s) 1.\n lon2, lat2 :: Location(s) 2.\n \n remove_indices_by_mask() def remove_indices_by_mask(array, mask):  \nRemoves indices from a 2-dimensional array, A, based on true elements of\nmask. A and mask variable should have the same shape.\n reinstate_indices_by_mask() def reinstate_indices_by_mask(array_removed, mask, fill_value=unknown):  \nRebuilds a 2D array from a 1D array created using remove_indices_by_mask().\nFalse elements of mask will be populated using array_removed. MAsked\nindices will be replaced with fill_value\n nearest_indices_2d() def nearest_indices_2d(mod_lon, mod_lat, new_lon, new_lat, mask=None):  \nObtains the 2 dimensional indices of the nearest model points to specified\nlists of longitudes and latitudes. Makes use of sklearn.neighbours\nand its BallTree haversine method. Ensure there are no NaNs in\ninput longitude/latitude arrays (or mask them using \u0026ldquo;mask\u0026rdquo;\u0026quot;)\nExample Usage\n\u0026mdash;\u0026mdash;\u0026mdash;-\nGet indices of model points closest to altimetry points\n ind_x, ind_y = nemo.nearest_indices(altimetry.dataset.longitude,\naltimetry.dataset.latitude)\nNearest neighbour interpolation of model dataset to these points\n interpolated = nemo.dataset.isel(x_dim = ind_x, y_dim = ind_y)\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nmod_lon (2D array): Model longitude (degrees) array (2-dimensional)\nmod_lat (2D array): Model latitude (degrees) array (2-dimensions)\nnew_lon (1D array): Array of longitudes (degrees) to compare with model\nnew_lat (1D array): Array of latitudes (degrees) to compare with model\nmask (2D array): Mask array. Where True (or 1), elements of array will\nnot be included. For example, use to mask out land in\ncase it ends up as the nearest point.\nReturns\n\u0026mdash;\u0026mdash;-\nArray of x indices, Array of y indices\n data_array_time_slice() def data_array_time_slice(data_array, date0, date1):  \nTakes an xr.DataArray object and returns a new object with times\nsliced between dates date0 and date1. date0 and date1 may be a string or\ndatetime type object.\n day_of_week() def day_of_week(date=None):  \nReturn the day of the week (3 letter str)\n ","excerpt":"Objects determine_season()\nsubset_indices_by_distance_balltree()\nsubset_indices_by_distance() …","ref":"/COAsT/docs/reference/general_utils/","title":"General_utils"},{"body":"Objects Glider()\nGlider.load_single()\nGlider class\nGlider() class Glider(Indexed): Glider class for reading in glider data (netcdf format) into an xarray object. Glider.load_single() def Glider.load_single(self, file_path, chunks=None):  \nLoads a single file into object\u0026rsquo;s dataset variable.\nArgs:\nfile_path (str): path to data file\nchunks (dict): chunks\n ","excerpt":"Objects Glider()\nGlider.load_single()\nGlider class\nGlider() class Glider(Indexed): Glider class for …","ref":"/COAsT/docs/reference/glider/","title":"Glider"},{"body":"Objects Gridded()\nGridded._setup_grid_obj()\nGridded.make_lonLat_2d()\nGridded.set_grid_vars()\nGridded.load_domain()\nGridded.merge_domain_into_dataset()\nGridded.set_grid_ref_attr()\nGridded.get_contour_complex()\nGridded.set_timezero_depths()\nGridded.calc_bathymetry()\nGridded.subset_indices()\nGridded.find_j_i()\nGridded.find_j_i_list()\nGridded.find_j_i_domain()\nGridded.transect_indices()\nGridded.interpolate_in_space()\nGridded.interpolate_in_time()\nGridded.construct_density()\nGridded.trim_domain_size()\nGridded.copy_domain_vars_to_dataset()\nGridded.differentiate()\nGridded.apply_doodson_x0_filter()\nGridded.get_e3_from_ssh()\nGridded.harmonics_combine()\nGridded.harmonics_convert()\nGridded.time_slice()\nGridded.calculate_vertical_mask()\nGridded class\nGridded() class Gridded(Coast): Words to describe the NEMO class kwargs -- define addition keyworded arguemts for domain file. E.g. ln_sco=1 if using s-scoord in an old domain file that does not carry this flag. Gridded._setup_grid_obj() def Gridded._setup_grid_obj(self, chunks, multiple, **kwargs):  \nThis is a helper method to reduce the size of def __init__\nArgs:\nchunks: This is a setting for xarray as to whether dask (parrell processing) should be on and how it works\nmultiple: falg to tell if we are loading one or more files\n**kwargs: pass direct to loaded xarray dataset\n Gridded.make_lonLat_2d() def Gridded.make_lonLat_2d(self):  \nExpand 1D latitude and longitude variables to 2D.\n Gridded.set_grid_vars() def Gridded.set_grid_vars(self):  \nDefine the variables to map from the domain file to the NEMO obj\n Gridded.load_domain() def Gridded.load_domain(self, fn_domain, chunks):  \nLoads domain file and renames dimensions with dim_mapping_domain\n Gridded.merge_domain_into_dataset() def Gridded.merge_domain_into_dataset(self, dataset_domain):  \nMerge domain dataset variables into self.dataset, using grid_ref\n Gridded.set_grid_ref_attr() def Gridded.set_grid_ref_attr(self):  \nNone\n Gridded.get_contour_complex() def Gridded.get_contour_complex(self, var, points_x, points_y, points_z, tolerance=0.2):  \nNone\n Gridded.set_timezero_depths() def Gridded.set_timezero_depths(self, dataset_domain, calculate_bathymetry=False):  \nCalculates the depths at time zero (from the domain_cfg input file)\nfor the appropriate grid.\nThe depths are assigned to domain_dataset.depth_0\nArgs:\ndataset_domain: a complex data object.\ncalculate_bathymetry: Flag that will either calculate bathymetry (true) or load it from dataset_domain file\n(false).\n Gridded.calc_bathymetry() def Gridded.calc_bathymetry(self, dataset_domain):  \nNEMO approach to defining bathymetry by summing scale factors at various\ngrid locations.\nWorks with z-coordinates on u- and v- faces where bathymetry is defined\nat the top of the cliff, not at the bottom\nArgs:\ndataset_domain: a complex data object.\n Gridded.subset_indices() def Gridded.subset_indices(self):  \nbased on transect_indices, this method looks to return all indices between the given points.\nThis results in a \u0026lsquo;box\u0026rsquo; (Quadrilateral) of indices.\nconsequently the returned lists may have different lengths.\n:param start: A lat/lon pair\n:param end: A lat/lon pair\n:return: list of y indices, list of x indices,\n Gridded.find_j_i() def Gridded.find_j_i(self):  \nA routine to find the nearest y x coordinates for a given latitude and longitude\nUsage: [y,x] = find_j_i(lat=49, lon=-12)\n:param lat: latitude\n:param lon: longitude\n:return: the y and x coordinates for the NEMO object\u0026rsquo;s grid_ref, i.e. t,u,v,f,w.\n Gridded.find_j_i_list() def Gridded.find_j_i_list(self):  \nA routine to find the nearest y x coordinates for a list of latitude and longitude values\nUsage: [y,x] = find_j_i(lat=[49,50,51], lon=[-12,-11,10])\n:param lat: latitude\n:param lon: longitude\n:optional n_nn=1 number of nearest neighbours\n:return: the j, i coordinates for the NEMO object\u0026rsquo;s grid_ref, i.e. t,u,v,f,w. and a distance measure\n Gridded.find_j_i_domain() def Gridded.find_j_i_domain(self):  \nA routine to find the nearest y x coordinates for a given latitude and longitude\nUsage: [y,x] = find_j_i_domain(lat=49, lon=-12, dataset_domain=dataset_domain)\n:param lat: latitude\n:param lon: longitude\n:param dataset_domain: dataset domain\n:return: the y and x coordinates for the grid_ref variable within the domain file\n Gridded.transect_indices() def Gridded.transect_indices(self, start, end):  \nThis method returns the indices of a simple straight line transect between two\nlat lon points defined on the NEMO object\u0026rsquo;s grid_ref, i.e. t,u,v,f,w.\n:type start: tuple A lat/lon pair\n:type end: tuple A lat/lon pair\n:return: array of y indices, array of x indices, number of indices in transect\n Gridded.interpolate_in_space() @staticmethod def Gridded.interpolate_in_space(model_array, new_lon, new_lat, mask=None):  \nInterpolates a provided xarray.DataArray in space to new longitudes\nand latitudes using a nearest neighbour method (BallTree).\nExample Usage\n\u0026mdash;\u0026mdash;\u0026mdash;-\nGet an interpolated DataArray for temperature onto two locations\n interpolated = nemo.interpolate_in_space(nemo.dataset.votemper,\n[0,1], [45,46])\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nmodel_array (xr.DataArray): Model variable DataArray to interpolate\nnew_lons (1Darray): Array of longitudes (degrees) to compare with model\nnew_lats (1Darray): Array of latitudes (degrees) to compare with model\nmask (2D array): Mask array. Where True (or 1), elements of array will\nnot be included. For example, use to mask out land in\ncase it ends up as the nearest point.\nReturns\n\u0026mdash;\u0026mdash;-\nInterpolated DataArray\n Gridded.interpolate_in_time() @staticmethod def Gridded.interpolate_in_time(model_array, new_times, interp_method=nearest, extrapolate=True):  \nInterpolates a provided xarray.DataArray in time to new python\ndatetimes using a specified scipy.interpolate method.\nExample Useage\n\u0026mdash;\u0026mdash;\u0026mdash;-\nGet an interpolated DataArray for temperature onto altimetry times\n new_times = altimetry.dataset.time\ninterpolated = nemo.interpolate_in_space(nemo.dataset.votemper,\nnew_times)\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nmodel_array (xr.DataArray): Model variable DataArray to interpolate\nnew_times (array): New times to interpolate to (array of datetimes)\ninterp_method (str): Interpolation method\nReturns\n\u0026mdash;\u0026mdash;-\nInterpolated DataArray\n Gridded.construct_density() def Gridded.construct_density(self, eos=EOS10, rhobar=False, Zd_mask=unknown, CT_AS=False, pot_dens=False, Tbar=True, Sbar=True):  \nConstructs the in-situ density using the salinity, temperture and\ndepth_0 fields and adds a density attribute to the t-grid dataset\nRequirements: The supplied t-grid dataset must contain the\nPractical Salinity and the Potential Temperature variables. The depth_0\nfield must also be supplied. The GSW package is used to calculate\nThe Absolute Pressure, Absolute Salinity and Conservate Temperature.\nNote that currently density can only be constructed using the EOS10\nequation of state.\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\neos : equation of state, optional\nDESCRIPTION. The default is \u0026lsquo;EOS10\u0026rsquo;.\nrhobar : Calculate density with depth mean T and S\nDESCRIPTION. The default is \u0026lsquo;False\u0026rsquo;.\nZd_mask : Provide a 3D mask for rhobar calculation\nCalculate using calculate_vertical_mask\nDESCRIPTION. The default is empty.\nCT_AS : Conservative Temperature and Absolute Salinity already provided\nDESCRIPTION. The default is \u0026lsquo;False\u0026rsquo;.\npot_dens :Calculation at zero pressure\nDESCRIPTION. The default is \u0026lsquo;False\u0026rsquo;.\nTbar and Sbar : If rhobar is True then these can be switch to False to allow one component to\nremain depth varying. So Tbar=Flase gives temperature component, Sbar=Flase gives Salinity component\nDESCRIPTION. The default is \u0026lsquo;True\u0026rsquo;.\nReturns\n\u0026mdash;\u0026mdash;-\nNone.\nadds attribute NEMO.dataset.density\n Gridded.trim_domain_size() def Gridded.trim_domain_size(self, dataset_domain):  \nTrim the domain variables if the dataset object is a spatial subset\nNote: This breaks if the SW \u0026amp; NW corner values of nav_lat and nav_lon\nare masked, as can happen if on land\u0026hellip;\n Gridded.copy_domain_vars_to_dataset() def Gridded.copy_domain_vars_to_dataset(self, dataset_domain, grid_vars):  \nMap the domain coordinates and metric variables to the dataset object.\nExpects the source and target DataArrays to be same sizes.\n Gridded.differentiate() def Gridded.differentiate(self, in_var_str, config_path=None, dim=z_dim, out_var_str=None, out_obj=None):  \nDerivatives are computed in x_dim, y_dim, z_dim (or i,j,k) directions\nwrt lambda, phi, or z coordinates (with scale factor in metres not degrees).\nDerivatives are calculated using the approach adopted in NEMO,\nspecifically using the 1st order accurate central difference\napproximation. For reference see section 3.1.2 (sec. Discrete operators)\nof the NEMO v4 Handbook.\nCurrently the method does not accomodate all possible eventualities. It\ncovers:\n d(grid_t)/dz \u0026ndash;\u0026gt; grid_w\nReturns an object (with the appropriate target grid_ref) containing\nderivative (out_var_str) as xr.DataArray\nThis is hardwired to expect:\n depth_0 and e3_0 fields exist\n xr.DataArrays are 4D\n self.filename_domain if out_obj not specified\n If out_obj is not specified, one is built that is the size of\nself.filename_domain. I.e. automatic subsetting of out_obj is not\nsupported.\nExample usage:\n\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;\n  Initialise DataArrays\n nemo_t = coast.NEMO( fn_data, fn_domain, grid_ref='t-grid\u0026rsquo; )\nCompute dT/dz\n nemo_w_1 = nemo_t.differentiate( \u0026lsquo;temperature\u0026rsquo;, dim='z_dim\u0026rsquo; )\nFor f(z)=-z. Compute df/dz = -1. Surface value is set to zero\n nemo_t.dataset[\u0026lsquo;depth4D\u0026rsquo;],_ = xr.broadcast( nemo_t.dataset[\u0026lsquo;depth_0\u0026rsquo;], nemo_t.dataset[\u0026lsquo;temperature\u0026rsquo;] )\nnemo_w_4 = nemo_t.differentiate( \u0026lsquo;depth4D\u0026rsquo;, dim='z_dim\u0026rsquo;, out_var_str='dzdz\u0026rsquo; )\nProvide an existing target NEMO object and target variable name:\nnemo_w_1 = nemo_t.differentiate( \u0026lsquo;temperature\u0026rsquo;, dim='z_dim\u0026rsquo;, out_var_str='dTdz\u0026rsquo;, out_obj=nemo_w_1 )\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nin_var_str : str, name of variable to differentiate\nconfig_path : str, path to the w grid config file\ndim : str, dimension to operate over. E.g. {\u0026lsquo;z_dim\u0026rsquo;, \u0026lsquo;y_dim\u0026rsquo;, \u0026lsquo;x_dim\u0026rsquo;, \u0026lsquo;t_dim\u0026rsquo;}\nout_var_str : str, (optional) name of the target xr.DataArray\nout_obj : exiting NEMO obj to store xr.DataArray (optional)\n Gridded.apply_doodson_x0_filter() def Gridded.apply_doodson_x0_filter(self, var_str):  \nApplies Doodson X0 filter to a variable.\nInput variable is expected to be hourly.\nOutput is saved back to original dataset as {var_str}_dxo\n!!WARNING: Will load in entire variable to memory. If dataset large,\nthen subset before using this method or ensure you have enough free\nRAM to hold the variable (twice).\nDB:: Currently not tested in unit_test.py\n Gridded.get_e3_from_ssh() @staticmethod def Gridded.get_e3_from_ssh(nemo_t, e3t=True, e3u=False, e3v=False, e3f=False, e3w=False, dom_fn=None):  \nWhere the model has been run with a nonlinear free surface\nand z* variable volumne (ln_vvl_zstar=True) then the vertical scale factors\nwill vary in time (and space). This function will compute the vertical\nscale factors e3t, e3u, e3v, e3f and e3w by using the sea surface height\nfield (ssh variable) and initial scale factors from the domain_cfg file.\nThe vertical scale factors will be computed at the same model time as the\nssh and if the ssh field is averaged in time then the scale factors will\nalso be time averages.\nA t-grid NEMO object containing the ssh variable must be passed in. Either\nthe domain_cfg path must have been passed in as an argument when the NEMO\nobject was created or it must be passed in here using the dom_fn argument.\ne.g. e3t,e3v,e3f = coast.NEMO.get_e3_from_ssh(nemo_t,true,false,true,true,false)\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nnemo_t : (Coast.NEMO), NEMO object on the t-grid containing the ssh variable\ne3t : (boolean), true if e3t is to be returned. Default True.\ne3u : (boolean), true if e3u is to be returned. Default False.\ne3v : (boolean), true if e3v is to be returned. Default False.\ne3f : (boolean), true if e3f is to be returned. Default False.\ne3w : (boolean), true if e3w is to be returned. Default False.\ndom_fn : (str), Optional, path to domain_cfg file.\nReturns\n\u0026mdash;\u0026mdash;-\nTuple of xarray.DataArrays\n(e3t, e3u, e3v, e3f, e3w)\nOnly those requested will be returned, but the ordering is always the same.\n Gridded.harmonics_combine() def Gridded.harmonics_combine(self, constituents, components=unknown):  \nContains a new NEMO object containing combined harmonic information\nfrom the original object.\nNEMO saves harmonics to individual variables such as M2x, M2y\u0026hellip; etc.\nThis routine will combine these variables (depending on constituents)\ninto a single data array. This new array will have the new dimension\n\u0026lsquo;constituent\u0026rsquo; and a new data coordinate \u0026lsquo;constituent_name\u0026rsquo;.\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nconstituents : List of strings containing constituent names to combine.\nThe case of these strings should match that used in\nNEMO output. If a constituent is not found, no problem,\nit just won\u0026rsquo;t be in the combined dataset.\ncomponents : List of strings containing harmonic components to look\nfor. By default, this looks for the complex components\n\u0026lsquo;x\u0026rsquo; and \u0026lsquo;y\u0026rsquo;. E.g. if constituents = [\u0026lsquo;M2\u0026rsquo;] and\ncomponents is left as default, then the routine looks\nfor [\u0026lsquo;M2x\u0026rsquo;, and \u0026lsquo;M2y\u0026rsquo;].\nReturns\n\u0026mdash;\u0026mdash;-\nNEMO() object, containing combined harmonic variables in a new dataset.\n Gridded.harmonics_convert() def Gridded.harmonics_convert(self, direction=cart2polar, x_var=harmonic_x, y_var=harmonic_y, a_var=harmonic_a, g_var=harmonic_g, degrees=True):  \nConverts NEMO harmonics from cartesian to polar or vice versa.\nMake sure this NEMO object contains combined harmonic variables\nobtained using harmonics_combine().\n*Note:\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\ndirection (str) : Choose \u0026lsquo;cart2polar\u0026rsquo; or \u0026lsquo;polar2cart\u0026rsquo;. If \u0026lsquo;cart2polar'\nThen will look for variables x_var and y_var. If\npolar2cart, will look for a_var (amplitude) and\ng_var (phase).\nx_var (str) : Harmonic x variable name in dataset (or output)\ndefault = \u0026lsquo;harmonic_x\u0026rsquo;.\ny_var (str) : Harmonic y variable name in dataset (or output)\ndefault = \u0026lsquo;harmonic_y\u0026rsquo;.\na_var (str) : Harmonic amplitude variable name in dataset (or output)\ndefault = \u0026lsquo;harmonic_a\u0026rsquo;.\ng_var (str) : Harmonic phase variable name in dataset (or output)\ndefault = \u0026lsquo;harmonic_g\u0026rsquo;.\ndegrees (bool) : Whether input/output phase are/will be in degrees.\nDefault is True.\nReturns\n\u0026mdash;\u0026mdash;-\nModifies NEMO() dataset in place. New variables added.\n Gridded.time_slice() def Gridded.time_slice(self, date0, date1):  \nReturn new Gridded object, indexed between dates date0 and date1\n Gridded.calculate_vertical_mask() def Gridded.calculate_vertical_mask(self, Zmax):  \nCalculates a 3D mask to a specified level Zmax. 1 for sea; 0 for below sea bed\nand linearly ramped for last level\n ","excerpt":"Objects Gridded()\nGridded._setup_grid_obj()\nGridded.make_lonLat_2d()\nGridded.set_grid_vars() …","ref":"/COAsT/docs/reference/gridded/","title":"Gridded"},{"body":"Objects setup_dask_client()\nIndexed()\nIndexed.apply_config_mappings()\nIndexed.insert_dataset()\nIndex class.\nsetup_dask_client() def setup_dask_client(workers=2, threads=2, memory_limit_per_worker=2GB):  \nNone\n Indexed() class Indexed(Coast): None Indexed.apply_config_mappings() def Indexed.apply_config_mappings(self):  \nApplies json configuration and mappings\n Indexed.insert_dataset() def Indexed.insert_dataset(self, dataset, apply_config_mappings=False):  \nInsert a dataset straight into this object instance\n ","excerpt":"Objects setup_dask_client()\nIndexed()\nIndexed.apply_config_mappings()\nIndexed.insert_dataset()\nIndex …","ref":"/COAsT/docs/reference/","title":"Index"},{"body":"Objects Lagrangian()\nLagrangian class\nLagrangian() class Lagrangian(Indexed): Parent class for subclasses OCEANPARCELS ... Common methods .... ","excerpt":"Objects Lagrangian()\nLagrangian class\nLagrangian() class Lagrangian(Indexed): Parent class for …","ref":"/COAsT/docs/reference/lagrangian/","title":"Lagrangian"},{"body":"Objects get_logger()\ncreate_handler()\nsetup_logging()\nget_slug()\nget_source()\nadd_info()\ndebug()\ninfo()\nwarning()\nwarn()\nerror()\nA logging unilty file\nget_logger() def get_logger(name=None, level=unknown):  \nNone\n create_handler() def create_handler(logger, stream=unknown, format_string=unknown):  \nNone\n setup_logging() def setup_logging(name=None, level=unknown, stream=unknown, format_string=unknown):  \nNone\n get_slug() def get_slug(obj):  \nNone\n get_source() def get_source(level=1):  \nNone\n add_info() def add_info(msg, level=3):  \nNone\n debug() def debug(msg, *args, **kwargs):  \nNone\n info() def info(msg, *args, **kwargs):  \nNone\n warning() def warning(msg, *args, **kwargs):  \nNone\n warn() def warn(msg, *args, **kwargs):  \nNone\n error() def error(msg, *args, **kwargs):  \nNone\n ","excerpt":"Objects get_logger()\ncreate_handler()\nsetup_logging()\nget_slug()\nget_source()\nadd_info()\ndebug() …","ref":"/COAsT/docs/reference/logging_util/","title":"Logging_util"},{"body":"Objects MaskMaker()\nMaskMaker.make_mask_dataset()\nMaskMaker.fill_polygon_by_index()\nMaskMaker.fill_polygon_by_lonlat()\nMaskMaker.region_def_nws_north_sea()\nMaskMaker.region_def_nws_outer_shelf()\nMaskMaker.region_def_nws_norwegian_trench()\nMaskMaker.region_def_nws_english_channel()\nMaskMaker.region_def_south_north_sea()\nMaskMaker.region_def_off_shelf()\nMaskMaker.region_def_irish_sea()\nMaskMaker.region_def_kattegat()\nMask maker\nMaskMaker() class MaskMaker(): None MaskMaker.make_mask_dataset() @staticmethod def MaskMaker.make_mask_dataset(longitude, latitude, mask_list):  \nNone\n MaskMaker.fill_polygon_by_index() @staticmethod def MaskMaker.fill_polygon_by_index(array_to_fill, vertices_r, vertices_c, fill_value=1, additive=False):  \nDraws and fills a polygon onto an existing numpy array based on array\nindices. To create a new mask, give np.zeros(shape) as input.\nPolygon vertices are drawn in the order given.\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\narray_to_fill (2D array): Array onto which to fill polygon\nvertices_r (1D array): Row indices for polygon vertices\nvertices_c (1D_array): Column indices for polygon vertices\nfill_value (float, bool or int): Fill value for polygon (Default: 1)\nadditive (bool): If true, add fill value to existing array. Otherwise\nindices will be overwritten. (Default: False)\nReturns\n\u0026mdash;\u0026mdash;-\nFilled 2D array\n MaskMaker.fill_polygon_by_lonlat() @staticmethod def MaskMaker.fill_polygon_by_lonlat(array_to_fill, longitude, latitude, vertices_lon, vertices_lat, fill_value=1, additive=False):  \nDraws and fills a polygon onto an existing numpy array based on\nvertices defined by longitude and latitude locations. This does NOT\ndraw a polygon on a sphere, but instead based on straight lines\nbetween points. This is OK for small regional areas, but not advisable\nfor large and global regions.\nPolygon vertices are drawn in the order given.\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\narray_to_fill (2D array): Array onto which to fill polygon\nvertices_r (1D array): Row indices for polygon vertices\nvertices_c (1D_array): Column indices for polygon vertices\nfill_value (float, bool or int): Fill value for polygon (Default: 1)\nadditive (bool): If true, add fill value to existing array. Otherwise\nindices will be overwritten. (Default: False)\nReturns\n\u0026mdash;\u0026mdash;-\nFilled 2D array\n MaskMaker.region_def_nws_north_sea() @classmethod def MaskMaker.region_def_nws_north_sea(cls, longitude, latitude, bath):  \nRegional definition for the North Sea (Northwest European Shelf)\nLongitude, latitude and bath should be 2D arrays corresponding to model\ncoordinates and bathymetry. Bath should be positive with depth.\n MaskMaker.region_def_nws_outer_shelf() @classmethod def MaskMaker.region_def_nws_outer_shelf(cls, longitude, latitude, bath):  \nRegional definition for the Outher Shelf (Northwest European Shelf)\nLongitude, latitude and bath should be 2D arrays corresponding to model\ncoordinates and bathymetry. Bath should be positive with depth.\n MaskMaker.region_def_nws_norwegian_trench() @classmethod def MaskMaker.region_def_nws_norwegian_trench(cls, longitude, latitude, bath):  \nRegional definition for the Norwegian Trench (Northwest European Shelf)\nLongitude, latitude and bath should be 2D arrays corresponding to model\ncoordinates and bathymetry. Bath should be positive with depth.\n MaskMaker.region_def_nws_english_channel() @classmethod def MaskMaker.region_def_nws_english_channel(cls, longitude, latitude, bath):  \nRegional definition for the English Channel (Northwest European Shelf)\nLongitude, latitude and bath should be 2D arrays corresponding to model\ncoordinates and bathymetry. Bath should be positive with depth.\n MaskMaker.region_def_south_north_sea() @classmethod def MaskMaker.region_def_south_north_sea(cls, longitude, latitude, bath):  \nNone\n MaskMaker.region_def_off_shelf() @classmethod def MaskMaker.region_def_off_shelf(cls, longitude, latitude, bath):  \nNone\n MaskMaker.region_def_irish_sea() @classmethod def MaskMaker.region_def_irish_sea(cls, longitude, latitude, bath):  \nNone\n MaskMaker.region_def_kattegat() @classmethod def MaskMaker.region_def_kattegat(cls, longitude, latitude, bath):  \nNone\n ","excerpt":"Objects MaskMaker()\nMaskMaker.make_mask_dataset()\nMaskMaker.fill_polygon_by_index() …","ref":"/COAsT/docs/reference/mask_maker/","title":"Mask_maker"},{"body":"Objects Oceanparcels()\nOceanparcels.load_single()\nOceanparcels class for reading ocean parcels data.\nOceanparcels() class Oceanparcels(Lagrangian): Reading ocean parcels data (netcdf format) into an xarray object. Oceanparcels.load_single() def Oceanparcels.load_single(self, file_path):  \nLoads a single file into object\u0026rsquo;s dataset variable.\nArgs:\nfile_path (str): path to data file\n ","excerpt":"Objects Oceanparcels()\nOceanparcels.load_single()\nOceanparcels class for reading ocean parcels data. …","ref":"/COAsT/docs/reference/oceanparcels/","title":"Oceanparcels"},{"body":"Objects OpendapInfo()\nOpendapInfo.get_store()\nOpendapInfo.open_dataset()\nOpendapInfo.from_cas()\nFunctionality for accessing OPeNDAP datasets.\nOpendapInfo() class OpendapInfo(): A class for accessing streamable OPeNDAP data. OpendapInfo.get_store() def OpendapInfo.get_store(self):  \nAccess an OPeNDAP data store.\nReturns:\nThe OPeNDAP data store accessed from the instance\u0026rsquo;s URL.\n OpendapInfo.open_dataset() def OpendapInfo.open_dataset(self, chunks=None):  \nOpen the remote XArray dataset for streaming.\nArgs:\nchunks: Chunks to use in Dask.\nReturns:\nThe opened XArray dataset.\n OpendapInfo.from_cas() @classmethod def OpendapInfo.from_cas(cls, url, cas_url, username, password):  \nInstantiate OpendapInfo with a session authenticated against CAS.\nArgs:\nurl: The OPeNDAP dataset URL.\ncas_url: The CAS login URL.\nusername: The username to authenticate with.\npassword: The password to authenticate with.\nReturns:\nThe instantiated OPeNDAP accessor.\n ","excerpt":"Objects OpendapInfo()\nOpendapInfo.get_store()\nOpendapInfo.open_dataset()\nOpendapInfo.from_cas() …","ref":"/COAsT/docs/reference/opendap/","title":"Opendap"},{"body":"Objects r2_lin()\nscatter_with_fit()\ncreate_geo_subplots()\ncreate_geo_axes()\nts_diagram()\ngeo_scatter()\ndetermine_colorbar_extension()\ndetermine_clim_by_standard_deviation()\nPython definitions used to help with plotting routines.\nMethods Overview -\u0026gt; geo_scatter(): Geographical scatter plot.\nr2_lin() def r2_lin(x, y, fit):  \nFor calculating r-squared of a linear fit. Fit should be a python polyfit object.\n scatter_with_fit() def scatter_with_fit(x, y, s=10, c=k, yex=True, dofit=True):  \nDoes a scatter plot with a linear fit. Will also draw y=x for\ncomparison.\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nx : (array) Values for the x-axis\ny : (array) Values for the y-axis\ns : (float or array) Marker size(s)\nc : (float or array) Marker colour(s)\nyex : (bool) True to plot y=x\ndofit : (bool) True to calculate and plot linear fit\nReturns\n\u0026mdash;\u0026mdash;-\nFigure and axis objects for further customisation\nExample Useage\n\u0026mdash;\u0026mdash;-\nx = np.arange(0,50)\ny = np.arange(0,50)/1.5\nf,a = scatter_with_fit(x,y)\na.set_title(\u0026lsquo;Example scatter with fit\u0026rsquo;)\na.set_xlabel(\u0026lsquo;Example x axis\u0026rsquo;)\na.set_ylabel(\u0026lsquo;Example y axis\u0026rsquo;)\n create_geo_subplots() def create_geo_subplots(lonbounds, latbounds, n_r=1, n_c=1, figsize=unknown):  \nA routine for creating an axis for any geographical plot. Within the\nspecified longitude and latitude bounds, a map will be drawn up using\ncartopy. Any type of matplotlib plot can then be added to this figure.\n For example:\nExample Useage\n#############\nf,a = create_geo_axes(lonbounds, latbounds)\nsca = a.scatter(stats.longitude, stats.latitude, c=stats.corr,\nvmin=.75, vmax=1,\nedgecolors='k\u0026rsquo;, linewidths=.5, zorder=100)\nf.colorbar(sca)\na.set_title(\u0026lsquo;SSH correlations Monthly PSMSL tide gauge vs CO9_AMM15p0\u0026rsquo;,\nfontsize=9)\n Note: For scatter plots, it is useful to set zorder = 100 (or similar\npositive number)\n   create_geo_axes() def create_geo_axes(lonbounds, latbounds):  \nA routine for creating an axis for any geographical plot. Within the\nspecified longitude and latitude bounds, a map will be drawn up using\ncartopy. Any type of matplotlib plot can then be added to this figure.\n For example:\nExample Useage\n#############\nf,a = create_geo_axes(lonbounds, latbounds)\nsca = a.scatter(stats.longitude, stats.latitude, c=stats.corr,\nvmin=.75, vmax=1,\nedgecolors='k\u0026rsquo;, linewidths=.5, zorder=100)\nf.colorbar(sca)\na.set_title(\u0026lsquo;SSH correlations Monthly PSMSL tide gauge vs CO9_AMM15p0\u0026rsquo;,\nfontsize=9)\n Note: For scatter plots, it is useful to set zorder = 100 (or similar\npositive number)\n   ts_diagram() def ts_diagram(temperature, salinity, depth):  \nNone\n geo_scatter() def geo_scatter(longitude, latitude, c=None, s=None, scatter_kwargs=None, coastline_kwargs=None, gridline_kwargs=None, figure_kwargs=unknown, title=, figsize=None):  \nUses CartoPy to create a geographical scatter plot with land boundaries.\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nlongitude : (array) Array of longitudes of marker locations\nlatitude : (array) Array of latitudes of marker locations\ncolors : (array) Array of values to use for colouring markers\ntitle : (str) Plot title, to appear at top of figure\nxlim : (tuple) Tuple of limits to apply to the x-axis (longitude axis)\nylim : (tuple) Limits to apply to the y-axis (latitude axis)\nReturns\n\u0026mdash;\u0026mdash;-\nFigure and axis objects for further customisation\n determine_colorbar_extension() def determine_colorbar_extension(color_data, vmin, vmax):  \nCan be used to automatically determine settings for colorbar\nextension arrows. Color_data is the data used for the colormap, vmin\nand vmax are the colorbar limits. Will output a string: \u0026ldquo;both\u0026rdquo;, \u0026ldquo;max\u0026rdquo;,\n\u0026ldquo;min\u0026rdquo; or \u0026ldquo;neither\u0026rdquo;, which can be inserted straight into a call to\nmatplotlib.pyplot.colorbar().\n determine_clim_by_standard_deviation() def determine_clim_by_standard_deviation(color_data, n_std_dev=2.5):  \nAutomatically determine color limits based on number of standard\ndeviations from the mean of the color data (color_data). Useful if there\nare outliers in the data causing difficulties in distinguishing most of\nthe data. Outputs vmin and vmax which can be passed to plotting routine\nor plt.clim().\n ","excerpt":"Objects r2_lin()\nscatter_with_fit()\ncreate_geo_subplots()\ncreate_geo_axes()\nts_diagram() …","ref":"/COAsT/docs/reference/plot_util/","title":"Plot_util"},{"body":"Objects Profile()\nProfile.read_en4()\nProfile.read_wod()\nProfile.subset_indices_lonlat_box()\nProfile.plot_profile()\nProfile.plot_map()\nProfile.plot_ts_diagram()\nProfile.process_en4()\nProfile.calculate_all_en4_qc_flags()\nProfile.obs_operator()\nProfile.reshape_2d()\nProfile.time_slice()\nProfile Class\nProfile() class Profile(Indexed): INDEXED type class for storing data from a CTD Profile (or similar down and up observations). The structure of the class is based around having discrete profile locations with independent depth dimensions and coords. The class dataset should contain two dimensions: \u0026gt; id_dim :: The profiles dimension. Each element of this dimension contains data (e.g. cast) for an individual location. \u0026gt; z_dim :: The dimension for depth levels. A profile object does not need to have shared depths, so NaNs might be used to pad any depth array. Alongside these dimensions, the following minimal coordinates should also be available: \u0026gt; longitude (id_dim) :: 1D array of longitudes, one for each id_dim \u0026gt; latitude (id_dim) :: 1D array of latitudes, one for each id_dim \u0026gt; time (id_dim) :: 1D array of times, one for each id_dim \u0026gt; depth (id_dim, z_dim) :: 2D array of depths, with different depth levels being provided for each profile. Note that these depth levels need to be stored in a 2D array, so NaNs can be used to pad out profiles with shallower depths. \u0026gt; id_name (id_dim) :: [Optional] Name of id_dim/case or id_dim number. You may create an empty profile object by using profile = coast.Profile(). You may then add your own dataset to the object profile or use one of the functions within Profile() for reading common profile datasets: \u0026gt; read_en4() \u0026gt; read_wod() Optionally, you may pass a dataset to the Profile object on creation: profile = coast.Profile(dataset = profile_dataset) A config file can also be provided, in which case any netcdf read functions will rename dimensions and variables as dictated. Profile.read_en4() def Profile.read_en4(self, fn_en4, chunks=unknown, multiple=False):  \nReads a single or multiple EN4 netCDF files into the COAsT profile\ndata structure.\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nfn_en4 : TYPE\npath to data file.\nchunks : dict, optional\nChunking specification\nmultiple : TYPE, optional\nTrue if reading multiple files otherwise False\nReturns\n\u0026mdash;\u0026mdash;-\nNone. Populates dataset within Profile object.\n Profile.read_wod() def Profile.read_wod(self, fn_wod, chunks=unknown):  \nReads a single World Ocean Database netCDF files into the COAsT profile data structure.\nArgs:\nfn_wod (str): path to data file\nchunks (dict): chunks\n Profile.subset_indices_lonlat_box() def Profile.subset_indices_lonlat_box(self, lonbounds, latbounds):  \nGet a subset of this Profile() object in a spatial box.\nlonbounds \u0026ndash; Array of form [min_longitude=-180, max_longitude=180]\nlatbounds \u0026ndash; Array of form [min_latitude, max_latitude]\nreturn: A new profile object containing subsetted data\n Profile.plot_profile() def Profile.plot_profile(self, var, profile_indices=None):  \nNone\n Profile.plot_map() def Profile.plot_map(self, var_str=None):  \nNone\n Profile.plot_ts_diagram() def Profile.plot_ts_diagram(self, profile_index, var_t=potential_temperature, var_s=practical_salinity):  \nNone\n Profile.process_en4() def Profile.process_en4(self, sort_time=True):  \nVERSION 1.4 (05/07/2021)\nPREPROCESSES EN4 data ready for comparison with model data.\nThis routine will cut out a desired geographical box of EN4 data and\nthen apply quality control according to the available flags in the\nnetCDF files. Quality control happens in two steps:\n1. Where a whole data profile is flagged, it is completely removed\nfrom the dataset\n2. Where a single datapoint is rejected in either temperature or\nsalinity, it is set to NaN.\nThis routine attempts to use xarray/dask chunking magic to keep\nmemory useage low however some memory is still needed for loading\nflags etc. May be slow if using large EN4 datasets.\nRoutine will return a processed profile object dataset and can write\nthe new dataset to file if fn_out is defined. If saving to the\nPROFILE object, be aware that DASK computations will not have happened\nand will need to be done using .load(), .compute() or similar before\naccessing the values. IF using multiple EN4 files or large dataset,\nmake sure you have chunked the data over N_PROF dimension.\nINPUTS\nfn_out (str) : Full path to a desired output file. If unspecified\nthen nothing is written.\nEXAMPLE USEAGE:\nprofile = coast.PROFILE()\nprofile.read_EN4(fn_en4, chunks={\u0026lsquo;N_PROF\u0026rsquo;:10000})\nfn_out = \u0026lsquo;~/output_file.nc'\nnew_profile = profile.preprocess_en4(fn_out = fn_out,\nlonbounds = [-10, 10],\nlatbounds = [45, 65])\n Profile.calculate_all_en4_qc_flags() @classmethod def Profile.calculate_all_en4_qc_flags(cls):  \nBrute force method for identifying all rejected points according to\nEN4 binary integers. It can be slow to convert large numbers of integers\nto a sequence of bits and is actually quicker to just generate every\ncombination of possible QC integers. That\u0026rsquo;s what this routine does.\nUsed in PROFILE.preprocess_en4().\nINPUTS\nNO INPUTS\nOUTPUTS\nqc_integers_tem : Array of integers signifying the rejection of ONLY\ntemperature datapoints\nqc_integers_sal : Array of integers signifying the rejection of ONLY\nsalinity datapoints\nqc_integers_both : Array of integers signifying the rejection of BOTH\ntemperature and salinity datapoints.\n Profile.obs_operator() def Profile.obs_operator(self, gridded, mask_bottom_level=True):  \nVERSION 2.0 (04/10/2021)\nAuthor: David Byrne\nDoes a spatial and time interpolation of a gridded object\u0026rsquo;s data.\nA nearest neighbour approach is used for both interpolations. Both\ndatasets (the Profile and Gridded objects) must contain longitude,\nlatitude and time coordinates. This routine expects there to be a\nlandmask variable in the gridded object. This is is not available,\nthen place an array of zeros into the dataset, with dimensions\n(y_dim, x_dim).\nThis routine will do the interpolation based on the chunking applied\nto the Gridded object. Please ensure you have the available memory to\nhave an entire Gridded chunk loaded to memory. If multiple files are\nused, then using one chunk per file will be most efficient. Time\nchunking is generally the better option for this routine.\nINPUTS:\ngridded (Gridded) : gridded object created on t-grid\nmask_bottom_level (bool) : Whether or not to mask any data below the\nmodel\u0026rsquo;s bottom level. If True, then ensure\nthe Gridded object\u0026rsquo;s dataset contain\u0026rsquo;s a\nbottom_level variable with dims\n(y_dim, x_dim).\nOUTPUTS:\nReturns a new PROFILE object containing a computed dataset of extracted\nprofiles.\n Profile.reshape_2d() def Profile.reshape_2d(self, var_user_want):  \nOBSERVATION type class for reshaping World Ocean Data (WOD) or similar that\ncontains 1D profiles (profile * depth levels) into a 2D array.\nNote that its variable has its own dimention and in some profiles\nonly some variables are present. WOD can be observed depth or a\nstandard depth as regrided by NOAA.\n Args:\n\u0026gt; X \u0026ndash; The variable (e.g,Temperatute, Salinity, Oxygen, DIC ..)\n\u0026gt; X_N \u0026ndash; Dimensions of observed variable as 1D\n(essentially number of obs variable = casts * osberved depths)\n\u0026gt; casts \u0026ndash; Dimension for locations of observations (ie. profiles)\n\u0026gt; z_N \u0026ndash; Dimension for depth levels of all observations as 1D\n(essentially number of depths = casts * osberved depths)\n\u0026gt; X_row_size \u0026ndash; Gives the vertical index (number of depths)\nfor each variable\n Profile.time_slice() def Profile.time_slice(self, date0, date1):  \nReturn new Gridded object, indexed between dates date0 and date1\n ","excerpt":"Objects Profile()\nProfile.read_en4()\nProfile.read_wod()\nProfile.subset_indices_lonlat_box() …","ref":"/COAsT/docs/reference/profile/","title":"Profile"},{"body":"Objects ProfileAnalysis()\nProfileAnalysis.depth_means()\nProfileAnalysis.bottom_means()\nProfileAnalysis.determine_mask_indices()\nProfileAnalysis.mask_means()\nProfileAnalysis.difference()\nProfileAnalysis.interpolate_vertical()\nProfileAnalysis.average_into_grid_boxes()\nProfile Class\nProfileAnalysis() class ProfileAnalysis(Indexed): A set of analysis routines suitable for datasets in a Profile object. See individual docstrings in each method for more info. ProfileAnalysis.depth_means() @classmethod def ProfileAnalysis.depth_means(cls, profile, depth_bounds):  \nCalculates a mean of all variable data that lie between two depths.\nReturns a new Profile() object containing the meaned data\nINPUTS:\ndataset (Dataset) : A dataset from a Profile object.\ndepth_bounds (Tuple) : A tuple of length 2 describing depth bounds\nShould be of form: (lower, upper) and in metres\n ProfileAnalysis.bottom_means() @classmethod def ProfileAnalysis.bottom_means(cls, profile, layer_thickness, depth_thresholds=unknown):  \nAverages profile data in some layer above the bathymetric depth. This\nroutine requires there to be a \u0026lsquo;bathymetry\u0026rsquo; variable in the Profile dataset.\nIt can apply a constant averaging layer thickness across all profiles\nor a bespoke thickness dependent on the bathymetric depth. For example,\nyou may want to define the \u0026lsquo;bottom\u0026rsquo; as the average of 100m above the\nbathymetry in very deep ocean but only 10m in the shallower ocean.\nIf there is no data available in the layer specified (e.g. CTD cast not\ndeep enough or model bathymetry wrong) then it will be NaN\nTo apply constant thickness, you only need to provide a value (in metre)\nfor layer_thickness. For different thicknesses, you also need to give\ndepth_thresholds. The last threshold must always be np.inf, i.e. all\ndata below a specific bathymetry depth.\nFor example, to apply 10m to everywhere \u0026lt;100m, 50m to 100m -\u0026gt; 500m and\n100m elsewhere, use:\nlayer_thickness = [10, 50, 100]\ndepth_thresholds = [100, 500, np.inf]\nThe bottom bound is always assumed to be 0.\n*NOTE: If time related issues arise, then remove any time variables\nfrom the profile dataset before running this routine.\nINPUTS:\nlayer_thickness (array) : A scalar layer thickness or list of values\ndepth_thresholds (array) : Optional. List of bathymetry thresholds.\nOUTPUTS:\nNew profile object containing bottom averaged data.\n ProfileAnalysis.determine_mask_indices() @classmethod def ProfileAnalysis.determine_mask_indices(cls, profile, mask_dataset):  \nDetermines whether each profile is within a mask (region) or not.\nThese masks should be in Dataset form, as returned by\nMask_maker().make_mask_dataset(). I.E, each mask\nshould be a 2D array with corresponding 2D longitude and latitude\narrays. Multiple masks should be stored along a dim_mask dimension.\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\ndataset : xarray.Dataset\nA dataset from a profile object\nmask_dataset : xarray.Dataset\nDataset with dimensions (dim_mask, x_dim, y_dim).\nShould contain longitude, latitude and mask. Mask has dimensions\n(dim_mask, y_dim, x_dim). Spatial dimensions should align with\nlongitude and latitude\nReturns\n\u0026mdash;\u0026mdash;-\nDataset describing which profiles are in which mask/region.\nReady for input to Profile.mask_means()\n ProfileAnalysis.mask_means() @classmethod def ProfileAnalysis.mask_means(cls, profile, mask_indices):  \nAverages all data inside a given profile dataset across a regional mask\nor for multiples regional masks.\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\ndataset : xarray.Dataset\nThe profile dataset to average.\nmask_indices : xarray.Dataset\nDescribes which profiles are in which region. Returned from\nprofile_analysis.determine_mask_indices().\nReturns\n\u0026mdash;\u0026mdash;-\nxarray.Dataset containing meaned data.\n ProfileAnalysis.difference() @classmethod def ProfileAnalysis.difference(cls, profile1, profile2, absolute_diff=True, square_diff=True):  \nCalculates differences between all matched variables in two Profile\ndatasets. Difference direction is dataset1 - dataset2.\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\ndataset1 : xarray.Dataset\nFirst profile dataset\ndataset2 : xarray.Dataset\nSecond profile dataset\nabsolute_diff : bool, optional\nWhether to calculate absolute differences. The default is True.\nsquare_diff : bool, optional\nWhether to calculate square differences. The default is True.\nReturns\n\u0026mdash;\u0026mdash;-\nNew Profile object containing differenced dataset.\nDifferences have suffix diff_\nAbsolute differences have suffix abs_\nSquare differences have suffic square_\n ProfileAnalysis.interpolate_vertical() @classmethod def ProfileAnalysis.interpolate_vertical(cls, profile, new_depth, interp_method=linear, print_progress=False):  \n(04/10/2021)\nAuthor: David Byrne\nFor vertical interpolation of all profiles within this object. User\nshould pass an array describing the new depths or another profile object\ncontaining the same number of profiles as this object.\nIf a 1D numpy array is passed then all profiles will be interpolated\nonto this single set of depths. If a xarray.DataArray is passed, it\nshould have dimensions (id_dim, z_dim) and contain a variable called\ndepth. This DataArray should contain the same number of profiles as\nthis object and will map profiles in order for interpolation. If\nanother profile object is passed, profiles will be mapped and\ninterpolated onto the other objects depth array.\nINPUTS:\nnew_depth (array or dataArray) : new depths onto which to interpolate\nsee description above for more info.\ninterp_method (str) : Any scipy interpolation string.\nOUTPUTS:\nReturns a new PROFILE object containing the interpolated dataset.\n ProfileAnalysis.average_into_grid_boxes() @classmethod def ProfileAnalysis.average_into_grid_boxes(cls, profile, grid_lon, grid_lat, min_datapoints=1, season=None, var_modifier=):  \nTakes the contents of this Profile() object and averages each variables\ninto geographical grid boxes. At the moment, this expects there to be\nno vertical dimension (z_dim), so make sure to slice the data out you\nwant first using isel, Profile.depth_means() or Profile.bottom_means().\nINPUTS\ngrid_lon (array) : 1d array of longitudes\ngrid_lat (array) : 1d array of latitude\nmin_datapoints (int) : Minimum N of datapoints at which to average\ninto box. Will return Nan in boxes with smaller N.\nNOTE this routine will also return the variable\ngrid_N, which tells you how many points were\naveraged into each box.\nseason (str) : \u0026lsquo;DJF\u0026rsquo;,\u0026lsquo;MAM\u0026rsquo;,\u0026lsquo;JJA\u0026rsquo; or \u0026lsquo;SON\u0026rsquo;. Will only average\ndata from specified season.\nvar_modifier (str) : Suffix to add to all averaged variables in the\noutput dataset. For example you may want to add\n_DJF to all vars if restricting only to winter.\nOUTPUTS\nCOAsT Gridded object containing averaged data.\n ","excerpt":"Objects ProfileAnalysis()\nProfileAnalysis.depth_means()\nProfileAnalysis.bottom_means() …","ref":"/COAsT/docs/reference/profile_analysis/","title":"Profile_analysis"},{"body":"Objects Module with attributes defining month ranges for the four seasons.\nUsed for convenience with Climatology.multiyear_averages(). Note: Summer is defined as JJAS, as opposed to the meteorological seasons of JJA.\n","excerpt":"Objects Module with attributes defining month ranges for the four seasons.\nUsed for convenience with …","ref":"/COAsT/docs/reference/seasons/","title":"Seasons"},{"body":"Objects quadratic_spline_roots()\nfind_maxima()\ndoodson_x0_filter()\nPython definitions used to aid with statistical calculations.\nMethods Overview -\u0026gt; normal_distribution(): Create values for a normal distribution -\u0026gt; cumulative_distribution(): Integration udner a PDF -\u0026gt; empirical_distribution(): Estimates CDF empirically\nquadratic_spline_roots() def quadratic_spline_roots(spl):  \nA custom function for the roots of a quadratic spline. Cleverness found at\nhttps://stackoverflow.com/questions/50371298/find-maximum-minimum-of-a-1d-interpolated-function\nUsed in find_maxima().\nExample usage:\nsee example_scripts/tidegauge_tutorial.py\n find_maxima() def find_maxima(x, y, method=comp, **kwargs):  \nFinds maxima of a time series y. Returns maximum values of y (e.g heights)\nand corresponding values of x (e.g. times).\n**kwargs are dependent on method.\n Methods:\n\u0026lsquo;comp\u0026rsquo; :: Find maxima by comparison with neighbouring values.\nUses scipy.signal.find_peaks. **kwargs passed to this routine\nwill be passed to scipy.signal.find_peaks.\n\u0026lsquo;cubic\u0026rsquo; :: Find the maxima and minima by fitting a cubic spline and\nfinding the roots of its derivative.\nExpect input as xr.DataArrays\nDB NOTE: Currently only the \u0026lsquo;comp\u0026rsquo; and \u0026lsquo;cubic\u0026rsquo; method are implemented.\nFuture methods include linear interpolation.\n JP NOTE: Cubic method:\ni) has intelligent fix for NaNs,\nExample usage:\nsee example_scripts/tidegauge_tutorial.py\n doodson_x0_filter() def doodson_x0_filter(elevation, ax=0):  \nThe Doodson X0 filter is a simple filter designed to damp out the main\ntidal frequencies. It takes hourly values, 19 values either side of the\ncentral one and applies a weighted average using:\n(1010010110201102112 0 2112011020110100101)/30.\n( http://www.ntslf.org/files/acclaimdata/gloup/doodson_X0.html )\nIn \u0026ldquo;Data Analaysis and Methods in Oceanography\u0026rdquo;:\n\u0026ldquo;The cosine-Lanczos filter, the transform filter, and the\nButterworth filter are often preferred to the Godin filter,\nto earlier Doodson filter, because of their superior ability\nto remove tidal period variability from oceanic signals.\u0026ldquo;\nThis routine can be used for any dimension input array.\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nelevation (ndarray) : Array of hourly elevation values.\naxis (int) : Time axis of input array. This axis must have \u0026gt;= 39\nelements\nReturns\n\u0026mdash;\u0026mdash;-\nFiltered array of same rank as elevation.\n ","excerpt":"Objects quadratic_spline_roots()\nfind_maxima()\ndoodson_x0_filter()\nPython definitions used to aid …","ref":"/COAsT/docs/reference/stats_util/","title":"Stats_util"},{"body":"Objects Tidegauge()\nTidegauge.read_gesla_v3()\nTidegauge.read_gesla()\nTidegauge._read_gesla_header_v5()\nTidegauge._read_gesla_header_v3()\nTidegauge._read_gesla_data()\nTidegauge.read_hlw()\nTidegauge._read_hlw_header()\nTidegauge._read_hlw_data()\nTidegauge.show()\nTidegauge.get_tide_table_times()\nTidegauge.read_ea_api_to_xarray()\nTidegauge.read_bodc()\nTidegauge._read_bodc_header()\nTidegauge._read_bodc_data()\nTidegauge.plot_timeseries()\nTidegauge.plot_on_map()\nTidegauge.plot_on_map_multiple()\nTidegauge.obs_operator()\nTidegauge.time_slice()\nTidegauge.subset_indices_lonlat_box()\nTide Gauge class\nTidegauge() class Tidegauge(Timeseries): This is an object for storage and manipulation of tide gauge data in a single dataset. This may require some processing of the observations such as interpolation to a common time step. This object's dataset should take the form (as with Timeseries): Dimensions: id_dim : The locations dimension. Each time series has an index time : The time dimension. Each datapoint at each port has an index Coordinates: longitude (id_dim) : Longitude values for each port index latitude (id_dim) : Latitude values for each port index time (time) : Time values for each time index (datetime) id_name (id_dim) : Name of index, e.g. port name or mooring id. An example data variable could be ssh, or ntr (non-tidal residual). This object can also be used for other instrument types, not just tide gauges. For example moorings. Every id index for this object should use the same time coordinates. Therefore, timeseries need to be aligned before being placed into the object. If there is any padding needed, then NaNs should be used. NaNs should also be used for quality control/data rejection. Tidegauge.read_gesla_v3() def Tidegauge.read_gesla_v3(self, fn_gesla, date_start=None, date_end=None):  \nDepreciated method.\nCall generalised method.\nReturns eiter a tidegauge object or a list of tidegauge objects\n Tidegauge.read_gesla() def Tidegauge.read_gesla(self, fn_gesla, date_start=None, date_end=None, format=v3):  \nFor reading from a GESLA2 (Format version 3.0) or GESLA3 (Format v5.0)\nfile(s) into an xarray dataset.\nv3 formatting according to Woodworth et al. (2017).\nv5 formatting \u0026hellip;.\nWebsite: https://www.gesla.org/\nIf no data lies between the specified dates, a dataset is still created\ncontaining information on the tide gauge, but the time dimension will\nbe empty.\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nfn_gesla (str) : path to gesla tide gauge file, list of files or a glob\ndate_start (datetime) : start date for returning data\ndate_end (datetime) : end date for returning data\nformat (str) : accepts \u0026ldquo;v3\u0026rdquo; or \u0026ldquo;v5\u0026quot;\nReturns\n\u0026mdash;\u0026mdash;-\nCreates xarray.dataset within tidegauge object containing loaded data.\nIf multiple files are provided then instead returns a list of NEW\ntidegauge objects.\n Tidegauge._read_gesla_header_v5() @classmethod def Tidegauge._read_gesla_header_v5(cls, fn_gesla):  \nReads header from a GESLA file (format version 5.0).\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nfn_gesla (str) : path to gesla tide gauge file\nReturns\n\u0026mdash;\u0026mdash;-\ndictionary of attributes\n Tidegauge._read_gesla_header_v3() @classmethod def Tidegauge._read_gesla_header_v3(cls, fn_gesla):  \nReads header from a GESLA file (format version 3.0).\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nfn_gesla (str) : path to gesla tide gauge file\nReturns\n\u0026mdash;\u0026mdash;-\ndictionary of attributes\n Tidegauge._read_gesla_data() @classmethod def Tidegauge._read_gesla_data(cls, fn_gesla, date_start=None, date_end=None, header_length=32):  \nReads observation data from a GESLA file (format version 3.0 and 5.0).\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nfn_gesla (str) : path to gesla tide gauge file\ndate_start (datetime) : start date for returning data\ndate_end (datetime) : end date for returning data\nheader_length (int) : number of lines in header (to skip when reading)\nReturns\n\u0026mdash;\u0026mdash;-\nxarray.Dataset containing times, sealevel and quality control flags\n Tidegauge.read_hlw() def Tidegauge.read_hlw(self, fn_hlw, date_start=None, date_end=None):  \nFor reading from a file of tidetable High and Low Waters (HLW) data into an\nxarray dataset. File contains high water and low water heights and times\nIf no data lies between the specified dates, a dataset is still created\ncontaining information on the tide gauge, but the time dimension will\nbe empty.\nThe data takes the form:\nLIVERPOOL (GLADSTONE DOCK) TZ: UT(GMT)/BST Units: METRES Datum: Chart Datum\n01/10/2020 06:29 1.65\n01/10/2020 11:54 9.01\n01/10/2020 18:36 1.87\n\u0026hellip;\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nfn_hlw (str) : path to tabulated High Low Water file\ndate_start (datetime) : start date for returning data\ndate_end (datetime) : end date for returning data\nReturns\n\u0026mdash;\u0026mdash;-\nxarray.Dataset object.\n Tidegauge._read_hlw_header() @classmethod def Tidegauge._read_hlw_header(cls, filnam):  \nReads header from a HWL file.\nThe data takes the form:\nLIVERPOOL (GLADSTONE DOCK) TZ: UT(GMT)/BST Units: METRES Datum: Chart Datum\n01/10/2020 06:29 1.65\n01/10/2020 11:54 9.01\n01/10/2020 18:36 1.87\n\u0026hellip;\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nfilnam (str) : path to file\nReturns\n\u0026mdash;\u0026mdash;-\ndictionary of attributes\n Tidegauge._read_hlw_data() @classmethod def Tidegauge._read_hlw_data(cls, filnam, header_dict, date_start=None, date_end=None, header_length=1):  \nReads HLW data from a tidetable file.\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nfilnam (str) : path to HLW tide gauge file\ndate_start (np.datetime64) : start date for returning data.\ndate_end (np.datetime64) : end date for returning data.\nheader_length (int) : number of lines in header (to skip when reading)\nReturns\n\u0026mdash;\u0026mdash;-\nxarray.Dataset containing times, High and Low water values\n Tidegauge.show() def Tidegauge.show(self, timezone=None):  \nPrint out the values in the xarray\nDisplays with specified timezone\n Tidegauge.get_tide_table_times() def Tidegauge.get_tide_table_times(self, time_guess=None, time_var=time, measure_var=ssh, method=window, winsize=None):  \nGet tide times and heights from tide table.\ninput:\ntime_guess : np.datetime64 or datetime\nassumes utc\ntime_var : name of time variable [default: \u0026lsquo;time\u0026rsquo;]\nmeasure_var : name of ssh variable [default: \u0026lsquo;ssh\u0026rsquo;]\nmethod =\nwindow: +/- hours window size, winsize, (int) return values in that window\nuses additional variable winsize (int) [default 2hrs]\nnearest_1: return only the nearest event, if in winsize [default:None]\nnearest_2: return nearest event in future and the nearest in the past (i.e. high and a low), if in winsize [default:None]\nnearest_HW: return nearest High Water event (computed as the max of nearest_2), if in winsize [default:None]\nreturns: xr.DataArray( measure_var, coords=time_var)\nE.g. ssh (m), time (utc)\nIf value is not found, it returns a NaN with time value as the\nguess value.\n Tidegauge.read_ea_api_to_xarray() @classmethod def Tidegauge.read_ea_api_to_xarray(cls, n_days=5, date_start=None, date_end=None, station_id=E70124):  \nload gauge data via environment.data.gov.uk EA API\nEither loads last n_days, or from date_start:date_end\nAPI Source:\nhttps://environment.data.gov.uk/flood-monitoring/doc/reference\nDetails of available tidal stations are recovered with:\nhttps://environment.data.gov.uk/flood-monitoring/id/stations?type=TideGauge\nRecover the \u0026ldquo;stationReference\u0026rdquo; for the gauge of interest and pass as\nstation_id:str. The default station_id=\u0026quot;E70124\u0026rdquo; is Liverpool.\nINPUTS:\nn_days : int. Extact the last n_days from now.\ndate_start : datetime. UTC format string \u0026ldquo;yyyy-MM-dd\u0026rdquo; E.g 2020-01-05\ndate_end : datetime\nstation_id : int. Station id. Also referred to as stationReference in\nEA API. Default value is for Liverpool.\nOUTPUT:\nssh, time : xr.Dataset\n Tidegauge.read_bodc() def Tidegauge.read_bodc(self, fn_bodc, date_start=None, date_end=None):  \nFor reading from a single BODC (processed) file into an\nxarray dataset.\nIf no data lies between the specified dates, a dataset is still created\ncontaining information on the tide gauge, but the time dimension will\nbe empty.\nData name: UK Tide Gauge Network, processed data.\nSource: https://www.bodc.ac.uk/\nSee data notes from source for description of QC flags.\nThe data takes the form:\nPort: P234\nSite: Liverpool, Gladstone Dock\nLatitude: 53.44969\nLongitude: -3.01800\nStart Date: 01AUG2020-00.00.00\nEnd Date: 31AUG2020-23.45.00\nContributor: National Oceanography Centre, Liverpool\nDatum information: The data refer to Admiralty Chart Datum (ACD)\nParameter code: ASLVBG02 = Surface elevation (unspecified datum)\nof the water body by bubbler tide gauge (second sensor)\nCycle Date Time ASLVBG02 Residual\nNumber yyyy mm dd hh mi ssf f f\n1) 2020/08/01 00:00:00 5.354M 0.265M\n2) 2020/08/01 00:15:00 5.016M 0.243M\n3) 2020/08/01 00:30:00 4.704M 0.241M\n4) 2020/08/01 00:45:00 4.418M 0.255M\n5) 2020/08/01 01:00:00 4.133 0.257\n\u0026hellip;\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nfn_bodc (str) : path to bodc tide gauge file\ndate_start (datetime) : start date for returning data\ndate_end (datetime) : end date for returning data\nReturns\n\u0026mdash;\u0026mdash;-\nxarray.Dataset object.\n Tidegauge._read_bodc_header() @staticmethod def Tidegauge._read_bodc_header(fn_bodc):  \nReads header from a BODC file (format version 3.0).\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nfn_bodc (str) : path to bodc tide gauge file\nReturns\n\u0026mdash;\u0026mdash;-\ndictionary of attributes\n Tidegauge._read_bodc_data() @staticmethod def Tidegauge._read_bodc_data(fn_bodc, date_start=None, date_end=None, header_length=11):  \nReads observation data from a BODC file.\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nfn_bodc (str) : path to bodc tide gauge file\ndate_start (datetime) : start date for returning data\ndate_end (datetime) : end date for returning data\nheader_length (int) : number of lines in header (to skip when reading)\nReturns\n\u0026mdash;\u0026mdash;-\nxarray.Dataset containing times, sealevel and quality control flags\n Tidegauge.plot_timeseries() def Tidegauge.plot_timeseries(self, id, var_list=unknown, date_start=None, date_end=None, plot_line=False):  \nQuick plot of time series stored within object\u0026rsquo;s dataset\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\ndate_start (datetime) : Start date for plotting\ndate_end (datetime) : End date for plotting\nvar_list (str) : List of variables to plot. Default: just ssh\nplot_line (bool) : If true, draw line between markers\nReturns\n\u0026mdash;\u0026mdash;-\nmatplotlib figure and axes objects\n Tidegauge.plot_on_map() def Tidegauge.plot_on_map(self):  \nShow the location of a tidegauge on a map.\nExample usage:\n\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;\nFor a TIDEGAUGE object tg\n tg.plot_map()\n Tidegauge.plot_on_map_multiple() @classmethod def Tidegauge.plot_on_map_multiple(cls, tidegauge_list, color_var_str=None):  \nShow the location of a tidegauge on a map.\nExample usage:\n\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;\nFor a TIDEGAUGE object tg\n tg.plot_map()\n Tidegauge.obs_operator() def Tidegauge.obs_operator(self, gridded, time_interp=nearest):  \nRegrids a Gridded object onto a tidegauge_multiple object. A nearest\nneighbour interpolation is done for spatial interpolation and time\ninterpolation can be specified using the time_interp argument. This\ntakes any scipy interpolation string. If Gridded object contains a\nlandmask variables, then the nearest WET point is taken for each tide\ngauge.\nOutput is a new tidegauge_multiple object containing interpolated data.\n Tidegauge.time_slice() def Tidegauge.time_slice(self, date0, date1):  \nReturn new Gridded object, indexed between dates date0 and date1\n Tidegauge.subset_indices_lonlat_box() def Tidegauge.subset_indices_lonlat_box(self, lonbounds, latbounds):  \nGet a subset of this Profile() object in a spatial box.\nlonbounds \u0026ndash; Array of form [min_longitude=-180, max_longitude=180]\nlatbounds \u0026ndash; Array of form [min_latitude, max_latitude]\nreturn: A new profile object containing subsetted data\n ","excerpt":"Objects Tidegauge()\nTidegauge.read_gesla_v3()\nTidegauge.read_gesla() …","ref":"/COAsT/docs/reference/tidegauge/","title":"Tidegauge"},{"body":"Objects TidegaugeAnalysis()\nTidegaugeAnalysis.match_missing_values()\nTidegaugeAnalysis.harmonic_analysis_utide()\nTidegaugeAnalysis.reconstruct_tide_utide()\nTidegaugeAnalysis.calculate_non_tidal_residuals()\nTidegaugeAnalysis.threshold_statistics()\nTidegaugeAnalysis.demean_timeseries()\nTidegaugeAnalysis.difference()\nTidegaugeAnalysis.find_high_and_low_water()\nTidegaugeAnalysis.doodson_x0_filter()\nTidegaugeAnalysis.crps()\nTidegaugeAnalysis.time_mean()\nTidegaugeAnalysis.time_std()\nTidegaugeAnalysis.time_slice()\nTidegaugeAnalysis.resample_mean()\nAn analysis class for tide gauge.\nTidegaugeAnalysis() class TidegaugeAnalysis(): This contains analysis methods suitable for use with the dataset structure of Tidegauge() TidegaugeAnalysis.match_missing_values() @classmethod def TidegaugeAnalysis.match_missing_values(cls, data_array1, data_array2, fill_value=unknown):  \nWill match any missing values between two tidegauge_multiple datasets.\nWhere missing values (defined by fill_value) are found in either dataset\nthey are also placed in the corresponding location in the other dataset.\nReturns two new tidegaugeMultiple objects containing the new\nssh data. Datasets must contain ssh variables and only ssh will be\nmasked.\n TidegaugeAnalysis.harmonic_analysis_utide() @classmethod def TidegaugeAnalysis.harmonic_analysis_utide(cls, data_array, min_datapoints=1000, nodal=False, trend=False, method=ols, conf_int=linear, Rayleigh_min=0.95):  \nDoes a harmonic analysis for each timeseries inside this object using\nthe utide library. All arguments except min_datapoints are arguments\nthat are passed to ut.solve(). Please see the utide website for more\ninformation:\nhttps://pypi.org/project/UTide/\nUtide will by default do it\u0026rsquo;s harmonic analysis using a set of harmonics\ndetermined using the Rayleigh criterion. This changes the number of\nharmonics depending on the length and frequency of the time series.\nOutput from this routine is not a new dataset, but a list of utide\nanalysis object. These are structures containing, amongst other things,\namplitudes, phases, constituent names and confidence intervals. This\nlist can be passed to reconstruct_tide_utide() in this object to create\na new TidegaugeMultiple object containing reconstructed tide data.\nINPUTS\ndata_array : Xarray data_array from a coast.Tidegauge() object\ne.g. tidegauge.dataset.ssh\nmin_datapoints : If a time series has less than this value number of\ndatapoints, then omit from the analysis.\n\u0026lt;all_others\u0026gt; : Inputs to utide.solve(). See website above.\nOUTPUTS\nA list of utide structures from the solve() routine. If a location\nis omitted, it will contain [] for it\u0026rsquo;s entry.\n TidegaugeAnalysis.reconstruct_tide_utide() @classmethod def TidegaugeAnalysis.reconstruct_tide_utide(cls, data_array, utide_solution_list, constit=None, output_name=reconstructed):  \nUse the tarray of times to reconstruct a time series series using a\nlist of utide analysis objects. This list can be obtained\nusing harmonic_analysis_utide(). Specify constituents to use in the\nreconstruction by passing a list of strings such as \u0026lsquo;M2\u0026rsquo; to the constit\nargument. This won\u0026rsquo;t work if a specified constituent is not present in\nthe analysis.\n TidegaugeAnalysis.calculate_non_tidal_residuals() @classmethod def TidegaugeAnalysis.calculate_non_tidal_residuals(cls, data_array_ssh, data_array_tide, apply_filter=True, window_length=25, polyorder=3):  \nCalculate non tidal residuals by subtracting values in data_array_tide\nfrom data_array_ssh. You may optionally apply a filter to the non\ntidal residual data by setting apply_filter = True. This uses the\nscipy.signal.savgol_filter function, which you ay pass window_length\nand poly_order.\n TidegaugeAnalysis.threshold_statistics() @classmethod def TidegaugeAnalysis.threshold_statistics(cls, dataset, thresholds=unknown, peak_separation=12):  \nDo some threshold statistics for all variables with a time dimension\ninside this tidegauge_multiple object. Specifically, this routine will\ncalculate:\npeak_count : The number of indepedent peaks over\neach specified threshold. Independent peaks\nare defined using the peak_separation\nargument. This is the number of datapoints\neither side of a peak within which data\nis ommited for further peak search.\ntime_over_threshold : The total time spent over each threshold\nThis is NOT an integral, but simple a count\nof all points over threshold.\ndailymax_count : A count of the number of daily maxima over\neach threshold\nmonthlymax_count : A count of the number of monthly maxima\nover each threshold.\nOutput is a xarray dataset containing analysed variables. The name of\neach analysis variable is constructed using the original variable name\nand one of the above analysis categories.\n TidegaugeAnalysis.demean_timeseries() @staticmethod def TidegaugeAnalysis.demean_timeseries(dataset):  \nSubtract time means from all variables within this tidegauge_multiple\nobject. This is done independently for each id_dim location.\n TidegaugeAnalysis.difference() @classmethod def TidegaugeAnalysis.difference(cls, dataset1, dataset2, absolute_diff=True, square_diff=True):  \nCalculates differences between two tide gauge objects datasets. Will calculate\ndifferences, absolute differences and square differences between all\ncommon variables within each object. Each object should have the same\nsized dimensions. When calling this routine, the differencing is done\nas follows:\ndataset1.difference(dataset2)\nThis will do dataset1 - dataset2.\nOutput is a new tidegauge object containing differenced variables.\n TidegaugeAnalysis.find_high_and_low_water() @staticmethod def TidegaugeAnalysis.find_high_and_low_water(data_array, method=comp, **kwargs):  \nFinds high and low water for a given variable.\nReturns in a new TIDEGAUGE object with similar data format to\na TIDETABLE. If this Tidegauge object contains more than one location\n(id_dim \u0026gt; 1) then a list of Tidegauges will be returned.\nMethods:\n\u0026lsquo;comp\u0026rsquo; :: Find maxima by comparison with neighbouring values.\nUses scipy.signal.find_peaks. **kwargs passed to this routine\nwill be passed to scipy.signal.find_peaks.\n\u0026lsquo;cubic\u0026rsquo;:: Find the maxima using the roots of cubic spline.\nUses scipy.interpolate.InterpolatedUnivariateSpline\nand scipy.signal.argrelmax. **kwargs are not activated.\nNOTE: Currently only the \u0026lsquo;comp\u0026rsquo; and \u0026lsquo;cubic\u0026rsquo; methods implemented. Future\nmethods include linear interpolation or refinements.\n TidegaugeAnalysis.doodson_x0_filter() @staticmethod def TidegaugeAnalysis.doodson_x0_filter(dataset, var_str):  \nApplies doodson X0 filter to a specified TIDEGAUGE variable\nInput ius expected to be hourly. Use resample_mean to average data\nto hourly frequency.\n TidegaugeAnalysis.crps() @classmethod def TidegaugeAnalysis.crps(cls, tidegauge_data, gridded_data, nh_radius=20, time_interp=linear):  \nComparison of observed variable to modelled using the Continuous\nRanked Probability Score. This is done using this TIDEGAUGE object.\nThis method specifically performs a single-observation neighbourhood-\nforecast method.\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\nmodel_object (model) : Model object (NEMO) containing model data\nmodel_var_name (str) : Name of model variable to compare.\nobs_var_name (str) : Name of observed variable to compare.\nnh_radius (float) : Neighbourhood rad\ncdf_type (str) : Type of cumulative distribution to use for the\nmodel data (\u0026lsquo;empirical\u0026rsquo; or \u0026lsquo;theoretical\u0026rsquo;).\nObservations always use empirical.\ntime_interp (str) : Type of time interpolation to use (s)\ncreate_new_obj (bool): If True, save output to new TIDEGAUGE obj.\nOtherwise, save to this obj.\nReturns\n\u0026mdash;\u0026mdash;-\nxarray.Dataset containing times, sealevel and quality control flags\nExample Useage\n\u0026mdash;\u0026mdash;-\nCompare modelled \u0026lsquo;sossheig\u0026rsquo; with \u0026lsquo;ssh\u0026rsquo; using CRPS\n crps = altimetry.crps(nemo, \u0026lsquo;sossheig\u0026rsquo;, \u0026lsquo;ssh\u0026rsquo;)\n TidegaugeAnalysis.time_mean() @classmethod def TidegaugeAnalysis.time_mean(cls, dataset, date0=None, date1=None):  \nTime mean of all variables between dates date0, date1\n TidegaugeAnalysis.time_std() @classmethod def TidegaugeAnalysis.time_std(cls, dataset, date0=None, date1=None):  \nTime st. dev of variable var_str between dates date0 and date1\n TidegaugeAnalysis.time_slice() @classmethod def TidegaugeAnalysis.time_slice(cls, dataset, date0=None, date1=None):  \nNone\n TidegaugeAnalysis.resample_mean() @classmethod def TidegaugeAnalysis.resample_mean(cls, dataset, time_freq, **kwargs):  \nResample a TIDEGAUGE variable in time by calculating the mean\nof all data points at a given frequency.\nParameters\n\u0026mdash;\u0026mdash;\u0026mdash;-\ntime_freq (str) : Time frequency. e.g. \u0026lsquo;1H\u0026rsquo; for hourly, \u0026lsquo;1D\u0026rsquo; for daily\nCan also be a timedelta object. See Pandas resample\nmethod for more info.\n**kwargs (other) : Other arguments to pass to xarray.Dataset.resample\n(http://xarray.pydata.org/en/stable/generated/xarray.Dataset.resample.html)\nReturns\n\u0026mdash;\u0026mdash;-\nNew Tidegauge() object containing resampled data\n ","excerpt":"Objects TidegaugeAnalysis()\nTidegaugeAnalysis.match_missing_values() …","ref":"/COAsT/docs/reference/tidegauge_analysis/","title":"Tidegauge_analysis"},{"body":"Objects Timeseries()\nTimeseries Class\nTimeseries() class Timeseries(Indexed): Parent class for Tidegauge and other timeseries type datasets Common methods ... ","excerpt":"Objects Timeseries()\nTimeseries Class\nTimeseries() class Timeseries(Indexed): Parent class for …","ref":"/COAsT/docs/reference/timeseries/","title":"Timeseries"},{"body":"Objects Track()\nTrack class\nTrack() class Track(Indexed): Parent class for subclasses Altimetry Common methods .... ","excerpt":"Objects Track()\nTrack class\nTrack() class Track(Indexed): Parent class for subclasses Altimetry …","ref":"/COAsT/docs/reference/track/","title":"Track"},{"body":"Objects xesmf_convert()\nxesmf_convert._get_xesmf_datasets()\nxesmf_convert.to_gridded()\nA class to convert from coast gridded to xesmf.\nxesmf_convert() class xesmf_convert(): Converts the main dataset within a COAsT.Gridded object to be suitable for input to XESMF for regridding to either a curvilinear or rectilienar grid. All you need to do if provide a Gridded object and a grid type when creating a new instance of this class. It will then contain an appropriate input dataset. You may also provide a second COAsT gridded object if regridding between two objects. For using xesmf, please see the package's documentation website here: https://xesmf.readthedocs.io/en/latest/index.html You can install XESMF using: conda install -c conda-forge xesmf. The setup used by this class has been tested for xesmf v0.6.2 alongside esmpy v8.0.0. It was installed using: conda install -c conda-forge xesmf esmpy=8.0.0 INPUTS input_gridded_obj (Gridded) :: Gridded object to be regridded output_gridded_obj(Gridded) :: (optional) Gridded object to regrid TO reorder_dims (bool) :: Xesmf requires that lat/lon dimensions are the last dimensions. If this is True, then will attempt to reorder dimensions. Not recommended for large datasets. [Default = False] \u0026gt;\u0026gt;\u0026gt; EXAMPLE USEAGE \u0026lt;\u0026lt;\u0026lt; If regridding a Gridded object to an arbitrarily defined rectilinear or curvilinear grid, you just need to do the following: import xesmf as xe # Create your gridded object gridded = coast.Gridded(*args, **kwargs) # Pass the gridded object over to xesmf_convert xesmf_ready = coast.xesmf_convert(gridded, input_grid_type = 'curvilinear') # Now this object will contain a dataset called xesmf_input, which can # be passed over to xesmf. E.G: destination_grid = xesmf.util.grid_2d(-15, 15, 1, 45, 65, 1) regridder = xe.Regridder(xesmf_ready.input_grid, destination_grid, \u0026quot;bilinear\u0026quot;) regridded_dataset = regridder(xesmf_ready.input_data) XESMF contains a couple of difference functions for quickly creating output grids, such as xesmf.util.grid_2d and xesmf.util.grid_global(). See their website for more info. The process is almost the same if regridding from one COAsT.Gridded object to another (gridded0 -\u0026gt; gridded1): xesmf_ready = coast.xesmf_convert(gridded0, gridded1, input_grid_type = \u0026quot;curvilinear\u0026quot;, output_grid_type = \u0026quot;curvilinear\u0026quot;) regridder = xe.Regridder(xesmf_ready.input_grid, xesmf_ready.output_grid, \u0026quot;bilinear\u0026quot;) regridded_dataset = regridder(xesmf_ready.input_data) Note that you can select which variables you want to regrid, either prior to using this tool or by indexing the input_data dataset. e.g.: regridded_dataset = regridder(xesmf_ready.input_data['temperature']) If your input datasets were lazy loaded, then so will the regridded dataset. At this point you can either load the data or (recomended) save the regridded data to file: regridded_dataset.to_netcdf(\u0026lt;filename_to_save\u0026gt;) Before saving back to file, call xesmf_ready.to_gridded() to convert the regridded xesmf object back to a gridded object xesmf_convert._get_xesmf_datasets() @classmethod def xesmf_convert._get_xesmf_datasets(cls, dataset, grid_type, reorder_dims=False):  \nFor a given dataset taken from a Gridded object and a grid_type\n(curvilinear or rectilinear), determine the xesmf formatted dataset.\nThis method does some checks to make sure the dataset is suitable and\nrenames the relevant dimensions/coordinates. Any vars that don\u0026rsquo;t have\nboth x_dim and y_dim will be dropped. If x_dim and y_dim are present\nBUT they are not the last dimensions AND reorder_dims=True then\nthe dimensions will be reordered (not good for lazy loading/chunking).\n xesmf_convert.to_gridded() @staticmethod def xesmf_convert.to_gridded(xesmf_dataset):  \nConverts an xesmf_dataset back to a Coast.Gridded() object. Returns\na Gridded object.\n ","excerpt":"Objects xesmf_convert()\nxesmf_convert._get_xesmf_datasets()\nxesmf_convert.to_gridded()\nA class to …","ref":"/COAsT/docs/reference/xesmf_convert/","title":"Xesmf_convert"},{"body":"  #td-cover-block-0 { background-image: url(/COAsT/about/featured-background_hu14d69772da4446f8c45afbc4cad362c8_132726_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/COAsT/about/featured-background_hu14d69772da4446f8c45afbc4cad362c8_132726_1920x1080_fill_q75_catmullrom_top.jpg); } }  About COAsT A site using the Docsy Hugo theme. --        COAsT is a Python package for managing and analysing high resolution NEMO output Read more here     This site was based off the Docsy Hugo theme.    ","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/COAsT/about/","title":"About Goldydocs"},{"body":"  #td-cover-block-0 { background-image: url(/COAsT/featured-background_hu14d69772da4446f8c45afbc4cad362c8_132726_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/COAsT/featured-background_hu14d69772da4446f8c45afbc4cad362c8_132726_1920x1080_fill_q75_catmullrom_top.jpg); } }  Welcome to the documentation: A Docsy site for COAsT Learn More   Download   COAsT\n\n        This site provides visibility into the COAsT python framework.       Download from Anaconda.org Get the COAsT framework!\nRead more …\n   Contributions welcome! We do a Pull Request contributions workflow on GitHub. New users are always welcome!\nRead more …\n   Follow us on Twitter! For announcement of latest features etc.\nRead more …\n    ","excerpt":"#td-cover-block-0 { background-image: …","ref":"/COAsT/","title":"COAsT"},{"body":"","excerpt":"","ref":"/COAsT/community/","title":"Community"},{"body":"","excerpt":"","ref":"/COAsT/search/","title":"Search Results"}]